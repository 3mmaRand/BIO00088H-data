[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for the Group Research Project",
    "section": "",
    "text": "You are either\n\nan integrated masters student doing BIO00088H Group Research Project or\nan MSc Bioinformatics student doing BIO00070M Research, Professional and Team Skills\n\nIntegrated masters students doing 88H will be doing one of these projects:\nThe project types are:\n\n\n\n\n\n\n\n\nTitle\nDirector\nData analysis strand\n\n\n\n\nIdentifying transcriptional targets of FGF signalling in Xenopus embryos.\nBetsy Pownall\nTranscriptomics, Emma Rand\n\n\nInvestigating the differentiation of stem cells in healthy bone marrow\nJillian Barlow\nTranscriptomics, Emma Rand\n\n\nInvestigatingÂ  pathways involved in the Nickel detoxification in Willow\nLiz Rylott\nTranscriptomics, Emma Rand\n\n\nInvestigating differential RNA expression through the Leishmania lifecycle\nPegine Walrad\nTranscriptomics, Emma Rand\n\n\nIdentifying novel proteins regulating synaptophagy\nRichard Maguire\nImage analysis, Richard Bingham\n\n\nDefining pathological cascades in dopaminergic neurons in a Parkinsonâ€™s model\nSean Sweeney\nImage analysis, Richard Bingham\n\n\nDiscovery proteins for biotech applications: new classes of antibody mimetics\nMichael Plevin\nStructure Analysis, Jon Agirre\n\n\n\nData Analysis compromises five workshops covering computational skills needed in your project. MSc Bioinformatics students do the Core workshops and the transcriptomics workshops as part of BIO00070M. The data analysis workshops are:\n\n\n\n\n\n\n\nWeek\nData Strand\n\n\n\n\n2\nCore 1 Supporting Information - reproducibility, project-oriented workflow, naming things, cool code, handy shortcuts\n\n\n3\nStrand specific 1\n\n\n4\nStrand specific 2\n\n\n5\nStrand specific 3\n\n\n6\nCore 2 Supporting Information - documenting with a README, curating code, non-coded processes\n\n\n\n\n\n\n\n\n\nStudents who successfully complete this module will be able to\n\nuse appropriate computational techniques to reproducibly process, analyse and visualise data and generate scientific reports based on project work.\n\n\n\n\nAll material is on the VLE so why is this site useful? This site collects everything together in a searchable way. The search icon is on the top right.\n\n\n\nRand E (2024). Data Analysis for Group Project. https://3mmarand.github.io/BIO00088H-data/.\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr [Xie (2024); knitr2; knitr3], kableExtra (Zhu 2021)\nReferences"
  },
  {
    "objectID": "index.html#module-learning-outcome-linked-to-this-content",
    "href": "index.html#module-learning-outcome-linked-to-this-content",
    "title": "Data Analysis for the Group Research Project",
    "section": "",
    "text": "Students who successfully complete this module will be able to\n\nuse appropriate computational techniques to reproducibly process, analyse and visualise data and generate scientific reports based on project work."
  },
  {
    "objectID": "index.html#what-is-this-site-for",
    "href": "index.html#what-is-this-site-for",
    "title": "Data Analysis for the Group Research Project",
    "section": "",
    "text": "All material is on the VLE so why is this site useful? This site collects everything together in a searchable way. The search icon is on the top right."
  },
  {
    "objectID": "index.html#please-cite-as",
    "href": "index.html#please-cite-as",
    "title": "Data Analysis for the Group Research Project",
    "section": "",
    "text": "Rand E (2024). Data Analysis for Group Project. https://3mmarand.github.io/BIO00088H-data/.\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr [Xie (2024); knitr2; knitr3], kableExtra (Zhu 2021)\nReferences"
  },
  {
    "objectID": "transcriptomics/week-5/overview.html",
    "href": "transcriptomics/week-5/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week we cover how to visualise the results of your differential expression analysis. The independent study will allow you to check you have what you should have following the Transcriptomics 2: Statistical Analysis workshop and Consolidation study. It will also summarise the the methods and plots we will go through in the workshop. It will also explain how to write the methods for the analyses with have conducted. In the workshop, we will learn how to carry out and plot a Principle Component Analysis (PCA) as well as how to create a nicely formatted Volcano plot.\nThe plots you have by the end of this week will be suitable for including in your report.\nWe suggest you sit together with your group in the workshop.\n\nLearning objectives\nThe successful student will be able to:\n\nverify they have the required RStudio Project set up and the data and code files from the previous Workshop and Consolidation study\nperform a PCA and understand how to interpret them\ncreate a volcano plot and understand how to interpret them\nwrite the methods for the analyses they have conducted\n\n\n\nInstructions\n\nPrepare\n\nğŸ“– Read what you should have so far\nğŸ“– Read about concepts in PCA and volcano plots\nğŸ“– Read about how to write the methods for the analyses you have conducted\n\nWorkshop\n\nğŸ’» Perform and plot a PCA\nğŸ’» Visualise all the results with a volcano plot\nğŸ’» Look after future you!\n\nConsolidate\n\nğŸ’» Use the work you completed in the workshop as a template to apply to a new case.\n\n\n\n\nReferences",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "About"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#overview",
    "href": "transcriptomics/week-5/study_before_workshop.html#overview",
    "title": "Independent Study to prepare for workshop",
    "section": "Overview",
    "text": "Overview\nIn these slides we will:\n\nCheck where you are\n\nlearn some concepts used omics visualisation\n\nPrinciple Component Analysis (PCA)\nVolcano plots\n\n\nFind out what packages to install before the workshop",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#what-we-did-in-transcriptomics-2-statistical-analysis",
    "href": "transcriptomics/week-5/study_before_workshop.html#what-we-did-in-transcriptomics-2-statistical-analysis",
    "title": "Independent Study to prepare for workshop",
    "section": "What we did in Transcriptomics 2: Statistical Analysis",
    "text": "What we did in Transcriptomics 2: Statistical Analysis\n\ncarried out differential expression analysis\nfound genes not expressed at all, or expressed in one group only\nSaved results files",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#where-should-you-be-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#where-should-you-be-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Where should you be?",
    "text": "Where should you be?\nAfter the Transcriptomics 2: ğŸ‘‹ Statistical Analysis Workshop including:\n\nğŸ¤— Look after future you! and\nthe Independent Study to consolidate, you should have:",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#arabidopisis",
    "href": "transcriptomics/week-5/study_before_workshop.html#arabidopisis",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ Arabidopisis\n",
    "text": "ğŸ„ Arabidopisis\n\nAn RStudio Project called arab-88H which contains:\n\ndata-raw: arabidopsis-root.csv, arabidopsis-aerial.csv\n\ndata-processed: root_filtered.csv, aerial_filtered.csv\n\nresults: root_cont_only.csv, root_lowni_only.csv, root_results.csv, and equivalents for aerial\nTwo scripts: cont-low-root.R, cont-low-aerial.R",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#leishmania",
    "href": "transcriptomics/week-5/study_before_workshop.html#leishmania",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nAn RStudio Project called leish-88H which contains:\n\ndata-raw: leishmania-mex-ama.csv, leishmania-mex-pro.csv, leishmania-mex-meta.csv\n\ndata-processed: pro_meta_filtered.csv, pro_ama_filtered.csv\n\nresults: pro_meta_results.csv, pro_ama_results.csv\n\nTwo scripts: pro_meta.R, pro_ama.R",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#stem-cells",
    "href": "transcriptomics/week-5/study_before_workshop.html#stem-cells",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nAn RStudio Project called mice-88H which contains:\n\ndata-raw: secretome_hspc.csv, secretome_prog.csv, secretome_lthsc.csv\n\n\n\ndata-processed: hspc_prog.csv, hspc_lthsc.csv\n\nresults: hspc_prog_results.csv, hspc-lthsc_results.csv,\nTwo scripts: hspc-prog.R, hspc-lthsc.R",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#additionally",
    "href": "transcriptomics/week-5/study_before_workshop.html#additionally",
    "title": "Independent Study to prepare for workshop",
    "section": "Additionallyâ€¦",
    "text": "Additionallyâ€¦\nFiles should be organised into folders. Code should well commented and easy to read. You should have curated your code to remove unnecessary commands that were useful to troubleshoot or understand objects in your environment but which are not needed for the final analysis.\nIf you are missing files, go through:\nGo through:\n\nTranscriptomics 2: Statistical Analysis including:\nğŸ¤— Look after future you! and\nthe Independent Study to consolidate",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#all-results-files",
    "href": "transcriptomics/week-5/study_before_workshop.html#all-results-files",
    "title": "Independent Study to prepare for workshop",
    "section": "All results files",
    "text": "All results files\nRemind yourself of the key columns in any of the results files:\n\nnormalised counts for each sample/cell\na log2 fold change\nan unadjusted p-value\na p value adjusted for multiple testing (called FDR or padj)\na gene id\nother information about each gene",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#results-files",
    "href": "transcriptomics/week-5/study_before_workshop.html#results-files",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ , ğŸ’‰ results files",
    "text": "ğŸ„ , ğŸ’‰ results files\n\n\nbaseMean is the mean of the normalised counts for the gene across all samples\n\nlfcSE standard error of the fold change\n\nstat is the test statistic (the Wald statistic)",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#stem-cells-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#stem-cells-1",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\n\nTop is the rank of the gene ordered by the p-value (smallest first)\n\nsummary.logFC and logFC.hspc give the same value (in this case since comparing two cell types)",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#what-is-the-purpose-of-a-transcriptomics-plot",
    "href": "transcriptomics/week-5/study_before_workshop.html#what-is-the-purpose-of-a-transcriptomics-plot",
    "title": "Independent Study to prepare for workshop",
    "section": "What is the purpose of a Transcriptomics plot?",
    "text": "What is the purpose of a Transcriptomics plot?\n\nIn general, we plot data to help us summarise and understand it\nThis is especially import for transcriptomics data where we have a very large number of variables and often a large number of observations\nWe will look at two plots very commonly used in transcriptomics analysis: Principal Component Analysis (PCA) plot and Volcano Plots",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\n\nPrincipal Component Analysis is an unsupervised machine learning technique\nUnsupervised methods1 are unsupervised in that they do not use/optimise to a particular output. The goal is to uncover structure. They do not test hypotheses\nIt is often used to visualise high dimensional data because it is a dimension reduction technique\nYou may wish to read a previous introduction to unsupervised methods I have written An introduction to Machine Learning: Unsupervised methods (Rand 2021)",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca-1",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\n\nTakes a large number of continuous variables (like gene expression) and reduces them to a smaller number of variables (called principal components) that explain most of the variation in the data\nThe principal components can be plotted to see how samples cluster together",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca-2",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca-2",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\n\n\nTo understand the logic of PCA, imagine we might plot the expression of one gene against that of another\n\n\n\n\n\n\nSamples\n\n\n\n\n\nCells\n\n\n\n\nThis gives us some in insight in how the sample/cells cluster. But we have a lot of genes (even for the stem cells) to consider. How do we know if the pair we use is typical? How can we consider all the genes at once?",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca-3",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca-3",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\n\n\nPCA is a solution for this - It takes a large number of continuous variables (like gene expression) and reduces them to a smaller number of â€œprincipal componentsâ€ that explain most of the variation in the data.\n\n\n\n\n\n\nSamples\n\n\n\n\n\nCells",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca-4",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca-4",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\nWe have done PCA after differential expression, but often PCA might is one of the first exploratory steps because it gives you an idea whether you expect general patterns in gene expression that distinguish groups.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Volcano plots",
    "text": "Volcano plots\n\nVolcano plots often used to visualise the results of differential expression analysis\nThey are just a scatter of the adjusted p value against the fold changeâ€¦.\nalmost - in fact, we plot the negative log of the adjusted p-value against the log fold change\n\nWhy?",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-2",
    "href": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-2",
    "title": "Independent Study to prepare for workshop",
    "section": "Volcano plots",
    "text": "Volcano plots\n\n\nIt is because small probabilities are important, large ones are not which means the axis is counter intuitive because small p-values (i.e., significant values) are at the bottom of the axis)\nAnd since p-values range from 1 to very tiny the important points are all squashed at the bottom of the axis\n\n\n\nVolcano plot padj against fold change",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-3",
    "href": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-3",
    "title": "Independent Study to prepare for workshop",
    "section": "Volcano plots",
    "text": "Volcano plots\n\n\nBy plotting the negative log of the adjusted p-value the values are spread out, and the most significant are at the top of the axis\n\n\n\nVolcano plot -log(adjusted p) against fold change",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#visualisations",
    "href": "transcriptomics/week-5/study_before_workshop.html#visualisations",
    "title": "Independent Study to prepare for workshop",
    "section": "Visualisations",
    "text": "Visualisations\n\nShould be done on normalised data so meaningful comparisons can be made\nThe ğŸ­ stem cell data were already log2normalised\nThe other datasets were normalised by the DE method and we saved the values to the results files. We will log transform them in the workshop",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#packages",
    "href": "transcriptomics/week-5/study_before_workshop.html#packages",
    "title": "Independent Study to prepare for workshop",
    "section": "Packages",
    "text": "Packages\nThis package is on the University computers which you can access on campus or remotely using the VDS\nIf you want to use your own machine you will need to install the package.\n\nInstall ggrepel from CRAN in the the normal way:\n\ninstall.packages(\"ggrepel\")\n\n\nThis package allows you to label points on a plot without them overlapping.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#workshops-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#workshops-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Workshops",
    "text": "Workshops\n\nTranscriptomics 1: Hello data Getting to know the data. Checking the distributions of values overall, across rows and columns to check things are as we expect and detect rows/columns that need to be removed\nTranscriptomics 2: Statistical Analysis. Identifying which genes are differentially expressed between treatments. This is the main analysis step. We will use different methods for bulk and single cell data.\nTranscriptomics 3: Visualising. Principal Component Analysis (PCA) volcano plots to visualise the results of the",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#references",
    "href": "transcriptomics/week-5/study_before_workshop.html#references",
    "title": "Independent Study to prepare for workshop",
    "section": "References",
    "text": "References\n\n\n\n\nRand, Emma. 2021. Data Science Strand of BIO00058M. https://doi.org/10.5281/zenodo.5527705.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html",
    "href": "transcriptomics/week-4/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In the workshop, you will learn how to perform differential expression analysis on raw counts using DESeq2 (Love, Huber, and Anders 2014) or on logged normalised expression values using scran (Lun, McCarthy, and Marioni 2016) or both.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#session-overview",
    "href": "transcriptomics/week-4/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In the workshop, you will learn how to perform differential expression analysis on raw counts using DESeq2 (Love, Huber, and Anders 2014) or on logged normalised expression values using scran (Lun, McCarthy, and Marioni 2016) or both.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#arabidopisis",
    "href": "transcriptomics/week-4/workshop.html#arabidopisis",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopisis\n",
    "text": "ğŸ„ Arabidopisis\n\nğŸ¬ Open the arab-88H RStudio Project and the cont-low-root.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#leishmania",
    "href": "transcriptomics/week-4/workshop.html#leishmania",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nğŸ¬ Open the leish-88H RStudio Project and the pro-meta.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#stem-cells",
    "href": "transcriptomics/week-4/workshop.html#stem-cells",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nğŸ¬ Open the mice-88H RStudio Project and the hspc-prog.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#everyone",
    "href": "transcriptomics/week-4/workshop.html#everyone",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nğŸ¬ Make a new folder results in the project directory.\nThis is where we will save our results.\nğŸ¬ Load tidyverse (Wickham et al. 2019) You most likely have this code at the top of your script already.\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.3     âœ” readr     2.1.4\nâœ” forcats   1.0.0     âœ” stringr   1.5.0\nâœ” ggplot2   3.4.3     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.0\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package to force all conflicts to become errors\nHave you ever stopped to think about this message? It is telling us that there are functions in the dplyr package that have the same name as functions in the stats package and that R will use the dplyr version. As this is what you want, this has always been fine. It still is fine in this case. However, as you start to load more packages, you will want to know if you are using a function from a package that has the same name as a function in another loaded package. This is where the conflicted (Wickham 2023) package comes in. Conflicted will warn you when you are using a function that has the same name as a function in another package. You can then choose which function to use.\nğŸ¬ Load the conflicted package:\n\nlibrary(conflicted)\n\nInstead of getting a warning every time you are using a function that has a function with the same name in another package, we can declare a preference for one function over another. This is useful for the functions you use a lot or ones where you are certain you always want to use a particular function.\nFor example, to always use the dplyr version of filter() by default you can add this to the top of your script:\n\nconflicts_prefer(dplyr::filter)\n\nWe will also want to ensure that we are using the setdiff() function from the GenomicRanges package which is used by DESeq2.\n\nconflicts_prefer(GenomicRanges::setdiff)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#arabidopisis-1",
    "href": "transcriptomics/week-4/workshop.html#arabidopisis-1",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopisis\n",
    "text": "ğŸ„ Arabidopisis\n\nWe need to import the root tissue data that were filtered to remove genes with 3 or 4 zeros and those where the total counts was less than 20.\nğŸ¬ Import the data from the data-processed folder.\nNow go to Differential Expression Analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#leishmania-1",
    "href": "transcriptomics/week-4/workshop.html#leishmania-1",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nWe need to import the procyclic- and metacyclic-promastigote data that were filtered to remove genes with 4, 5 or 6 zeros and those where the total counts was less than 20.\nğŸ¬ Import the data from the data-processed folder.\nNow go to Differential Expression Analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#stem-cells-1",
    "href": "transcriptomics/week-4/workshop.html#stem-cells-1",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nWe need to import the progenitor and HSPC cell data that were combined.\nğŸ¬ Import the data from the data-processed folder..\nNow go to Differential Expression Analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#arabidopisis-differential",
    "href": "transcriptomics/week-4/workshop.html#arabidopisis-differential",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopisis\n",
    "text": "ğŸ„ Arabidopisis\n\nThese are the steps we will take\n\nFind the genes that are expressed in only one treatment group.\nCreate a DESeqDataSet object. This is a special object that is used by the DESeq2 package\nPrepare the normalised counts from the DESeqDataSet object.\nDo differential expression analysis on the genes. This needs to be done on the raw counts.\n\nAll but the first step are done with the DESeq2 package\n1. Genes expressed in one treatment\nThe genes expressed in only one treatment group are those with zeros in all replicates in one group and non-zero values in all replicates in the other group. For example, those shown here:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngene_id\ngene_name\nCTR1\nCTR2\nCTR3\nCTR4\nCTR5\nCTR6\nLWR1\nLWR2\nLWR3\nLWR4\nLWR5\nLWR6\n\n\n\nAT1G63820\nAT1G63820\n0\n0\n0\n0\n0\n0\n13\n4\n4\n7\n10\n9\n\n\nAT1G27090\nAT1G27090\n0\n0\n0\n0\n0\n0\n16\n6\n6\n9\n13\n14\n\n\nAT1G14950\nAT1G14950\n0\n0\n0\n0\n0\n0\n14\n7\n4\n9\n13\n17\n\n\nAT1G18350\nMKK7\n0\n0\n0\n0\n0\n0\n6\n8\n4\n4\n6\n8\n\n\nAT1G65910\nNAC028\n0\n0\n0\n0\n0\n0\n15\n11\n5\n1\n3\n15\n\n\n\n\n\nWe will use filter() to find these genes.\nğŸ¬ Find the genes that are expressed only in the control group:\n\nroot_lowni_only &lt;- root_filtered |&gt;\n  filter(if_all(starts_with(\"CTR\"), \\(x) x == 0),\n         if_all(starts_with(\"LWR\"), \\(x) x &gt; 0) )\n\nThis code creates a new data frame called root_lowni_only that contains only the rows from root_filtered where:\n\nAll CTR columns have a value of 0\nAll LWR columns have a value greater than 0\n\nfilter() keeps rows that match the conditions you give it. Here we give two conditions:\n\nAll columns whose names start with â€œCTRâ€ must have a value of 0.\nAll columns whose names start with â€œLWRâ€ must have a value greater than 0.\n\nThe if_all() function applies a test to each selected column (those starting with â€œCTRâ€ or â€œLWRâ€) for each row. It returns TRUE only if all the selected columns pass the test.\nThe bit \\(x) x == 0 is an anonymous function â€” a function without a name. In base R syntax, itâ€™s equivalent to: function(x) x == 0 Here, x will be the values from one column at a time, and the function checks whether they are equal to 0. We use an anonymous function because the condition needs to be applied separately to each column, but itâ€™s simple enough that thereâ€™s no need to define a full named function elsewhere in the script.\nâ“ How many genes are expressed only in the low nickel group?\n\n\nNote: There may have been more genes with counts in the low nickel group that were removed when we filtered on the total number of counts being less that 50.\nğŸ¬ Now you find any genes that are expressed only in the control group.\nâ“ How many genes are expressed only in the control group?\n\n\nâ“ Do the results make sense to you in light of what you know about the biology?\n\n\n\n\n\n\nğŸ¬ Write all the genes that are expressed one group only to file (saved in results)\n2. Create DESeqDataSet object\nğŸ¬ Load the DESeq2 package:\nA DEseqDataSet object is a custom data type that is used by DESeq2. Custom data types are common in the Bioconductor1 packages. They are used to store data in a way that is useful for the analysis. These data types typically have data, transformed data, metadata and experimental designs within them.\nTo create a DESeqDataSet object, we need to provide three things:\n\nThe raw counts - these are in root_filtered\n\nThe meta data which gives information about the samples and which treatment groups they belong to\nA design matrix which captures the design of the statistical model.\n\nThe counts must in a matrix rather than a dataframe. Unlike a dataframe, a matrix has columns of all the same type. That is, it will contain only the counts. The gene ids are given as row names rather than a column. The matrix() function will create a matrix from a dataframe of columns of the same type and the select() function can be used to remove the gene ids column.\nğŸ¬ Create a matrix of the counts:\n\nroot_count_mat &lt;- root_filtered |&gt;\n  select(-gene_id, -gene_name) |&gt;\n  as.matrix()\n\nğŸ¬ Add the gene ids as row names to the matrix:\n\n# add the row names to the matrix\nrownames(root_count_mat) &lt;- root_filtered$gene_id\n\nYou might want to view the matrix (click on it in your environment pane).\nThe metadata are in a file, arab_meta_data.txt. This is a tab-delimited file. The first column is the sample name and the other columns give the â€œtreatmentsâ€. In this case, the treatments are tissue (with two levels) and nickel (with two levels).\nğŸ¬ Make a folder called meta and save the file to it.\nğŸ¬ Read the metadata into a dataframe:\n\nmeta &lt;- read_table(\"meta/arab_meta_data.txt\")\n\nğŸ¬ Examine the resulting dataframe.\nWe need to add the sample names as row names to the metadata dataframe. This is because the DESeqDataSet object will use the row names to match the samples in the metadata to the samples in the counts matrix.\nğŸ¬ Add the sample names as row names to the metadata dataframe:\n\nmeta &lt;- meta |&gt;\n  column_to_rownames(\"sample_id\")\n\nWe are dealing only with the root data so we need to remove the samples that are not in the root data.\nğŸ¬ Filter the metadata to keep only the root information:\n\nmeta_root &lt;- meta |&gt;\n  filter(tissue == \"root\")\n\nWe can now create the DESeqDataSet object. The design formula describes the statistical model. You should recognise the form from previous work. The ~ can be read as â€œexplain byâ€ and on its right hand side are the explanatory variables. That is, the model is counts explained by nickel status.\nNote that:\n\nThe names of the columns in the count matrix have to exactly match the names of the rows in the metadata dataframe. They also need to be in the same order.\nThe names of the explanatory variables in the design formula have to match the names of columns in the metadata.\n\nğŸ¬ Create the DESeqDataSet object:\n\ndds &lt;- DESeqDataSetFromMatrix(root_count_mat,\n                              colData = meta_root,\n                              design = ~ nickel)\n\nThe warning â€œWarning: some variables in design formula are characters, converting to factorsâ€ just means that the variable type of nickel in the metadata dataframe is â€œcharâ€ and it has been converted into a factor type.\nTo help you understand what the DESeqDataSet object we have called dds contains, we can look its contents\nThe counts are in dds@assays@data@listData[[\"counts\"]] and the metadata are in dds@colData but the easiest way to see them is to use the counts() and colData() functions from the DESeq2 package.\nğŸ¬ View the counts:\n\ncounts(dds) |&gt; View()\n\nYou should be able to see that this is the same as in root_count_mat.\nğŸ¬ View the column information:\n\ncolData(dds)\n\nDataFrame with 12 rows and 2 columns\n          tissue   nickel\n     &lt;character&gt; &lt;factor&gt;\nCTR1        root  control\nCTR2        root  control\nCTR3        root  control\nCTR4        root  control\nCTR5        root  control\n...          ...      ...\nLWR2        root   low_ni\nLWR3        root   low_ni\nLWR4        root   low_ni\nLWR5        root   low_ni\nLWR6        root   low_ni\n\n\nYou should be able to see this is the same as in meta_root.\n3. Prepare the normalised counts\nThe normalised counts are the counts that have been transformed to account for the library size (i.e., the total number of reads in a sample) and the gene length. We have to first estimate the normalisation factors and store them in the DESeqDataSet object and then we can get the normalised counts.\nğŸ¬ Estimate the factors for normalisation and store them in the DESeqDataSet object:\n\ndds &lt;- estimateSizeFactors(dds)\n\nğŸ¬ Look at the factors (just for information):\n\nsizeFactors(dds)\n\n     CTR1      CTR2      CTR3      CTR4      CTR5      CTR6      LWR1      LWR2 \n1.0241774 1.3312269 1.0264881 1.1672503 1.0274692 1.0104721 0.9390444 0.8343701 \n     LWR3      LWR4      LWR5      LWR6 \n0.8624386 0.9117912 1.0260560 0.9740408 \n\n\nThe normalised counts will be useful to use later. To get the normalised counts we again used the counts() function but this time we use the normalized=TRUE argument.\nğŸ¬ Save the normalised to a matrix:\n\nnormalised_counts &lt;- counts(dds, normalized = TRUE)\n\nğŸ¬ Make a dataframe of the normalised counts, adding a column for the gene ids at the same time:\n\nroot_normalised_counts &lt;- data.frame(normalised_counts,\n                                    gene_id = row.names(normalised_counts))\n\n4. Differential expression analysis\nWe use the DESeq() function to do the differential expression analysis. This function fits the statistical model to the data and then uses the model to calculate the significance of the difference between the treatments. It again stores the results in the DESseqDataSet object. Note that the differential expression needs the raw (unnormalised counts) as it does its own normalisation as part of the process.\nğŸ¬ Run the differential expression analysis and store the results in the same object:\n\ndds &lt;- DESeq(dds)\n\nThe function will take only a few moments to run on this data but can take longer for bigger datasets.\nWe need to define the contrasts we want to test. We want to test the difference between the treatments so we will define the contrast as control and low_ni.\nğŸ¬ Define the contrast:\n\ncontrast_suf &lt;- c(\"nickel\", \"control\", \"low_ni\")\n\nNote that nickel is the name of the column in the metadata dataframe and control and low_ni are the names of the levels in the nickel column. By putting them in the order control , low_ni we are saying the fold change will be control / low_ni. This means:\n\npositive log fold changes indicate control &gt; low_ni and\nnegative log fold changes indicates low_ni &gt; control.\n\nIf we had put them in the order low_ni, control we would have the reverse.\nğŸ¬ Extract the results from the DESseqDataSet object:\n\nresults_suf &lt;- results(dds,\n                       contrast = contrast_suf)\n\nThis will give us the log2 fold change, the p-value and the adjusted p-value for the comparison between the control and low_ni for each gene.\nğŸ¬ Put the results in a dataframe and add the gene ids as a column:\n\nroot_results &lt;- data.frame(results_suf,\n                          gene_id = row.names(results_suf))\n\nIt is useful to have the normalised counts and the statistical results in one dataframe.\nğŸ¬ Merge the two dataframes:\n\n# merge the results with the normalised counts\nroot_results &lt;- root_normalised_counts |&gt;\n  left_join(root_results, by = \"gene_id\")\n\nNow go to Add gene information.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#leishmania-differential",
    "href": "transcriptomics/week-4/workshop.html#leishmania-differential",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nThese are the steps we will take\n\nFind the genes that are expressed in only one treatment group.\nCreate a DESeqDataSet object. This is a special object that is used by the DESeq2 package\nPrepare the normalised counts from the DESeqDataSet object.\nDo differential expression analysis on the genes. This needs to be done on the raw counts.\n\nAll but the first step are done with the DESeq2 package\n1. Genes expressed in one treatment\nThe genes expressed in only one treatment group are those with zeros in all replicates in one group and non-zero values in all replicates in the other group.\nWe will use filter() to find these genes.\nğŸ¬ Find the genes that are expressed only at the procyclic-promastigote stage:\n\npro_meta_pro_only &lt;- pro_meta_filtered  |&gt;\n  filter(lm_pro_1 &gt; 0,\n         lm_pro_2 &gt; 0,\n         lm_pro_3 &gt; 0,\n         lm_meta_1 == 0,\n         lm_meta_2 == 0,\n         lm_meta_2 == 0)\n\nâ“ How many genes are expressed only in the procyclic-promastigote stage group?\n\n\nğŸ¬ Now you find any genes that are expressed only at the metacyclic stage\nâ“ How many genes are expressed only at the metacyclic stage?\n\n\nâ“ Do the results make sense to you in light of what you know about the biology?\n\n\n\n\nğŸ¬ Write all the genes that are expressed one group only to file (saved in results)\n2. Create DESeqDataSet object\nğŸ¬ Load the DESeq2 package:\nA DEseqDataSet object is a custom data type that is used by DESeq2. Custom data types are common in the Bioconductor2 packages. They are used to store data in a way that is useful for the analysis. These data types typically have data, transformed data, metadata and experimental designs within them.\nTo create a DESeqDataSet object, we need to provide three things:\n\nThe raw counts - these are in pro_meta_filtered\n\nThe meta data which gives information about the samples and which treatment groups they belong to\nA design matrix which captures the design of the statistical model.\n\nThe counts must in a matrix rather than a dataframe. Unlike a dataframe, a matrix has columns of all the same type. That is, it will contain only the counts. The gene ids are given as row names rather than a column. The matrix() function will create a matrix from a dataframe of columns of the same type and the select() function can be used to remove the gene ids column.\nğŸ¬ Create a matrix of the counts:\n\npro_meta_count_mat &lt;- pro_meta_filtered  |&gt;\n  select(-gene_id) |&gt;\n  as.matrix()\n\nğŸ¬ Add the gene ids as row names to the matrix:\n\n# add the row names to the matrix\nrownames(pro_meta_count_mat) &lt;- pro_meta_filtered$gene_id\n\nYou might want to view the matrix (click on it in your environment pane).\nThe metadata are in a file, leish_meta_data.txt. This is a tab-delimited file. The first column is the sample name and the other columns give the â€œtreatmentsâ€. In this case, the treatment is stage (with three levels).\nğŸ¬ Make a folder called meta and save the file to it.\nğŸ¬ Read the metadata into a dataframe:\n\nmeta &lt;- read_table(\"meta/leish_meta_data.txt\")\n\nğŸ¬ Examine the resulting dataframe.\nWe need to add the sample names as row names to the metadata dataframe. This is because the DESeqDataSet object will use the row names to match the samples in the metadata to the samples in the counts matrix.\nğŸ¬ Add the sample names as row names to the metadata dataframe:\n\nmeta &lt;- meta |&gt;\n  column_to_rownames(\"sample_id\")\n\nWe are dealing only with the wild data so we need to remove the samples that are not in the wild data.\nğŸ¬ Filter the metadata to keep only the procyclic and metacyclic information:\n\nmeta_pro_meta &lt;- meta |&gt;\n  filter(stage != \"amastigotes\")\n\nWe can now create the DESeqDataSet object. The design formula describes the statistical model. You should recognise the form from previous work. The ~ can be read as â€œexplain byâ€ and on its right hand side are the explanatory variables. That is, the model is counts explained by stage status.\nNote that:\n\nThe names of the columns in the count matrix have to exactly match the names of the rows in the metadata dataframe. They also need to be in the same order.\nThe names of the explanatory variables in the design formula have to match the names of columns in the metadata.\n\nğŸ¬ Create the DESeqDataSet object:\n\ndds &lt;- DESeqDataSetFromMatrix(pro_meta_count_mat,\n                              colData = meta_pro_meta,\n                              design = ~ stage)\n\nThe warning â€œWarning: some variables in design formula are characters, converting to factorsâ€ just means that the variable type of stage in the metadata dataframe is â€œcharâ€ and it has been converted into a factor type.\nTo help you understand what the DESeqDataSet object we have called dds contains, we can look its contents\nThe counts are in dds@assays@data@listData[[\"counts\"]] and the metadata are in dds@colData but the easiest way to see them is to use the counts() and colData() functions from the DESeq2 package.\nğŸ¬ View the counts:\n\ncounts(dds) |&gt; View()\n\nYou should be able to see that this is the same as in pro_meta_count_mat.\nğŸ¬ View the column information:\n\ncolData(dds)\n\nDataFrame with 6 rows and 2 columns\n               stage replicate\n            &lt;factor&gt; &lt;numeric&gt;\nlm_pro_1  procyclic          1\nlm_pro_2  procyclic          2\nlm_pro_3  procyclic          3\nlm_meta_1 metacyclic         1\nlm_meta_2 metacyclic         2\nlm_meta_3 metacyclic         3\n\n\nYou should be able to see this is the same as in meta_pro_meta.\n3. Prepare the normalised counts\nThe normalised counts are the counts that have been transformed to account for the library size (i.e., the total number of reads in a sample) and the gene length. We have to first estimate the normalisation factors and store them in the DESeqDataSet object and then we can get the normalised counts.\nğŸ¬ Estimate the factors for normalisation and store them in the DESeqDataSet object:\n\ndds &lt;- estimateSizeFactors(dds)\n\nğŸ¬ Look at the factors (just for information):\n\nsizeFactors(dds)\n\n lm_pro_1  lm_pro_2  lm_pro_3 lm_meta_1 lm_meta_2 lm_meta_3 \n1.3029351 0.9158157 0.9943186 0.7849299 0.8443586 1.3250409 \n\n\nThe normalised counts will be useful to use later. To get the normalised counts we again used the counts() function but this time we use the normalized=TRUE argument.\nğŸ¬ Save the normalised to a matrix:\n\nnormalised_counts &lt;- counts(dds, normalized = TRUE)\n\nğŸ¬ Make a dataframe of the normalised counts, adding a column for the gene ids at the same time:\n\npro_meta_normalised_counts &lt;- data.frame(normalised_counts,\n                                    gene_id = row.names(normalised_counts))\n\n4. Differential expression analysis\nWe use the DESeq() function to do the differential expression analysis. This function fits the statistical model to the data and then uses the model to calculate the significance of the difference between the treatments. It again stores the results in the DESseqDataSet object. Note that the differential expression needs the raw (unnormalised counts) as it does its own normalisation as part of the process.\nğŸ¬ Run the differential expression analysis and store the results in the same object:\n\ndds &lt;- DESeq(dds)\n\nThe function will take only a few moments to run on this data but can take longer for bigger datasets.\nWe need to define the contrasts we want to test. We want to test the difference between the treatments so we will define the contrast as procyclic and metacyclic.\nğŸ¬ Define the contrast:\n\ncontrast_pro_meta &lt;- c(\"stage\", \"procyclic\", \"metacyclic\")\n\nNote that stage is the name of the column in the metadata dataframe and procyclic and metacyclic are the names of the levels in the stage column. By putting them in the order procyclic , metacyclic we are saying the fold change will be procyclic / metacyclic. This means:\n\npositive log fold changes indicate procyclic &gt; metacyclic and\nnegative log fold changes indicates metacyclic &gt; procyclic.\n\nIf we had put them in the order metacyclic, procyclic we would have the reverse.\nğŸ¬ Extract the results from the DESseqDataSet object:\n\nresults_pro_meta &lt;- results(dds,\n                       contrast = contrast_pro_meta)\n\nThis will give us the log2 fold change, the p-value and the adjusted p-value for the comparison between procyclic and metacyclic stage for each gene\nğŸ¬ Put the results in a dataframe and add the gene ids as a column:\n\npro_meta_results &lt;- data.frame(results_pro_meta,\n                          gene_id = row.names(results_pro_meta))\n\nIt is useful to have the normalised counts and the statistical results in one dataframe.\nğŸ¬ Merge the two dataframes:\n\n# merge the results with the normalised counts\npro_meta_results &lt;- pro_meta_normalised_counts |&gt;\n  left_join(pro_meta_results, by = \"gene_id\")\n\nNow go to Add gene information.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#stem-cells-differential",
    "href": "transcriptomics/week-4/workshop.html#stem-cells-differential",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nThese are the steps we will take\n\nFind the genes that are expressed in only one cell type (the prog or the hspc).\nPrepare the data for differential expression analysis with the scran package.\nDo differential expression analysis on the genes using the scran package. This needs to be done on the logged normalised counts.\n\n1. Genes expressed in one cell type\nThe genes expressed in only cell type are those with zeros in all the cells of the other type. We can find these by summing the expression values for each gene across the cells of one type and filtering for those that are zero.\nTo do row wise aggregates such as the sum across rows we can use the rowwise() function. c_across() allows us to use the colon notation to select columns. This is very useful when you have a lot of columns because it would be annoying to have to list all of them. HSPC_001:HSPC_852 means all the columns from HSPC_001 to HSPC_852.\nğŸ¬ Find the genes expressed only in progenitor cells (i.e., those that are 0 in every HSPC cell):\n\nhspc_prog |&gt; \n  rowwise() |&gt; \n  filter(sum(c_across(HSPC_001:HSPC_852)) == 0)\n\n# A tibble: 0 Ã— 1,500\n# Rowwise: \n# â„¹ 1,500 variables: ensembl_gene_id &lt;chr&gt;, HSPC_001 &lt;dbl&gt;, HSPC_002 &lt;dbl&gt;,\n#   HSPC_003 &lt;dbl&gt;, HSPC_004 &lt;dbl&gt;, HSPC_006 &lt;dbl&gt;, HSPC_008 &lt;dbl&gt;,\n#   HSPC_009 &lt;dbl&gt;, HSPC_011 &lt;dbl&gt;, HSPC_012 &lt;dbl&gt;, HSPC_014 &lt;dbl&gt;,\n#   HSPC_015 &lt;dbl&gt;, HSPC_016 &lt;dbl&gt;, HSPC_017 &lt;dbl&gt;, HSPC_018 &lt;dbl&gt;,\n#   HSPC_020 &lt;dbl&gt;, HSPC_021 &lt;dbl&gt;, HSPC_022 &lt;dbl&gt;, HSPC_023 &lt;dbl&gt;,\n#   HSPC_024 &lt;dbl&gt;, HSPC_025 &lt;dbl&gt;, HSPC_026 &lt;dbl&gt;, HSPC_027 &lt;dbl&gt;,\n#   HSPC_028 &lt;dbl&gt;, HSPC_030 &lt;dbl&gt;, HSPC_031 &lt;dbl&gt;, HSPC_033 &lt;dbl&gt;, â€¦\n\n\nWe already know from last weekâ€™s work that there are no genes that are zero across all the cells (both types). If we did not know that, we would need to add |&gt; filter(sum(c_across(Prog_001:Prog_852)) != 0)\nmeaning zero in all the HSPC but not zero in all the Prog\nâ“ How many genes are expressed only in the progenitor cells only\n\n\nğŸ¬ Now you find any genes that are expressed only in the HSPC cells.\nâ“ How many genes are expressed only in the HSPC cells?\n\n\nâ“ Do the results make sense to you in light of what you know about the biology?\n\n\n\n\n\n\n\nğŸ¬ Write all the genes that are expressed one cell type only to file (saved in results)\n2. Prepare the data for analysis with scran\n\nscran can use a matrix or a dataframe of counts but these must be log normalised counts. If using a dataframe, the columns must only contain the expression values (not the gene ids). The rows can be named to retain the gene ids.\nhspc_prog is a dataframe so we will use the ensembl gene ids to name the rows and remove the gene ids from the dataframe.\nğŸ¬ Add the gene ids as the row names:\n\nhspc_prog &lt;- hspc_prog |&gt;\n  column_to_rownames(\"ensembl_gene_id\")\n\nLike DESeq2, scran needs metadata to define which columns were in which group. Instead of having this is a file, we will create a vector that indicates which column belongs to which cell type.\nğŸ¬ Create a vector that indicates which column belongs to which cell type:\n\nn_hspc &lt;- 701\nn_prog &lt;- 798\n\ncell_type &lt;- rep(c(\"hspc\",\"prog\"), \n                 times = c(n_hspc, n_prog))\n\nThe number of times each cell type is repeated is the number of cells of that type. Do check that the length of the cell_type vector is the same as the number of columns in the hspc_prog dataframe.\n3. Differential expression analysis\nğŸ¬ Load the scran package:\nDifferential expression is carried out with the findMarkers() function. It takes two arguments. The first argument is the dataframe containing the data and the second argument is the vector indicating which columns are in which cell type. Make sure that the order of the cell types in the vector is appropriate for the order of the columns in the dataframe.\nğŸ¬ Run the differential expression analysis:\n\nresults_hspc_prog &lt;- findMarkers(hspc_prog, \n                             cell_type)\n\nThe output is a list object which, rather unnecessarily, includes two dataframes. This is not really necessary as the results are the same except for the fold change having a different sign.\n\n\nThe dataframe results_hspc_prog$prog is log prog - log hspc (i.e.,Prog/HSPC). This means:\n\nPositive fold change: prog is higher than hspc\nNegative fold change: hspc is higher than prog\n\n\n\n\n\n\nThe results_hspc_prog$prog dataframe\n\n\n\n\n\n\n\n\n\n\n\nTop\np.value\nFDR\nsummary.logFC\nlogFC.hspc\nensembl_gene_id\n\n\n\nENSMUSG00000024399\n1\n0\n0\n-4.3900364\n-4.3900364\nENSMUSG00000024399\n\n\nENSMUSG00000079563\n2\n0\n0\n-4.0005511\n-4.0005511\nENSMUSG00000079563\n\n\nENSMUSG00000042817\n3\n0\n0\n-3.6990453\n-3.6990453\nENSMUSG00000042817\n\n\nENSMUSG00000063856\n4\n0\n0\n0.9115206\n0.9115206\nENSMUSG00000063856\n\n\nENSMUSG00000024053\n5\n0\n0\n3.0351647\n3.0351647\nENSMUSG00000024053\n\n\nENSMUSG00000069792\n6\n0\n0\n-3.4659544\n-3.4659544\nENSMUSG00000069792\n\n\n\n\n\n\n\nThe dataframe results_hspc_prog$hspc is log hspc - log prog (i.e., HSPC/Prog). This means:\n\nPositive fold change: hspc is higher than prog\nNegative fold change: prog is higher than hspc\n\n\n\n\n\n\nThe results_hspc_prog$hspc dataframe. Notice the sign of the fold change is the other way\n\n\n\n\n\n\n\n\n\n\n\nTop\np.value\nFDR\nsummary.logFC\nlogFC.prog\nensembl_gene_id\n\n\n\nENSMUSG00000024399\n1\n0\n0\n4.3900364\n4.3900364\nENSMUSG00000024399\n\n\nENSMUSG00000079563\n2\n0\n0\n4.0005511\n4.0005511\nENSMUSG00000079563\n\n\nENSMUSG00000042817\n3\n0\n0\n3.6990453\n3.6990453\nENSMUSG00000042817\n\n\nENSMUSG00000063856\n4\n0\n0\n-0.9115206\n-0.9115206\nENSMUSG00000063856\n\n\nENSMUSG00000024053\n5\n0\n0\n-3.0351647\n-3.0351647\nENSMUSG00000024053\n\n\nENSMUSG00000069792\n6\n0\n0\n3.4659544\n3.4659544\nENSMUSG00000069792\n\n\n\n\n\nWe only need one of these results dataframes and we will use the prog one. It does not matter which one you use but you do need keep the direction of the foldchange in mind when interpreting the results.\nNotice that the dataframes are ordered by significance rather than the original gene order.\nIt is useful to have the normalised counts and the statistical results in one dataframe to which we will add the gene information from Ensembl. Having all the information together will make it easier to interpret the results and select genes of interest. We will need to extract the results from the list object, add the gene ids and then join with the normalised counts.\nğŸ¬ Extract the results dataframe from the list object and add the gene ids as a column:\n\nhspc_prog_results &lt;- data.frame(results_hspc_prog$prog, \n                                ensembl_gene_id = row.names(results_hspc_prog$prog)) \n\nğŸ¬ Return the ensembl gene ids as a column to the normalised counts:\n\nhspc_prog &lt;- hspc_prog |&gt;\n  rownames_to_column(var = \"ensembl_gene_id\")\n\nğŸ¬ Merge the results dataframe with the normalised counts:\n\n# merge the results with the normalised counts\nhspc_prog_results &lt;- hspc_prog_results |&gt;\n  left_join(hspc_prog, by = \"ensembl_gene_id\")\n\nNow go to Add gene information.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#arabidopisis-add-info",
    "href": "transcriptomics/week-4/workshop.html#arabidopisis-add-info",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopisis\n",
    "text": "ğŸ„ Arabidopisis\n\nEnsembl (Martin et al. 2023; Birney et al. 2004)is a bioinformatics project to organise all the biological information around the sequences of large genomes. The are a large number of databases and BioMart (Smedley et al. 2009) provides a consistent interface to the material. There are web-based tools to use these but the R package biomaRt (Durinck et al. 2009, 2005) gives you programmatic access making it easier to integrate information into R dataframes.\nğŸ¬ Load the biomaRt (Durinck et al. 2009, 2005) package:\n\nlibrary(biomaRt)\n\nThe biomaRt package includes a function to list all the available datasets\nğŸ¬ List the Ensembl â€œmartsâ€ available:\n\nlistEnsemblGenomes()\n\n              biomart                        version\n1       protists_mart      Ensembl Protists Genes 61\n2 protists_variations Ensembl Protists Variations 61\n3          fungi_mart         Ensembl Fungi Genes 61\n4    fungi_variations    Ensembl Fungi Variations 61\n5        metazoa_mart       Ensembl Metazoa Genes 61\n6  metazoa_variations  Ensembl Metazoa Variations 61\n7         plants_mart        Ensembl Plants Genes 61\n8   plants_variations   Ensembl Plants Variations 61\n\n\nplants_mart looks like the one we want. We can see what genomes are available with names like â€œArabidopsisâ€ in this mart using the searchDatasets() function.\nğŸ¬\n\nsearchDatasets(useEnsemblGenomes(biomart = \"plants_mart\"), \n               pattern = \"Arabidopsis\")\n\n             dataset                         description version\n4   ahalleri_eg_gene Arabidopsis halleri genes (Ahal2.2) Ahal2.2\n6    alyrata_eg_gene    Arabidopsis lyrata genes (v.1.0)   v.1.0\n11 athaliana_eg_gene Arabidopsis thaliana genes (TAIR10)  TAIR10\n\n\nathaliana_eg_gene is the Arabidopsis thaliana genes (TAIR10) dataset we want.\nğŸ¬ Connect to the athaliana_eg_gene database in plants_mart:\n\nensembl &lt;- useEnsemblGenomes(biomart = \"plants_mart\",\n                             dataset = \"athaliana_eg_gene\")\n\nğŸ¬ See the the types of information we can retrieve:\n\nlistAttributes(mart = ensembl) |&gt; View()\n\nThere are many (1,796!) possible bits of information (attributes) that can be obtained.\nWe use the getBM() function to retrieve information from the database. The filters argument is used to specified what kind of identifier we are supplying in values to retrieve information. The attributes argument is used to select the information we want to retrieve. The values argument is used to specify the identifiers. The mart argument is used to specify the connection we created.\nğŸ¬ Get the the gene name and a description. We also retreive the gene id so we can later join the information with the results:\n\ngene_info &lt;- getBM(filters = \"ensembl_gene_id\",\n                   attributes = c(\"ensembl_gene_id\",\n                                  \"external_gene_name\",\n                                  \"description\"),\n                   values = root_results$gene_id,\n                   mart = ensembl)\n\nYou should view the resulting dataframe to see what information is available. You can use glimpse() or View().\nğŸ¬ Merge the gene information with the results:\n\n# join the gene info with the results\nroot_results &lt;- root_results |&gt;\n  left_join(gene_info,\n            by = join_by(gene_id == ensembl_gene_id))\n\nğŸ¬ Save the results to a file:\n\nwrite_csv(root_results, file = \"results/root_results.csv\")\n\nNow go to Look after future you.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#leishmania-add-info",
    "href": "transcriptomics/week-4/workshop.html#leishmania-add-info",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\n\nI got the information from TriTrypDB\nwhich is a functional genomic resource for the Trypanosomatidae and Plasmodidae\nhttps://tritrypdb.org/tritrypdb/app/downloads section\nI downloaded the L. mexicana MHOM/GT/2001/U1103 Full GFF and extracted the gene information and saved it as leishmania_mex.xlsx\n\nWe will import this file and join it to the results dataframe.\nğŸ¬ Load the readxl (Wickham and Bryan 2023) package:\n\nlibrary(readxl)\n\nğŸ¬ Import the Xenbase gene information file:\n\ngene_info &lt;- read_excel(\"meta/leishmania_mex.xlsx\") \n\nError in read_excel(\"meta/leishmania_mex.xlsx\"): could not find function \"read_excel\"\n\n\nYou should view the resulting dataframe to see what information is available. You can use glimpse() or View().\nğŸ¬ Merge the gene information with the results:\n\n# join the gene info with the results\npro_meta_results &lt;- pro_meta_results |&gt;\n  left_join(gene_info, by = \"gene_id\")\n\nError in `left_join()`:\n! Join columns in `y` must be present in the data.\nâœ– Problem with `gene_id`.\n\n\nğŸ¬ Save the results to a file:\n\nwrite_csv(pro_meta_results, file = \"results/pro_meta_results.csv\")\n\nNow go to Look after future you.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#stem-cells-add-info",
    "href": "transcriptomics/week-4/workshop.html#stem-cells-add-info",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nEnsembl (Martin et al. 2023; Birney et al. 2004)is a bioinformatics project to organise all the biological information around the sequences of large genomes. The are a large number of databases but BioMart (Smedley et al. 2009) provides a consistent interface to the material. There are web-based tools to use these but the R package biomaRt (Durinck et al. 2009, 2005) gives you programmatic access making it easier to integrate information into R dataframes\nğŸ¬ Load the biomaRt (Durinck et al. 2009, 2005) package:\n\nlibrary(biomaRt)\n\nğŸ¬ Connect to the mouse database\n\n# Connect to the mouse database\n\nensembl &lt;- useMart(biomart = \"ensembl\", \n                   dataset = \"mmusculus_gene_ensembl\")\n\nğŸ¬ See information we can retrieve:\n\n# See what information we can retrieve\nlistAttributes(mart = ensembl) |&gt; View()\n\nThere are many (~3000!) possible bits of information (attributes) that can be obtained.\nWe use the getBM() function to retrieve information from the database. The filters argument is used to specified what kind of identifier we are supplying in values to retrieve information. The attributes argument is used to select the information we want to retrieve. The values argument is used to specify the identifiers. The mart argument is used to specify the connection we created.\nğŸ¬ Get the the gene name and a description. We also retrieve the gene id so we can later join the information with the results:\n\ngene_info &lt;- getBM(filters = \"ensembl_gene_id\",\n                   attributes = c(\"ensembl_gene_id\",\n                                  \"external_gene_name\",\n                                  \"description\"),\n                   values = hspc_prog_results$ensembl_gene_id,\n                   mart = ensembl)\n\nYou should view the resulting dataframe to see what information is available. You can use glimpse() or View(). Notice the dataframe returned only has 422 rows - one of the ids does not have information.\nğŸ¬ Merge the gene information with the results:\n\n# join the gene info with the results\nhspc_prog_results &lt;- hspc_prog_results |&gt; \n  left_join(gene_info, by = \"ensembl_gene_id\")\n\nğŸ¬ Save the results to a file:\n\nwrite_csv(hspc_prog_results, file = \"results/hspc_prog_results.csv\")\n\nNow go to Look after future you.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#footnotes",
    "href": "transcriptomics/week-4/workshop.html#footnotes",
    "title": "Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\nBioconductor is a project that develops and supports R packages for bioinformatics.â†©ï¸\nBioconductor is a project that develops and supports R packages for bioinformatics.â†©ï¸",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_after_workshop.html",
    "href": "transcriptomics/week-4/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "You need only do the section for your own project data\nğŸ„ Arabidopsis\nğŸ¬ Open your arab-88H RStudio Project and the cont-low-aerial.R script you began in the Consolidation study last week. Use the differential expression analysis you did in the workshop (in cont-low-root.R) as a template to continue your script.\nğŸ’‰ Leishmania\nğŸ¬ Open your leish-88H RStudio Project and the pro_ama.R script you began in the Consolidation study last week. Use the differential expression analysis you did in the workshop (in pro_meta.R) as a template to continue your script.\nğŸ­ Stem cells\nğŸ¬ Open your mice-88H RStudio Project and the hspc-lthsc.R script you began in the Consolidation study last week. Use the differential expression analysis you did in the workshop (in hspc-prog.R) as a template to continue your script.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html",
    "href": "transcriptomics/kelly/workshop.html",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "I have some data and information from Kelly. I have interpreted it and written some code to do the calculations.\nHowever, Kelly hasnâ€™t had a chance to look at it yet so I am providing the exact information and data he supplied along with my suggested workflow based on my interpretation of the data and info.\n\nThe file is a CSV file, with some notes on top and the data in the following order, post notes and headers. Please note that all chemical data is in millimolar. There are 62 rows of actual data.\nSample Name â€“ Replicate, Time (days), Acetate, Propanoate, Isobutyrate, Butyrate, Isopentanoate, Pentanoate, Isohexanoate, Hexanoate\nThe students should be able to transform the data from mM to mg/L, and to g/L. To do this they only need to multiply the molecular weight of the compound (listed in the notes in the file) by the concentration in mM to get mg/L. Obviously to get g/L they will just divide by 1000. They should be able to graph the VFA concentrations with time.\nThey should also be able to do a simple flux measurement, which is the change in VFA concentration over a period of time, divided by weight or volume of material. In this case it might be equal to == Delta(Acetate at 3 days - Acetate at 1 day)/Delta (3days - 1day)/50 mls sludge. This would provide a final flux with the units of mg acetate per ml sludge per day. Let me know if this isnâ€™t clear.\nPerhaps more importantly they should be able to graph and extract the reaction rate, assuming a first order chemical/biological reaction and an exponential falloff rate. I found this as a starting point (https://martinlab.chem.umass.edu/r-fitting-data/) , but I assume Emma has something much more effective already in the pipeline.\n\nI created these two data files from the original.\n\n8 VFA in mM for 60 samples vfa.csv. There were 63 rows of data in the original file. There were no time 0 for one treatment and all values were zero for the other treatment so I removed those.\n\nTwo treatments: straw (CN10) and water (NC)\n10 time points: 1, 3, 5, 9, 11, 13, 16, 18, 20, 22\nthree replicates per treatment per time point\n2 x 10 x 3 = 60 groups\n8 VFA with concentration in mM (millimolar): acetate, propanoate, isobutyrate, butyrate, isopentanoate, pentanoate, isohexanoate, hexanoate\n\n\nMolecular weights for each VFA in grams per mole mol_wt.txt VFAs from AD vials\n\nWe need to:\n\nCalculate Change in VFA g/l with time\nRecalculate the data into grams per litre - convert to molar: 1 millimolar to molar = 0.001 molar - multiply by the molecular weight of each VFA\nCalculate the percent representation of each VFA, by mM and by weight\nCalculate the flux (change in VFA concentration over a period of time, divided by weight or volume of material) of each VFA, by mM and by weight\nGraph and extract the reaction rate, assuming a first order chemical/biological reaction and an exponential falloff rate\n\nğŸ¬ Start RStudio from the Start menu\nğŸ¬ Make an RStudio project. Be deliberate about where you create it so that it is a good place for you\nğŸ¬ Use the Files pane to make new folders for the data. I suggest data-raw and data-processed\nğŸ¬ Make a new script called analysis.R to carry out the rest of the work.\nğŸ¬ Load tidyverse (Wickham et al. 2019) for importing, summarising, plotting and filtering.\n\nlibrary(tidyverse)\n\n\nğŸ¬ Save the files to data-raw. Open them and examine them. You may want to use Excel for the csv file.\nğŸ¬ Answer the following questions:\n\nWhat is in the rows and columns of each file?\nHow many rows and columns are there in each file?\nHow are the data organised ?\n\nğŸ¬ Import\n\nvfa_cummul &lt;- read_csv(\"data-raw/vfa.csv\") |&gt; janitor::clean_names()\n\nğŸ¬ Split treatment and replicate to separate columns so there is a treatment column:\n\nvfa_cummul &lt;- vfa_cummul |&gt; \n  separate(col = sample_replicate, \n           into = c(\"treatment\", \"replicate\"), \n           sep = \"-\",\n           remove = FALSE)\n\nğŸ“¢ This code depends on the sample_replicate column being in the form treatment-replicate. In the sample data CN10 and NC are the treatments. The replicate is a number from 1 to 3. The value does include a encoding for time. You might want to edit your file to match this format.\nThe provided data is cumulative/absolute. We need to calculate the change in VFA with time. There is a function, lag() that will help us do this. It will take the previous value and subtract it from the current value. We need to do that separately for each sample_replicate so we need to group by sample_replicate first. We also need to make sure the data is in the right order so we will arrange by sample_replicate and time_day.\n\nğŸ¬ Create dataframe for the change in VFA ğŸ“¢ and the change in time\n\nvfa_delta &lt;- vfa_cummul |&gt; \n    group_by(sample_replicate)  |&gt; \n    arrange(sample_replicate, time_day) |&gt;\n    mutate(acetate = acetate - lag(acetate),\n           propanoate = propanoate - lag(propanoate),\n           isobutyrate = isobutyrate - lag(isobutyrate),\n           butyrate = butyrate - lag(butyrate),\n           isopentanoate = isopentanoate - lag(isopentanoate),\n           pentanoate = pentanoate - lag(pentanoate),\n           isohexanoate = isohexanoate - lag(isohexanoate),\n           hexanoate = hexanoate - lag(hexanoate),\n           delta_time = time_day - lag(time_day))\n\nNow we have two dataframes, one for the cumulative data and one for the change in VFA and time. Note that the VFA values have been replaced by the change in VFA but the change in time is in a separate column. I have done this because we later want to plot flux (not yet added) against time\nğŸ“¢ This code also depends on the sample_replicate column being in the form treatment-replicate. lag is calculating the difference between a value at one time point and the next for a treatment-replicate combination.\n\nTo make conversions from mM to g/l we need to do mM * 0.001 * MW. We will import the molecular weight data, pivot the VFA data to long format and join the molecular weight data to the VFA data. Then we can calculate the g/l. We will do this for both the cumulative and delta dataframes.\nğŸ¬ import molecular weight data\n\nmol_wt &lt;- read_table(\"data-raw/mol_wt.txt\") |&gt;\n  mutate(vfa = tolower(vfa))\n\nğŸ¬ Pivot the cumulative data to long format:\n\nvfa_cummul &lt;- vfa_cummul |&gt; \n  pivot_longer(cols = -c(sample_replicate,\n                         treatment, \n                         replicate,\n                         time_day),\n               values_to = \"conc_mM\",\n               names_to = \"vfa\") \n\nView vfa_cummul to check you understand what you have done.\nğŸ¬ Join molecular weight to data and calculate g/l (mutate to convert to g/l * 0.001 * MW):\n\nvfa_cummul &lt;- vfa_cummul |&gt; \n  left_join(mol_wt, by = \"vfa\") |&gt;\n  mutate(conc_g_l = conc_mM * 0.001 * mw)\n\nView vfa_cummul to check you understand what you have done.\nRepeat for the delta data.\nğŸ¬ Pivot the change data, delta_vfa to long format (ğŸ“¢ delta_time is added to the list of columns that do not need to be pivoted but repeated):\n\nvfa_delta &lt;- vfa_delta |&gt; \n  pivot_longer(cols = -c(sample_replicate,\n                         treatment, \n                         replicate,\n                         time_day,\n                         delta_time),\n               values_to = \"conc_mM\",\n               names_to = \"vfa\") \n\nView vfa_delta to check it looks like vfa_cummul\nğŸ¬ Join molecular weight to data and calculate g/l (mutate to convert to g/l * 0.001 * MW):\n\nvfa_delta &lt;- vfa_delta |&gt; \n  left_join(mol_wt, by = \"vfa\") |&gt;\n  mutate(conc_g_l = conc_mM * 0.001 * mw)\n\n\nby mM and by weight\nğŸ¬ Add a column which is the percent representation of each VFA for mM and g/l:\n\nvfa_cummul &lt;- vfa_cummul |&gt; \n  group_by(sample_replicate, time_day) |&gt; \n  mutate(percent_conc_g_l = conc_g_l / sum(conc_g_l) * 100,\n         percent_conc_mM = conc_mM / sum(conc_mM) * 100)\n\n\nğŸ¬ Make summary data for graphing\n\nvfa_cummul_summary &lt;- vfa_cummul |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_g_l = mean(conc_g_l),\n            se_g_l = sd(conc_g_l)/sqrt(length(conc_g_l)),\n            mean_mM = mean(conc_mM),\n            se_mM = sd(conc_mM)/sqrt(length(conc_mM))) |&gt; \n  ungroup()\n\n\nvfa_delta_summary &lt;- vfa_delta |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_g_l = mean(conc_g_l),\n            se_g_l = sd(conc_g_l)/sqrt(length(conc_g_l)),\n            mean_mM = mean(conc_mM),\n            se_mM = sd(conc_mM)/sqrt(length(conc_mM))) |&gt; \n  ungroup()\n\nğŸ¬ Graph the cumulative data, grams per litre:\n\nvfa_cummul_summary |&gt; \n  ggplot(aes(x = time_day, colour = vfa)) +\n  geom_line(aes(y = mean_g_l), \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean_g_l - se_g_l,\n                    ymax = mean_g_l + se_g_l),\n                width = 0.5, \n                show.legend = F,\n                linewidth = 1) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean VFA concentration (g/l)\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nğŸ¬ Graph the change data, grams per litre:\n\nvfa_delta_summary |&gt; \n  ggplot(aes(x = time_day, colour = vfa)) +\n  geom_line(aes(y = mean_g_l), \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean_g_l - se_g_l,\n                    ymax = mean_g_l + se_g_l),\n                width = 0.5, \n                show.legend = F,\n                linewidth = 1) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean change in VFA concentration (g/l)\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nğŸ¬ Graph the mean percent representation of each VFA g/l. Note geom_col() will plot proportion if we setposition = \"fill\"\n\nvfa_cummul_summary |&gt; \n  ggplot(aes(x = time_day, y = mean_g_l, fill = vfa)) +\n  geom_col(position = \"fill\") +\n  scale_fill_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean Proportion VFA\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\nWe have 8 VFA in our dataset. PCA will allow us to plot our samples in the â€œVFAâ€ space so we can see if treatments, time or replicate cluster.\nHowever, PCA expects a matrix with samples in rows and VFA, the variables, in columns. We will need to select the columns we need and pivot wider. Then convert to a matrix.\nğŸ¬\n\nvfa_cummul_pca &lt;- vfa_cummul |&gt; \n  select(sample_replicate, \n         treatment, \n         replicate, \n         time_day, \n         vfa, \n         conc_g_l) |&gt; \n  pivot_wider(names_from = vfa, \n              values_from = conc_g_l)\n\n\nmat &lt;- vfa_cummul_pca |&gt; \n  ungroup() |&gt;\n  select(-sample_replicate, \n         -treatment, \n         -replicate, \n         -time_day) |&gt; \n  as.matrix()\n\nğŸ¬ Perform PCA on the matrix:\n\npca &lt;- mat |&gt;\n  prcomp(scale. = TRUE, \n         rank. = 4) \n\nThe scale. argument tells prcomp() to scale the data to have a mean of 0 and a standard deviation of 1. The rank. argument tells prcomp() to only calculate the first 4 principal components. This is useful for visualisation as we can only plot in 2 or 3 dimensions. We can see the results of the PCA by viewing the summary() of the pca object.\n\nsummary(pca)\n\nImportance of first k=4 (out of 8) components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     2.4977 0.9026 0.77959 0.45567\nProportion of Variance 0.7798 0.1018 0.07597 0.02595\nCumulative Proportion  0.7798 0.8816 0.95760 0.98355\n\n\nThe Proportion of Variance tells us how much of the variance is explained by each component. We can see that the first component explains 0.7798 of the variance, the second 0.1018, and the third 0.07597. Together the first three components explain nearly 96% of the total variance in the data. Plotting PC1 against PC2 will capture about 78% of the variance which is likely much better than we would get plotting any two VFA against each other. To plot the PC1 against PC2 we will need to extract the PC1 and PC2 score from the pca object and add labels for the samples.\nğŸ¬ Create a dataframe of the PC1 and PC2 scores which are in pca$x and add the sample information from vfa_cummul_pca:\n\npca_labelled &lt;- data.frame(pca$x,\n                           sample_replicate = vfa_cummul_pca$sample_replicate,\n                           treatment = vfa_cummul_pca$treatment,\n                           replicate = vfa_cummul_pca$replicate,\n                           time_day = vfa_cummul_pca$time_day) \n\nThe dataframe should look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nsample_replicate\ntreatment\nreplicate\ntime_day\n\n\n\n-2.9592362\n0.6710553\n0.0068846\n-0.4453904\nCN10-1\nCN10\n1\n1\n\n\n-2.7153060\n0.7338367\n-0.2856872\n-0.2030110\nCN10-2\nCN10\n2\n1\n\n\n-2.7423102\n0.8246832\n-0.4964249\n-0.1434490\nCN10-3\nCN10\n3\n1\n\n\n-1.1909064\n-1.0360724\n1.1249513\n-0.7360599\nCN10-1\nCN10\n1\n3\n\n\n-1.3831563\n0.9572091\n-1.5561657\n0.0582755\nCN10-2\nCN10\n2\n3\n\n\n-1.1628940\n-0.0865412\n-0.6046780\n-0.1976743\nCN10-3\nCN10\n3\n3\n\n\n-0.2769661\n-0.2221055\n1.1579897\n-0.6079395\nCN10-1\nCN10\n1\n5\n\n\n0.3480962\n0.3612522\n0.5841649\n-0.0612366\nCN10-2\nCN10\n2\n5\n\n\n-0.7281116\n1.6179706\n-0.6430170\n0.0660727\nCN10-3\nCN10\n3\n5\n\n\n0.9333578\n-0.1339061\n1.0870945\n-0.4374103\nCN10-1\nCN10\n1\n9\n\n\n2.0277528\n0.6993342\n0.3850147\n0.0723540\nCN10-2\nCN10\n2\n9\n\n\n1.9931908\n0.5127260\n0.6605782\n0.1841974\nCN10-3\nCN10\n3\n9\n\n\n1.8365692\n-0.4189762\n0.7029015\n-0.3873133\nCN10-1\nCN10\n1\n11\n\n\n2.3313978\n0.3274834\n-0.0135608\n0.0264372\nCN10-2\nCN10\n2\n11\n\n\n1.5833035\n0.9263509\n-0.1909483\n0.1358320\nCN10-3\nCN10\n3\n11\n\n\n2.8498246\n0.3815854\n-0.4763500\n-0.0280281\nCN10-1\nCN10\n1\n13\n\n\n3.5652461\n-0.0836709\n-0.5948483\n-0.1612809\nCN10-2\nCN10\n2\n13\n\n\n4.1314944\n-1.2254642\n0.2699666\n-0.3152100\nCN10-3\nCN10\n3\n13\n\n\n3.7338024\n-0.6744610\n0.4344639\n-0.3736234\nCN10-1\nCN10\n1\n16\n\n\n3.6748427\n0.5202498\n-0.4333685\n-0.1607235\nCN10-2\nCN10\n2\n16\n\n\n3.9057053\n0.3599520\n-0.3049074\n0.0540037\nCN10-3\nCN10\n3\n16\n\n\n3.4561583\n-0.0996639\n0.4472090\n-0.0185889\nCN10-1\nCN10\n1\n18\n\n\n3.6354729\n0.3809673\n-0.0934957\n0.0018722\nCN10-2\nCN10\n2\n18\n\n\n2.9872250\n0.7890400\n-0.2361098\n-0.1628506\nCN10-3\nCN10\n3\n18\n\n\n3.3562231\n-0.2866224\n0.1331068\n-0.2056366\nCN10-1\nCN10\n1\n20\n\n\n3.2009943\n0.4795967\n-0.2092384\n-0.5962183\nCN10-2\nCN10\n2\n20\n\n\n3.9948127\n0.7772640\n-0.3181372\n0.1218382\nCN10-3\nCN10\n3\n20\n\n\n2.8874207\n0.4554681\n0.3106044\n-0.2220240\nCN10-1\nCN10\n1\n22\n\n\n3.6868864\n0.9681097\n-0.2174166\n-0.2246775\nCN10-2\nCN10\n2\n22\n\n\n4.8689622\n0.5218563\n-0.2906042\n0.3532981\nCN10-3\nCN10\n3\n22\n\n\n-3.8483418\n1.5205541\n-0.8809715\n-0.5306228\nNC-1\nNC\n1\n1\n\n\n-3.7653460\n1.5598499\n-1.0570798\n-0.4075397\nNC-2\nNC\n2\n1\n\n\n-3.8586309\n1.6044929\n-1.0936576\n-0.4292404\nNC-3\nNC\n3\n1\n\n\n-2.6934553\n-0.9198406\n0.7439841\n-0.9881115\nNC-1\nNC\n1\n3\n\n\n-2.5064076\n-1.0856761\n0.6334250\n-0.8999028\nNC-2\nNC\n2\n3\n\n\n-2.4097945\n-1.2731546\n1.1767665\n-0.8715948\nNC-3\nNC\n3\n3\n\n\n-3.0567309\n0.5804906\n-0.1391344\n-0.3701763\nNC-1\nNC\n1\n5\n\n\n-2.3511737\n-0.3692016\n0.7053757\n-0.3284113\nNC-2\nNC\n2\n5\n\n\n-2.6752311\n-0.0637855\n0.4692194\n-0.3841240\nNC-3\nNC\n3\n5\n\n\n-1.2335368\n-0.6717374\n0.2155285\n0.1060486\nNC-1\nNC\n1\n9\n\n\n-1.6550689\n0.1576557\n0.0687658\n0.2750388\nNC-2\nNC\n2\n9\n\n\n-0.8948103\n-0.8171884\n0.8062876\n0.5032756\nNC-3\nNC\n3\n9\n\n\n-1.2512737\n-0.4720993\n0.4071788\n0.4693106\nNC-1\nNC\n1\n11\n\n\n-1.8091407\n0.0552546\n0.0424090\n0.3918222\nNC-2\nNC\n2\n11\n\n\n-2.4225566\n0.4998948\n-0.1987773\n0.1959282\nNC-3\nNC\n3\n11\n\n\n-0.9193427\n-0.7741826\n0.0918984\n0.5089847\nNC-1\nNC\n1\n13\n\n\n-0.8800183\n-0.7850404\n0.0895146\n0.6050052\nNC-2\nNC\n2\n13\n\n\n-1.3075763\n-0.2525829\n-0.2993318\n0.5874269\nNC-3\nNC\n3\n13\n\n\n-0.9543813\n-0.3170305\n0.0885062\n0.7153071\nNC-1\nNC\n1\n16\n\n\n-0.4303679\n-0.9952374\n0.2038883\n0.8214647\nNC-2\nNC\n2\n16\n\n\n-0.9457300\n-0.7180646\n0.3081282\n0.6563748\nNC-3\nNC\n3\n16\n\n\n-1.3830063\n0.0614677\n-0.2805342\n0.5462137\nNC-1\nNC\n1\n18\n\n\n-0.7960522\n-0.5792768\n-0.0369684\n0.6621526\nNC-2\nNC\n2\n18\n\n\n-1.6822927\n0.1041656\n0.0634251\n0.4337240\nNC-3\nNC\n3\n18\n\n\n-1.3157478\n-0.0835664\n-0.1246253\n0.5599467\nNC-1\nNC\n1\n20\n\n\n-1.7425068\n0.3029227\n-0.0161466\n0.5134360\nNC-2\nNC\n2\n20\n\n\n-1.3970678\n-0.2923056\n0.4324586\n0.4765460\nNC-3\nNC\n3\n20\n\n\n-1.0777451\n-0.1232925\n0.2388682\n0.7585307\nNC-1\nNC\n1\n22\n\n\n0.4851039\n-4.1291445\n-4.0625050\n-0.4582436\nNC-2\nNC\n2\n22\n\n\n-1.0516226\n-0.7228479\n1.0641320\n0.4955951\nNC-3\nNC\n3\n22\n\n\n\n\n\nğŸ¬ Plot PC1 against PC2 and colour by time and shape by treatment:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = factor(time_day),\n             shape = treatment)) +\n  geom_point(size = 3) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = \"Time\") +\n  scale_shape_manual(values = c(17, 19),\n                     name = NULL) +\n  theme_classic()\n\n\n\n\n\n\n\nğŸ¬ Plot PC1 against PC2 and colour by time and facet treatment:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, colour = factor(time_day))) +\n  geom_point(size = 3) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = \"Time\") +\n  facet_wrap(~treatment, ncol = 1) +\n  theme_classic()\n\n\n\n\n\n\n\nreplicates are similar at the same time and treatment especially early as we might expect. PC is essentially an axis of time.\n\nWe are going to create an interactive heatmap with the heatmaply (Galili et al. 2017) package. heatmaply takes a matrix as input so we can use mat\nğŸ¬ Set the rownames to the sample id whihcih is combination of sample_replicate and time_day:\n\nrownames(mat) &lt;- interaction(vfa_cummul_pca$sample_replicate, \n                             vfa_cummul_pca$time_day)\n\nYou might want to view the matrix by clicking on it in the environment pane.\nğŸ¬ Load the heatmaply package:\n\nlibrary(heatmaply)\n\nWe need to tell the clustering algorithm how many clusters to create. We will set the number of clusters for the treatments to be 2 and the number of clusters for the vfa to be the same since it makes sense to see what clusters of genes correlate with the treatments.\nğŸ¬ Set the number of clusters for the treatments and vfa:\n\nn_treatment_clusters &lt;- 2\nn_vfa_clusters &lt;- 2\n\nğŸ¬ Create the heatmap:\n\nheatmaply(mat, \n          scale = \"column\",\n          k_col = n_vfa_clusters,\n          k_row = n_treatment_clusters,\n          fontsize_row = 7, fontsize_col = 10,\n          labCol = colnames(mat),\n          labRow = rownames(mat),\n          heatmap_layers = theme(axis.line = element_blank()))\n\n\n\n\n\nThe heatmap will open in the viewer pane (rather than the plot pane) because it is html. You can â€œShow in a new windowâ€ to see it in a larger format. You can also zoom in and out and pan around the heatmap and download it as a png. You might feel the colour bars is not adding much to the plot. You can remove it by setting hide_colorbar = TRUE, in the heatmaply() function.\nOne of the NC replicates at time = 22 is very different from the other replicates. The CN10 treatments cluster together at high time points. CN10 samples are more similar to NC samples early on. Most of the VFAs behave similarly with highest values later in the experiment for CN10 but isohexanoate and hexanoate differ. The difference might be because isohexanoate is especially low in the NC replicates at time = 1 and hexanoate is especially high in the NC replicate 2 at time = 22\n\nCalculate the flux(change in VFA concentration over a period of time, divided by weight or volume of material) of each VFA, by mM and by weight. Emmaâ€™s note: I think the terms flux and reaction rate are used interchangeably\nIâ€™ve requested clarification: for the flux measurements, do they need graphs of the rate of change wrt time? And is the sludge volume going to be a constant for all samples or something they measure and varies by vial?\nAnswer: The sludge volume is constant, at 30 mls within a 120ml vial. Some students will want to graph reaction rate with time, others will want to compare the measured GC-FID concentrations against the model output.\nğŸ“¢ Kelly asked for â€œ.. a simple flux measurement, which is the change in VFA concentration over a period of time, divided by weight or volume of material. In this case it might be equal to == Delta(Acetate at 3 days - Acetate at 1 day)/Delta (3days - 1day)/50 mls sludge. This would provide a final flux with the units of mg acetate per ml sludge per day.â€\nNote: Kelly says mg/ml where earlier he used g/L. These are the same (but I called my column conc_g_l)\nWe need to use the vfa_delta data frame. It contains the change in VFA concentration and the change in time. We will add a column for the flux of each VFA in g/L/day. (mg/ml/day)\n\nsludge_volume &lt;- 30 # ml\nvfa_delta &lt;- vfa_delta |&gt; \n  mutate(flux = conc_g_l / delta_time / sludge_volume)\n\nNAs at time 1 are expected because thereâ€™s no time before that to calculate a changes\n\nGraph and extract the reaction rate assuming a first order chemical/biological reaction and an exponential falloff rate\nIâ€™ve requested clarification: for the nonlinear least squares curve fitting, I assume x is time but Iâ€™m not clear what the Y variable is - concentration? or change in concentration? or rate of change of concentration?\nAnswer: The non-linear equation describes concentration change with time. Effectively the change in concentration is dependent upon the available concentration, in this example [Hex] represents the concentration of Hexanoic acid, while the T0 and T1 represent time steps.\n[Hex]T1 = [Hex]T0 - [Hex]T0 * k\nOr. the amount of Hexanoic acid remaining at T1 (letâ€™s say one hour from the last data point) is equal to the starting concentration ([Hex]T0) minus the concentration dependent metabolism ([Hex]To * k).\nğŸ“¢ We can now plot the observed fluxes (reaction rates) over time\nIâ€™ve summarised the data to add error bars and means\n\nvfa_delta_summary &lt;- vfa_delta |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_flux = mean(flux),\n            se_flux = sd(flux)/sqrt(length(flux))) |&gt; \n  ungroup()\n\n\nggplot(data = vfa_delta, aes(x = time_day, colour = vfa)) +\n  geom_point(aes(y = flux), alpha = 0.6) +\n  geom_errorbar(data = vfa_delta_summary, \n                aes(ymin = mean_flux - se_flux, \n                    ymax = mean_flux + se_flux), \n                width = 1) +\n  geom_errorbar(data = vfa_delta_summary, \n                aes(ymin = mean_flux, \n                    ymax = mean_flux), \n                width = 0.8) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"VFA Flux mg/ml/day\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nOr maybe this is easier to read:\n\nggplot(data = vfa_delta, aes(x = time_day, colour = treatment)) +\n  geom_point(aes(y = flux), alpha = 0.6) +\n  geom_errorbar(data = vfa_delta_summary, \n                aes(ymin = mean_flux - se_flux, \n                    ymax = mean_flux + se_flux), \n                width = 1) +\n  geom_errorbar(data = vfa_delta_summary, \n                aes(ymin = mean_flux, \n                    ymax = mean_flux), \n                width = 0.8) +\n  scale_color_viridis_d(name = NULL, begin = 0.2, end = 0.7) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"VFA Flux mg/ml/day\") +\n  theme_bw() +\n  facet_wrap(~ vfa, nrow = 2) +\n  theme(strip.background = element_blank(),\n        legend.position = \"top\")\n\n\n\n\n\n\n\nI have not yet worked out the best way to plot the modelled reaction rate",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#exact-information-supplied-by-kelly",
    "href": "transcriptomics/kelly/workshop.html#exact-information-supplied-by-kelly",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "The file is a CSV file, with some notes on top and the data in the following order, post notes and headers. Please note that all chemical data is in millimolar. There are 62 rows of actual data.\nSample Name â€“ Replicate, Time (days), Acetate, Propanoate, Isobutyrate, Butyrate, Isopentanoate, Pentanoate, Isohexanoate, Hexanoate\nThe students should be able to transform the data from mM to mg/L, and to g/L. To do this they only need to multiply the molecular weight of the compound (listed in the notes in the file) by the concentration in mM to get mg/L. Obviously to get g/L they will just divide by 1000. They should be able to graph the VFA concentrations with time.\nThey should also be able to do a simple flux measurement, which is the change in VFA concentration over a period of time, divided by weight or volume of material. In this case it might be equal to == Delta(Acetate at 3 days - Acetate at 1 day)/Delta (3days - 1day)/50 mls sludge. This would provide a final flux with the units of mg acetate per ml sludge per day. Let me know if this isnâ€™t clear.\nPerhaps more importantly they should be able to graph and extract the reaction rate, assuming a first order chemical/biological reaction and an exponential falloff rate. I found this as a starting point (https://martinlab.chem.umass.edu/r-fitting-data/) , but I assume Emma has something much more effective already in the pipeline.",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#emmas-worklflow-interpretation",
    "href": "transcriptomics/kelly/workshop.html#emmas-worklflow-interpretation",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "I created these two data files from the original.\n\n8 VFA in mM for 60 samples vfa.csv. There were 63 rows of data in the original file. There were no time 0 for one treatment and all values were zero for the other treatment so I removed those.\n\nTwo treatments: straw (CN10) and water (NC)\n10 time points: 1, 3, 5, 9, 11, 13, 16, 18, 20, 22\nthree replicates per treatment per time point\n2 x 10 x 3 = 60 groups\n8 VFA with concentration in mM (millimolar): acetate, propanoate, isobutyrate, butyrate, isopentanoate, pentanoate, isohexanoate, hexanoate\n\n\nMolecular weights for each VFA in grams per mole mol_wt.txt VFAs from AD vials\n\nWe need to:\n\nCalculate Change in VFA g/l with time\nRecalculate the data into grams per litre - convert to molar: 1 millimolar to molar = 0.001 molar - multiply by the molecular weight of each VFA\nCalculate the percent representation of each VFA, by mM and by weight\nCalculate the flux (change in VFA concentration over a period of time, divided by weight or volume of material) of each VFA, by mM and by weight\nGraph and extract the reaction rate, assuming a first order chemical/biological reaction and an exponential falloff rate",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#getting-started",
    "href": "transcriptomics/kelly/workshop.html#getting-started",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "ğŸ¬ Start RStudio from the Start menu\nğŸ¬ Make an RStudio project. Be deliberate about where you create it so that it is a good place for you\nğŸ¬ Use the Files pane to make new folders for the data. I suggest data-raw and data-processed\nğŸ¬ Make a new script called analysis.R to carry out the rest of the work.\nğŸ¬ Load tidyverse (Wickham et al. 2019) for importing, summarising, plotting and filtering.\n\nlibrary(tidyverse)",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#examine-the-data",
    "href": "transcriptomics/kelly/workshop.html#examine-the-data",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "ğŸ¬ Save the files to data-raw. Open them and examine them. You may want to use Excel for the csv file.\nğŸ¬ Answer the following questions:\n\nWhat is in the rows and columns of each file?\nHow many rows and columns are there in each file?\nHow are the data organised ?",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#import",
    "href": "transcriptomics/kelly/workshop.html#import",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "ğŸ¬ Import\n\nvfa_cummul &lt;- read_csv(\"data-raw/vfa.csv\") |&gt; janitor::clean_names()\n\nğŸ¬ Split treatment and replicate to separate columns so there is a treatment column:\n\nvfa_cummul &lt;- vfa_cummul |&gt; \n  separate(col = sample_replicate, \n           into = c(\"treatment\", \"replicate\"), \n           sep = \"-\",\n           remove = FALSE)\n\nğŸ“¢ This code depends on the sample_replicate column being in the form treatment-replicate. In the sample data CN10 and NC are the treatments. The replicate is a number from 1 to 3. The value does include a encoding for time. You might want to edit your file to match this format.\nThe provided data is cumulative/absolute. We need to calculate the change in VFA with time. There is a function, lag() that will help us do this. It will take the previous value and subtract it from the current value. We need to do that separately for each sample_replicate so we need to group by sample_replicate first. We also need to make sure the data is in the right order so we will arrange by sample_replicate and time_day.",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#calculate-change-in-vfa-gl-with-time",
    "href": "transcriptomics/kelly/workshop.html#calculate-change-in-vfa-gl-with-time",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "ğŸ¬ Create dataframe for the change in VFA ğŸ“¢ and the change in time\n\nvfa_delta &lt;- vfa_cummul |&gt; \n    group_by(sample_replicate)  |&gt; \n    arrange(sample_replicate, time_day) |&gt;\n    mutate(acetate = acetate - lag(acetate),\n           propanoate = propanoate - lag(propanoate),\n           isobutyrate = isobutyrate - lag(isobutyrate),\n           butyrate = butyrate - lag(butyrate),\n           isopentanoate = isopentanoate - lag(isopentanoate),\n           pentanoate = pentanoate - lag(pentanoate),\n           isohexanoate = isohexanoate - lag(isohexanoate),\n           hexanoate = hexanoate - lag(hexanoate),\n           delta_time = time_day - lag(time_day))\n\nNow we have two dataframes, one for the cumulative data and one for the change in VFA and time. Note that the VFA values have been replaced by the change in VFA but the change in time is in a separate column. I have done this because we later want to plot flux (not yet added) against time\nğŸ“¢ This code also depends on the sample_replicate column being in the form treatment-replicate. lag is calculating the difference between a value at one time point and the next for a treatment-replicate combination.",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#recalculate-the-data-into-grams-per-litre",
    "href": "transcriptomics/kelly/workshop.html#recalculate-the-data-into-grams-per-litre",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "To make conversions from mM to g/l we need to do mM * 0.001 * MW. We will import the molecular weight data, pivot the VFA data to long format and join the molecular weight data to the VFA data. Then we can calculate the g/l. We will do this for both the cumulative and delta dataframes.\nğŸ¬ import molecular weight data\n\nmol_wt &lt;- read_table(\"data-raw/mol_wt.txt\") |&gt;\n  mutate(vfa = tolower(vfa))\n\nğŸ¬ Pivot the cumulative data to long format:\n\nvfa_cummul &lt;- vfa_cummul |&gt; \n  pivot_longer(cols = -c(sample_replicate,\n                         treatment, \n                         replicate,\n                         time_day),\n               values_to = \"conc_mM\",\n               names_to = \"vfa\") \n\nView vfa_cummul to check you understand what you have done.\nğŸ¬ Join molecular weight to data and calculate g/l (mutate to convert to g/l * 0.001 * MW):\n\nvfa_cummul &lt;- vfa_cummul |&gt; \n  left_join(mol_wt, by = \"vfa\") |&gt;\n  mutate(conc_g_l = conc_mM * 0.001 * mw)\n\nView vfa_cummul to check you understand what you have done.\nRepeat for the delta data.\nğŸ¬ Pivot the change data, delta_vfa to long format (ğŸ“¢ delta_time is added to the list of columns that do not need to be pivoted but repeated):\n\nvfa_delta &lt;- vfa_delta |&gt; \n  pivot_longer(cols = -c(sample_replicate,\n                         treatment, \n                         replicate,\n                         time_day,\n                         delta_time),\n               values_to = \"conc_mM\",\n               names_to = \"vfa\") \n\nView vfa_delta to check it looks like vfa_cummul\nğŸ¬ Join molecular weight to data and calculate g/l (mutate to convert to g/l * 0.001 * MW):\n\nvfa_delta &lt;- vfa_delta |&gt; \n  left_join(mol_wt, by = \"vfa\") |&gt;\n  mutate(conc_g_l = conc_mM * 0.001 * mw)",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#calculate-the-percent-representation-of-each-vfa",
    "href": "transcriptomics/kelly/workshop.html#calculate-the-percent-representation-of-each-vfa",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "by mM and by weight\nğŸ¬ Add a column which is the percent representation of each VFA for mM and g/l:\n\nvfa_cummul &lt;- vfa_cummul |&gt; \n  group_by(sample_replicate, time_day) |&gt; \n  mutate(percent_conc_g_l = conc_g_l / sum(conc_g_l) * 100,\n         percent_conc_mM = conc_mM / sum(conc_mM) * 100)",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#graphs-for-info-so-far",
    "href": "transcriptomics/kelly/workshop.html#graphs-for-info-so-far",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "ğŸ¬ Make summary data for graphing\n\nvfa_cummul_summary &lt;- vfa_cummul |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_g_l = mean(conc_g_l),\n            se_g_l = sd(conc_g_l)/sqrt(length(conc_g_l)),\n            mean_mM = mean(conc_mM),\n            se_mM = sd(conc_mM)/sqrt(length(conc_mM))) |&gt; \n  ungroup()\n\n\nvfa_delta_summary &lt;- vfa_delta |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_g_l = mean(conc_g_l),\n            se_g_l = sd(conc_g_l)/sqrt(length(conc_g_l)),\n            mean_mM = mean(conc_mM),\n            se_mM = sd(conc_mM)/sqrt(length(conc_mM))) |&gt; \n  ungroup()\n\nğŸ¬ Graph the cumulative data, grams per litre:\n\nvfa_cummul_summary |&gt; \n  ggplot(aes(x = time_day, colour = vfa)) +\n  geom_line(aes(y = mean_g_l), \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean_g_l - se_g_l,\n                    ymax = mean_g_l + se_g_l),\n                width = 0.5, \n                show.legend = F,\n                linewidth = 1) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean VFA concentration (g/l)\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nğŸ¬ Graph the change data, grams per litre:\n\nvfa_delta_summary |&gt; \n  ggplot(aes(x = time_day, colour = vfa)) +\n  geom_line(aes(y = mean_g_l), \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean_g_l - se_g_l,\n                    ymax = mean_g_l + se_g_l),\n                width = 0.5, \n                show.legend = F,\n                linewidth = 1) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean change in VFA concentration (g/l)\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nğŸ¬ Graph the mean percent representation of each VFA g/l. Note geom_col() will plot proportion if we setposition = \"fill\"\n\nvfa_cummul_summary |&gt; \n  ggplot(aes(x = time_day, y = mean_g_l, fill = vfa)) +\n  geom_col(position = \"fill\") +\n  scale_fill_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean Proportion VFA\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\nWe have 8 VFA in our dataset. PCA will allow us to plot our samples in the â€œVFAâ€ space so we can see if treatments, time or replicate cluster.\nHowever, PCA expects a matrix with samples in rows and VFA, the variables, in columns. We will need to select the columns we need and pivot wider. Then convert to a matrix.\nğŸ¬\n\nvfa_cummul_pca &lt;- vfa_cummul |&gt; \n  select(sample_replicate, \n         treatment, \n         replicate, \n         time_day, \n         vfa, \n         conc_g_l) |&gt; \n  pivot_wider(names_from = vfa, \n              values_from = conc_g_l)\n\n\nmat &lt;- vfa_cummul_pca |&gt; \n  ungroup() |&gt;\n  select(-sample_replicate, \n         -treatment, \n         -replicate, \n         -time_day) |&gt; \n  as.matrix()\n\nğŸ¬ Perform PCA on the matrix:\n\npca &lt;- mat |&gt;\n  prcomp(scale. = TRUE, \n         rank. = 4) \n\nThe scale. argument tells prcomp() to scale the data to have a mean of 0 and a standard deviation of 1. The rank. argument tells prcomp() to only calculate the first 4 principal components. This is useful for visualisation as we can only plot in 2 or 3 dimensions. We can see the results of the PCA by viewing the summary() of the pca object.\n\nsummary(pca)\n\nImportance of first k=4 (out of 8) components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     2.4977 0.9026 0.77959 0.45567\nProportion of Variance 0.7798 0.1018 0.07597 0.02595\nCumulative Proportion  0.7798 0.8816 0.95760 0.98355\n\n\nThe Proportion of Variance tells us how much of the variance is explained by each component. We can see that the first component explains 0.7798 of the variance, the second 0.1018, and the third 0.07597. Together the first three components explain nearly 96% of the total variance in the data. Plotting PC1 against PC2 will capture about 78% of the variance which is likely much better than we would get plotting any two VFA against each other. To plot the PC1 against PC2 we will need to extract the PC1 and PC2 score from the pca object and add labels for the samples.\nğŸ¬ Create a dataframe of the PC1 and PC2 scores which are in pca$x and add the sample information from vfa_cummul_pca:\n\npca_labelled &lt;- data.frame(pca$x,\n                           sample_replicate = vfa_cummul_pca$sample_replicate,\n                           treatment = vfa_cummul_pca$treatment,\n                           replicate = vfa_cummul_pca$replicate,\n                           time_day = vfa_cummul_pca$time_day) \n\nThe dataframe should look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nsample_replicate\ntreatment\nreplicate\ntime_day\n\n\n\n-2.9592362\n0.6710553\n0.0068846\n-0.4453904\nCN10-1\nCN10\n1\n1\n\n\n-2.7153060\n0.7338367\n-0.2856872\n-0.2030110\nCN10-2\nCN10\n2\n1\n\n\n-2.7423102\n0.8246832\n-0.4964249\n-0.1434490\nCN10-3\nCN10\n3\n1\n\n\n-1.1909064\n-1.0360724\n1.1249513\n-0.7360599\nCN10-1\nCN10\n1\n3\n\n\n-1.3831563\n0.9572091\n-1.5561657\n0.0582755\nCN10-2\nCN10\n2\n3\n\n\n-1.1628940\n-0.0865412\n-0.6046780\n-0.1976743\nCN10-3\nCN10\n3\n3\n\n\n-0.2769661\n-0.2221055\n1.1579897\n-0.6079395\nCN10-1\nCN10\n1\n5\n\n\n0.3480962\n0.3612522\n0.5841649\n-0.0612366\nCN10-2\nCN10\n2\n5\n\n\n-0.7281116\n1.6179706\n-0.6430170\n0.0660727\nCN10-3\nCN10\n3\n5\n\n\n0.9333578\n-0.1339061\n1.0870945\n-0.4374103\nCN10-1\nCN10\n1\n9\n\n\n2.0277528\n0.6993342\n0.3850147\n0.0723540\nCN10-2\nCN10\n2\n9\n\n\n1.9931908\n0.5127260\n0.6605782\n0.1841974\nCN10-3\nCN10\n3\n9\n\n\n1.8365692\n-0.4189762\n0.7029015\n-0.3873133\nCN10-1\nCN10\n1\n11\n\n\n2.3313978\n0.3274834\n-0.0135608\n0.0264372\nCN10-2\nCN10\n2\n11\n\n\n1.5833035\n0.9263509\n-0.1909483\n0.1358320\nCN10-3\nCN10\n3\n11\n\n\n2.8498246\n0.3815854\n-0.4763500\n-0.0280281\nCN10-1\nCN10\n1\n13\n\n\n3.5652461\n-0.0836709\n-0.5948483\n-0.1612809\nCN10-2\nCN10\n2\n13\n\n\n4.1314944\n-1.2254642\n0.2699666\n-0.3152100\nCN10-3\nCN10\n3\n13\n\n\n3.7338024\n-0.6744610\n0.4344639\n-0.3736234\nCN10-1\nCN10\n1\n16\n\n\n3.6748427\n0.5202498\n-0.4333685\n-0.1607235\nCN10-2\nCN10\n2\n16\n\n\n3.9057053\n0.3599520\n-0.3049074\n0.0540037\nCN10-3\nCN10\n3\n16\n\n\n3.4561583\n-0.0996639\n0.4472090\n-0.0185889\nCN10-1\nCN10\n1\n18\n\n\n3.6354729\n0.3809673\n-0.0934957\n0.0018722\nCN10-2\nCN10\n2\n18\n\n\n2.9872250\n0.7890400\n-0.2361098\n-0.1628506\nCN10-3\nCN10\n3\n18\n\n\n3.3562231\n-0.2866224\n0.1331068\n-0.2056366\nCN10-1\nCN10\n1\n20\n\n\n3.2009943\n0.4795967\n-0.2092384\n-0.5962183\nCN10-2\nCN10\n2\n20\n\n\n3.9948127\n0.7772640\n-0.3181372\n0.1218382\nCN10-3\nCN10\n3\n20\n\n\n2.8874207\n0.4554681\n0.3106044\n-0.2220240\nCN10-1\nCN10\n1\n22\n\n\n3.6868864\n0.9681097\n-0.2174166\n-0.2246775\nCN10-2\nCN10\n2\n22\n\n\n4.8689622\n0.5218563\n-0.2906042\n0.3532981\nCN10-3\nCN10\n3\n22\n\n\n-3.8483418\n1.5205541\n-0.8809715\n-0.5306228\nNC-1\nNC\n1\n1\n\n\n-3.7653460\n1.5598499\n-1.0570798\n-0.4075397\nNC-2\nNC\n2\n1\n\n\n-3.8586309\n1.6044929\n-1.0936576\n-0.4292404\nNC-3\nNC\n3\n1\n\n\n-2.6934553\n-0.9198406\n0.7439841\n-0.9881115\nNC-1\nNC\n1\n3\n\n\n-2.5064076\n-1.0856761\n0.6334250\n-0.8999028\nNC-2\nNC\n2\n3\n\n\n-2.4097945\n-1.2731546\n1.1767665\n-0.8715948\nNC-3\nNC\n3\n3\n\n\n-3.0567309\n0.5804906\n-0.1391344\n-0.3701763\nNC-1\nNC\n1\n5\n\n\n-2.3511737\n-0.3692016\n0.7053757\n-0.3284113\nNC-2\nNC\n2\n5\n\n\n-2.6752311\n-0.0637855\n0.4692194\n-0.3841240\nNC-3\nNC\n3\n5\n\n\n-1.2335368\n-0.6717374\n0.2155285\n0.1060486\nNC-1\nNC\n1\n9\n\n\n-1.6550689\n0.1576557\n0.0687658\n0.2750388\nNC-2\nNC\n2\n9\n\n\n-0.8948103\n-0.8171884\n0.8062876\n0.5032756\nNC-3\nNC\n3\n9\n\n\n-1.2512737\n-0.4720993\n0.4071788\n0.4693106\nNC-1\nNC\n1\n11\n\n\n-1.8091407\n0.0552546\n0.0424090\n0.3918222\nNC-2\nNC\n2\n11\n\n\n-2.4225566\n0.4998948\n-0.1987773\n0.1959282\nNC-3\nNC\n3\n11\n\n\n-0.9193427\n-0.7741826\n0.0918984\n0.5089847\nNC-1\nNC\n1\n13\n\n\n-0.8800183\n-0.7850404\n0.0895146\n0.6050052\nNC-2\nNC\n2\n13\n\n\n-1.3075763\n-0.2525829\n-0.2993318\n0.5874269\nNC-3\nNC\n3\n13\n\n\n-0.9543813\n-0.3170305\n0.0885062\n0.7153071\nNC-1\nNC\n1\n16\n\n\n-0.4303679\n-0.9952374\n0.2038883\n0.8214647\nNC-2\nNC\n2\n16\n\n\n-0.9457300\n-0.7180646\n0.3081282\n0.6563748\nNC-3\nNC\n3\n16\n\n\n-1.3830063\n0.0614677\n-0.2805342\n0.5462137\nNC-1\nNC\n1\n18\n\n\n-0.7960522\n-0.5792768\n-0.0369684\n0.6621526\nNC-2\nNC\n2\n18\n\n\n-1.6822927\n0.1041656\n0.0634251\n0.4337240\nNC-3\nNC\n3\n18\n\n\n-1.3157478\n-0.0835664\n-0.1246253\n0.5599467\nNC-1\nNC\n1\n20\n\n\n-1.7425068\n0.3029227\n-0.0161466\n0.5134360\nNC-2\nNC\n2\n20\n\n\n-1.3970678\n-0.2923056\n0.4324586\n0.4765460\nNC-3\nNC\n3\n20\n\n\n-1.0777451\n-0.1232925\n0.2388682\n0.7585307\nNC-1\nNC\n1\n22\n\n\n0.4851039\n-4.1291445\n-4.0625050\n-0.4582436\nNC-2\nNC\n2\n22\n\n\n-1.0516226\n-0.7228479\n1.0641320\n0.4955951\nNC-3\nNC\n3\n22\n\n\n\n\n\nğŸ¬ Plot PC1 against PC2 and colour by time and shape by treatment:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = factor(time_day),\n             shape = treatment)) +\n  geom_point(size = 3) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = \"Time\") +\n  scale_shape_manual(values = c(17, 19),\n                     name = NULL) +\n  theme_classic()\n\n\n\n\n\n\n\nğŸ¬ Plot PC1 against PC2 and colour by time and facet treatment:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, colour = factor(time_day))) +\n  geom_point(size = 3) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = \"Time\") +\n  facet_wrap(~treatment, ncol = 1) +\n  theme_classic()\n\n\n\n\n\n\n\nreplicates are similar at the same time and treatment especially early as we might expect. PC is essentially an axis of time.\n\nWe are going to create an interactive heatmap with the heatmaply (Galili et al. 2017) package. heatmaply takes a matrix as input so we can use mat\nğŸ¬ Set the rownames to the sample id whihcih is combination of sample_replicate and time_day:\n\nrownames(mat) &lt;- interaction(vfa_cummul_pca$sample_replicate, \n                             vfa_cummul_pca$time_day)\n\nYou might want to view the matrix by clicking on it in the environment pane.\nğŸ¬ Load the heatmaply package:\n\nlibrary(heatmaply)\n\nWe need to tell the clustering algorithm how many clusters to create. We will set the number of clusters for the treatments to be 2 and the number of clusters for the vfa to be the same since it makes sense to see what clusters of genes correlate with the treatments.\nğŸ¬ Set the number of clusters for the treatments and vfa:\n\nn_treatment_clusters &lt;- 2\nn_vfa_clusters &lt;- 2\n\nğŸ¬ Create the heatmap:\n\nheatmaply(mat, \n          scale = \"column\",\n          k_col = n_vfa_clusters,\n          k_row = n_treatment_clusters,\n          fontsize_row = 7, fontsize_col = 10,\n          labCol = colnames(mat),\n          labRow = rownames(mat),\n          heatmap_layers = theme(axis.line = element_blank()))\n\n\n\n\n\nThe heatmap will open in the viewer pane (rather than the plot pane) because it is html. You can â€œShow in a new windowâ€ to see it in a larger format. You can also zoom in and out and pan around the heatmap and download it as a png. You might feel the colour bars is not adding much to the plot. You can remove it by setting hide_colorbar = TRUE, in the heatmaply() function.\nOne of the NC replicates at time = 22 is very different from the other replicates. The CN10 treatments cluster together at high time points. CN10 samples are more similar to NC samples early on. Most of the VFAs behave similarly with highest values later in the experiment for CN10 but isohexanoate and hexanoate differ. The difference might be because isohexanoate is especially low in the NC replicates at time = 1 and hexanoate is especially high in the NC replicate 2 at time = 22",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#calculate-the-flux",
    "href": "transcriptomics/kelly/workshop.html#calculate-the-flux",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "Calculate the flux(change in VFA concentration over a period of time, divided by weight or volume of material) of each VFA, by mM and by weight. Emmaâ€™s note: I think the terms flux and reaction rate are used interchangeably\nIâ€™ve requested clarification: for the flux measurements, do they need graphs of the rate of change wrt time? And is the sludge volume going to be a constant for all samples or something they measure and varies by vial?\nAnswer: The sludge volume is constant, at 30 mls within a 120ml vial. Some students will want to graph reaction rate with time, others will want to compare the measured GC-FID concentrations against the model output.\nğŸ“¢ Kelly asked for â€œ.. a simple flux measurement, which is the change in VFA concentration over a period of time, divided by weight or volume of material. In this case it might be equal to == Delta(Acetate at 3 days - Acetate at 1 day)/Delta (3days - 1day)/50 mls sludge. This would provide a final flux with the units of mg acetate per ml sludge per day.â€\nNote: Kelly says mg/ml where earlier he used g/L. These are the same (but I called my column conc_g_l)\nWe need to use the vfa_delta data frame. It contains the change in VFA concentration and the change in time. We will add a column for the flux of each VFA in g/L/day. (mg/ml/day)\n\nsludge_volume &lt;- 30 # ml\nvfa_delta &lt;- vfa_delta |&gt; \n  mutate(flux = conc_g_l / delta_time / sludge_volume)\n\nNAs at time 1 are expected because thereâ€™s no time before that to calculate a changes",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#graph-and-extract-the-reaction-rate---pending",
    "href": "transcriptomics/kelly/workshop.html#graph-and-extract-the-reaction-rate---pending",
    "title": "Workflow for VFA analysis",
    "section": "",
    "text": "Graph and extract the reaction rate assuming a first order chemical/biological reaction and an exponential falloff rate\nIâ€™ve requested clarification: for the nonlinear least squares curve fitting, I assume x is time but Iâ€™m not clear what the Y variable is - concentration? or change in concentration? or rate of change of concentration?\nAnswer: The non-linear equation describes concentration change with time. Effectively the change in concentration is dependent upon the available concentration, in this example [Hex] represents the concentration of Hexanoic acid, while the T0 and T1 represent time steps.\n[Hex]T1 = [Hex]T0 - [Hex]T0 * k\nOr. the amount of Hexanoic acid remaining at T1 (letâ€™s say one hour from the last data point) is equal to the starting concentration ([Hex]T0) minus the concentration dependent metabolism ([Hex]To * k).\nğŸ“¢ We can now plot the observed fluxes (reaction rates) over time\nIâ€™ve summarised the data to add error bars and means\n\nvfa_delta_summary &lt;- vfa_delta |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_flux = mean(flux),\n            se_flux = sd(flux)/sqrt(length(flux))) |&gt; \n  ungroup()\n\n\nggplot(data = vfa_delta, aes(x = time_day, colour = vfa)) +\n  geom_point(aes(y = flux), alpha = 0.6) +\n  geom_errorbar(data = vfa_delta_summary, \n                aes(ymin = mean_flux - se_flux, \n                    ymax = mean_flux + se_flux), \n                width = 1) +\n  geom_errorbar(data = vfa_delta_summary, \n                aes(ymin = mean_flux, \n                    ymax = mean_flux), \n                width = 0.8) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"VFA Flux mg/ml/day\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nOr maybe this is easier to read:\n\nggplot(data = vfa_delta, aes(x = time_day, colour = treatment)) +\n  geom_point(aes(y = flux), alpha = 0.6) +\n  geom_errorbar(data = vfa_delta_summary, \n                aes(ymin = mean_flux - se_flux, \n                    ymax = mean_flux + se_flux), \n                width = 1) +\n  geom_errorbar(data = vfa_delta_summary, \n                aes(ymin = mean_flux, \n                    ymax = mean_flux), \n                width = 0.8) +\n  scale_color_viridis_d(name = NULL, begin = 0.2, end = 0.7) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"VFA Flux mg/ml/day\") +\n  theme_bw() +\n  facet_wrap(~ vfa, nrow = 2) +\n  theme(strip.background = element_blank(),\n        legend.position = \"top\")\n\n\n\n\n\n\n\nI have not yet worked out the best way to plot the modelled reaction rate",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#vfa-data",
    "href": "transcriptomics/kelly/workshop.html#vfa-data",
    "title": "Workflow for VFA analysis",
    "section": "VFA data",
    "text": "VFA data\nğŸ¬ Load packages\n\nlibrary(tidyverse)\n\n\nvfa_cummul &lt;- read_csv(\"data-raw/vfa2.csv\") |&gt; janitor::clean_names()\n\nThis what I think we have:\n\nset_number Two data sets, one about VFA treatment (2) and one about Protein treatments (1)\n\nreplicate\n\nfor set 1: 1-4\nfor set 2: 1-3\n\n\n\ntreatment\n\nfor set 1: Casein, Keratin\nfor set 2: Acetate, Hexanoic, Decanoic\n\n\ntime_day time in days (note time in the ph data in in hours)\nthen columns for each of the 8 measured VFA: acetate, propanoate, isobutyrate, butyrate, isopentanoate, pentanoate, isohexanoate, hexanoate\n\nI think some of the data have been mislabelled as set 2 when it is set 1. I changed these in the csv file but not on the google sheet until someone can confirm there really are mislabelled).\nIâ€™m going to split these into the two data sets and work on them separately\nğŸ¬ Split the data into the two sets. Note we also remove the set number column as it isnâ€™t need if the data is split into the two sets.\n\nvfa_cummul_protein &lt;- vfa_cummul |&gt; \n  filter(set_number == 1) |&gt; \n  select(-set_number)\n\nvfa_cummul_vfa &lt;- vfa_cummul |&gt; \n  filter(set_number == 2) |&gt;\n  select(-set_number)\n\nvfa_cummul_protein has 2 treatments, four replicates and 10 days = 2 x 4 x 10 = 80 rows\nvfa_cummul_vfa has 3 treatments, 3 replicates and 10 days = 3 x 3 x 10 = 90 rows\nğŸ¬ import molecular weight data\n\nmol_wt &lt;- read_table(\"data-raw/mol_wt.txt\") |&gt;\n  mutate(vfa = tolower(vfa))\n\nSet 1: Protein treatments\n1. Calculate Change in VFA g/l with time\nğŸ¬ Create dataframe for the change in VFA the change in time\n\nvfa_delta_protein &lt;- vfa_cummul_protein |&gt; \n    group_by(treatment, replicate)  |&gt; \n    arrange(treatment, replicate, time_day) |&gt;\n    mutate(acetate = acetate - lag(acetate),\n           propanoate = propanoate - lag(propanoate),\n           isobutyrate = isobutyrate - lag(isobutyrate),\n           butyrate = butyrate - lag(butyrate),\n           isopentanoate = isopentanoate - lag(isopentanoate),\n           pentanoate = pentanoate - lag(pentanoate),\n           isohexanoate = isohexanoate - lag(isohexanoate),\n           hexanoate = hexanoate - lag(hexanoate),\n           delta_time = time_day - lag(time_day))\n\nNow we have two dataframes, one for the cumulative data and one for the change in VFA and time. Note that the VFA values have been replaced by the change in VFA but the change in time is in a separate column. I have done this because we later want to plot flux. Note that unlike the sample data, the time steps are all 1 day so the change in time is always 1 and not really needed. I have included it here to make more clear that the units of flux are which is the change in VFA concentration per unit of time per unit of weight or volume of material\n2. Recalculate the data into grams per litre\nTo make conversions from mM to g/l we need to do mM * 0.001 * MW. We will pivot the VFA data to long format and join the molecular weight data to the VFA data. Then we can calculate the g/l. We will do this for both the cumulative and delta dataframes.\nğŸ¬ Pivot the cumulative data to long format:\n\nvfa_cummul_protein &lt;- vfa_cummul_protein |&gt; \n  pivot_longer(cols = -c(treatment, \n                         replicate,\n                         time_day),\n               values_to = \"conc_mM\",\n               names_to = \"vfa\") \n\nView vfa_cummul_protein to check you understand what you have done.\nğŸ¬ Join molecular weight to data and calculate g/l (mutate to convert to g/l * 0.001 * MW):\n\nvfa_cummul_protein &lt;- vfa_cummul_protein |&gt; \n  left_join(mol_wt, by = \"vfa\") |&gt;\n  mutate(conc_g_l = conc_mM * 0.001 * mw)\n\nView vfa_cummul_protein to check you understand what you have done.\nRepeat for the delta data.\nğŸ¬ Pivot the change data, vfa_delta_protein to long format (ğŸ“¢ delta_time is added to the list of columns that do not need to be pivoted but repeated):\n\nvfa_delta_protein &lt;- vfa_delta_protein |&gt; \n  pivot_longer(cols = -c(treatment, \n                         replicate,\n                         time_day, \n                         delta_time),\n               values_to = \"conc_mM\",\n               names_to = \"vfa\") \n\nView vfa_delta_protein to check it looks like vfa_cummul_protein.\nğŸ¬ Join molecular weight to data and calculate g/l (mutate to convert to g/l * 0.001 * MW):\n\nvfa_delta_protein &lt;- vfa_delta_protein |&gt; \n  left_join(mol_wt, by = \"vfa\") |&gt;\n  mutate(conc_g_l = conc_mM * 0.001 * mw)\n\n3. Calculate the percent representation of each VFA\nby mM and by weight\nğŸ¬ Add a column which is the percent representation of each VFA for mM and g/l:\n\nvfa_cummul_protein &lt;- vfa_cummul_protein |&gt; \n  group_by(treatment, replicate, time_day) |&gt; \n  mutate(percent_conc_g_l = conc_g_l / sum(conc_g_l) * 100,\n         percent_conc_mM = conc_mM / sum(conc_mM) * 100)\n\nGraphs for info so far\nğŸ¬ Make summary data for graphing\n\nvfa_cummul_protein_summary &lt;- vfa_cummul_protein |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_g_l = mean(conc_g_l),\n            se_g_l = sd(conc_g_l)/sqrt(length(conc_g_l)),\n            mean_mM = mean(conc_mM),\n            se_mM = sd(conc_mM)/sqrt(length(conc_mM))) |&gt; \n  ungroup()\n\n\nvfa_delta_protein_summary &lt;- vfa_delta_protein |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_g_l = mean(conc_g_l),\n            se_g_l = sd(conc_g_l)/sqrt(length(conc_g_l)),\n            mean_mM = mean(conc_mM),\n            se_mM = sd(conc_mM)/sqrt(length(conc_mM))) |&gt; \n  ungroup()\n\nğŸ¬ Graph the cumulative data, grams per litre:\n\nvfa_cummul_protein_summary |&gt; \n  ggplot(aes(x = time_day, colour = vfa)) +\n  geom_line(aes(y = mean_g_l), \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean_g_l - se_g_l,\n                    ymax = mean_g_l + se_g_l),\n                width = 0.5, \n                show.legend = F,\n                linewidth = 1) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean VFA concentration (g/l)\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nğŸ¬ Graph the change data, grams per litre:\n\nvfa_delta_protein_summary |&gt; \n  ggplot(aes(x = time_day, colour = vfa)) +\n  geom_line(aes(y = mean_g_l), \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean_g_l - se_g_l,\n                    ymax = mean_g_l + se_g_l),\n                width = 0.5, \n                show.legend = F,\n                linewidth = 1) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean change in VFA concentration (g/l)\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nğŸ¬ Graph the mean percent representation of each VFA g/l. Note geom_col() will plot proportion if we setposition = \"fill\"\n\nvfa_cummul_protein_summary |&gt; \n  ggplot(aes(x = time_day, y = mean_g_l, fill = vfa)) +\n  geom_col(position = \"fill\") +\n  scale_fill_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean Proportion VFA\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n4. Calculate the flux\nCalculate the flux(change in VFA concentration over a period of time, divided by weight or volume of material) of each VFA, by mM and by weight. Emmaâ€™s note: I think the terms flux and reaction rate are used interchangeably\nThe sludge volume is constant, at 30 mls. Flux units are mg vfa per ml sludge per day\nNote: Kelly says mg/ml where earlier he used g/L. These are the same (but I called my column conc_g_l)\nWe need to use the vfa_delta_protein data frame. It contains the change in VFA concentration and the change in time. We will add a column for the flux of each VFA in g/L/day. (mg/ml/day)\n\nsludge_volume &lt;- 30 # ml\nvfa_delta_protein &lt;- vfa_delta_protein |&gt; \n  mutate(flux = conc_g_l / delta_time / sludge_volume)\n\nNAs at time 1 are expected because thereâ€™s no time before that to calculate a changes\n5. Graph and extract the reaction rate\nWe can now plot the observed fluxes (reaction rates) over time\nIâ€™ve summarised the data to add error bars and means\n\nvfa_delta_protein_summary &lt;- vfa_delta_protein |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_flux = mean(flux),\n            se_flux = sd(flux)/sqrt(length(flux))) |&gt; \n  ungroup()\n\n\nggplot(data = vfa_delta_protein, aes(x = time_day, colour = vfa)) +\n  geom_point(aes(y = flux), alpha = 0.6) +\n  geom_errorbar(data = vfa_delta_protein_summary, \n                aes(ymin = mean_flux - se_flux, \n                    ymax = mean_flux + se_flux), \n                width = 1) +\n  geom_errorbar(data = vfa_delta_protein_summary, \n                aes(ymin = mean_flux, \n                    ymax = mean_flux), \n                width = 0.8) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"VFA Flux mg/ml/day\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nOr maybe this is easier to read:\n\nggplot(data = vfa_delta_protein, aes(x = time_day, colour = treatment)) +\n  geom_point(aes(y = flux), alpha = 0.6) +\n  geom_errorbar(data = vfa_delta_protein_summary, \n                aes(ymin = mean_flux - se_flux, \n                    ymax = mean_flux + se_flux), \n                width = 1) +\n  geom_errorbar(data = vfa_delta_protein_summary, \n                aes(ymin = mean_flux, \n                    ymax = mean_flux), \n                width = 0.8) +\n  scale_color_viridis_d(name = NULL, begin = 0.2, end = 0.7) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"VFA Flux mg/ml/day\") +\n  theme_bw() +\n  facet_wrap(~ vfa, nrow = 2) +\n  theme(strip.background = element_blank(),\n        legend.position = \"top\")\n\n\n\n\n\n\n\nSet 2: VFA treatments\n1. Calculate Change in VFA g/l with time\nğŸ¬ Create dataframe for the change in VFA the change in time\n\nvfa_delta_vfa &lt;- vfa_cummul_vfa |&gt; \n    group_by(treatment, replicate)  |&gt; \n    arrange(treatment, replicate, time_day) |&gt;\n    mutate(acetate = acetate - lag(acetate),\n           propanoate = propanoate - lag(propanoate),\n           isobutyrate = isobutyrate - lag(isobutyrate),\n           butyrate = butyrate - lag(butyrate),\n           isopentanoate = isopentanoate - lag(isopentanoate),\n           pentanoate = pentanoate - lag(pentanoate),\n           isohexanoate = isohexanoate - lag(isohexanoate),\n           hexanoate = hexanoate - lag(hexanoate),\n           delta_time = time_day - lag(time_day))\n\nNow we have two dataframes, one for the cumulative data and one for the change in VFA and time. Note that the VFA values have been replaced by the change in VFA but the change in time is in a separate column. I have done this because we later want to plot flux. Note that unlike the sample data, the time steps are all 1 day so the change in time is always 1 and not really needed. I have included it here to make more clear that the units of flux are which is the change in VFA concentration per unit of time per unit of weight or volume of material\n2. Recalculate the data into grams per litre\nTo make conversions from mM to g/l we need to do mM * 0.001 * MW. We will pivot the VFA data to long format and join the molecular weight data to the VFA data. Then we can calculate the g/l. We will do this for both the cumulative and delta dataframes.\nğŸ¬ Pivot the cumulative data to long format:\n\nvfa_cummul_vfa &lt;- vfa_cummul_vfa |&gt; \n  pivot_longer(cols = -c(treatment, \n                         replicate,\n                         time_day),\n               values_to = \"conc_mM\",\n               names_to = \"vfa\") \n\nView vfa_cummul_vfa to check you understand what you have done.\nğŸ¬ Join molecular weight to data and calculate g/l (mutate to convert to g/l * 0.001 * MW):\n\nvfa_cummul_vfa &lt;- vfa_cummul_vfa |&gt; \n  left_join(mol_wt, by = \"vfa\") |&gt;\n  mutate(conc_g_l = conc_mM * 0.001 * mw)\n\nView vfa_cummul_vfa to check you understand what you have done.\nRepeat for the delta data.\nğŸ¬ Pivot the change data, vfa_delta_vfa to long format (ğŸ“¢ delta_time is added to the list of columns that do not need to be pivoted but repeated):\n\nvfa_delta_vfa &lt;- vfa_delta_vfa |&gt; \n  pivot_longer(cols = -c(treatment, \n                         replicate,\n                         time_day,\n                         delta_time),\n               values_to = \"conc_mM\",\n               names_to = \"vfa\") \n\nView vfa_delta_vfa to check it looks like vfa_cummul_vfa.\nğŸ¬ Join molecular weight to data and calculate g/l (mutate to convert to g/l * 0.001 * MW):\n\nvfa_delta_vfa &lt;- vfa_delta_vfa |&gt; \n  left_join(mol_wt, by = \"vfa\") |&gt;\n  mutate(conc_g_l = conc_mM * 0.001 * mw)\n\n3. Calculate the percent representation of each VFA\nby mM and by weight\nğŸ¬ Add a column which is the percent representation of each VFA for mM and g/l:\n\nvfa_cummul_vfa &lt;- vfa_cummul_vfa |&gt; \n  group_by(treatment, replicate, time_day) |&gt; \n  mutate(percent_conc_g_l = conc_g_l / sum(conc_g_l) * 100,\n         percent_conc_mM = conc_mM / sum(conc_mM) * 100)\n\nGraphs for info so far\nğŸ¬ Make summary data for graphing\n\nvfa_cummul_vfa_summary &lt;- vfa_cummul_vfa |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_g_l = mean(conc_g_l),\n            se_g_l = sd(conc_g_l)/sqrt(length(conc_g_l)),\n            mean_mM = mean(conc_mM),\n            se_mM = sd(conc_mM)/sqrt(length(conc_mM))) |&gt; \n  ungroup()\n\n\nvfa_delta_vfa_summary &lt;- vfa_delta_vfa |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_g_l = mean(conc_g_l),\n            se_g_l = sd(conc_g_l)/sqrt(length(conc_g_l)),\n            mean_mM = mean(conc_mM),\n            se_mM = sd(conc_mM)/sqrt(length(conc_mM))) |&gt; \n  ungroup()\n\nğŸ¬ Graph the cumulative data, grams per litre:\n\nvfa_cummul_vfa_summary |&gt; \n  ggplot(aes(x = time_day, colour = vfa)) +\n  geom_line(aes(y = mean_g_l), \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean_g_l - se_g_l,\n                    ymax = mean_g_l + se_g_l),\n                width = 0.5, \n                show.legend = F,\n                linewidth = 1) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean VFA concentration (g/l)\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nğŸ¬ Graph the change data, grams per litre:\n\nvfa_delta_vfa_summary |&gt; \n  ggplot(aes(x = time_day, colour = vfa)) +\n  geom_line(aes(y = mean_g_l), \n            linewidth = 1) +\n  geom_errorbar(aes(ymin = mean_g_l - se_g_l,\n                    ymax = mean_g_l + se_g_l),\n                width = 0.5, \n                show.legend = F,\n                linewidth = 1) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean change in VFA concentration (g/l)\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nğŸ¬ Graph the mean percent representation of each VFA g/l. Note geom_col() will plot proportion if we setposition = \"fill\"\n\nvfa_cummul_vfa_summary |&gt; \n  ggplot(aes(x = time_day, y = mean_g_l, fill = vfa)) +\n  geom_col(position = \"fill\") +\n  scale_fill_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"Mean Proportion VFA\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n4. Calculate the flux\nCalculate the flux(change in VFA concentration over a period of time, divided by weight or volume of material) of each VFA, by mM and by weight. Emmaâ€™s note: I think the terms flux and reaction rate are used interchangeably\nThe sludge volume is constant, at 30 mls. Flux units are mg vfa per ml sludge per day\nNote: Kelly says mg/ml where earlier he used g/L. These are the same (but I called my column conc_g_l)\nWe need to use the vfa_delta_vfa data frame. It contains the change in VFA concentration and the change in time. We will add a column for the flux of each VFA in g/L/day. (mg/ml/day)\n\nsludge_volume &lt;- 30 # ml\nvfa_delta_vfa &lt;- vfa_delta_vfa |&gt; \n  mutate(flux = conc_g_l / delta_time / sludge_volume)\n\nNAs at time 1 are expected because thereâ€™s no time before that to calculate a changes\n5. Graph and extract the reaction rate\nWe can now plot the observed fluxes (reaction rates) over time\nIâ€™ve summarised the data to add error bars and means\n\nvfa_delta_vfa_summary &lt;- vfa_delta_vfa |&gt; \n  group_by(treatment, time_day, vfa) |&gt; \n  summarise(mean_flux = mean(flux),\n            se_flux = sd(flux)/sqrt(length(flux))) |&gt; \n  ungroup()\n\n\nggplot(data = vfa_delta_vfa, aes(x = time_day, colour = vfa)) +\n  geom_point(aes(y = flux), alpha = 0.6) +\n  geom_errorbar(data = vfa_delta_vfa_summary, \n                aes(ymin = mean_flux - se_flux, \n                    ymax = mean_flux + se_flux), \n                width = 1) +\n  geom_errorbar(data = vfa_delta_vfa_summary, \n                aes(ymin = mean_flux, \n                    ymax = mean_flux), \n                width = 0.8) +\n  scale_color_viridis_d(name = NULL) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"VFA Flux mg/ml/day\") +\n  theme_bw() +\n  facet_wrap(~treatment) +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\nOr maybe this is easier to read:\n\nggplot(data = vfa_delta_vfa, aes(x = time_day, colour = treatment)) +\n  geom_point(aes(y = flux), alpha = 0.6) +\n  geom_errorbar(data = vfa_delta_vfa_summary, \n                aes(ymin = mean_flux - se_flux, \n                    ymax = mean_flux + se_flux), \n                width = 1) +\n  geom_errorbar(data = vfa_delta_vfa_summary, \n                aes(ymin = mean_flux, \n                    ymax = mean_flux), \n                width = 0.8) +\n  scale_color_viridis_d(name = NULL, begin = 0.2, end = 0.7) +\n  scale_x_continuous(name = \"Time (days)\") +\n  scale_y_continuous(name = \"VFA Flux mg/ml/day\") +\n  theme_bw() +\n  facet_wrap(~ vfa, nrow = 2) +\n  theme(strip.background = element_blank(),\n        legend.position = \"top\")",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/kelly/workshop.html#ph-data",
    "href": "transcriptomics/kelly/workshop.html#ph-data",
    "title": "Workflow for VFA analysis",
    "section": "ph data",
    "text": "ph data\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr (Xie 2024, 2015, 2014), kableExtra (Zhu 2021)",
    "crumbs": [
      "Transcriptomics",
      "Kelly's Project",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html",
    "href": "transcriptomics/week-3/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop you will learn what steps to take to get a good understanding of your transcriptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It gives you the deep understanding of the data structures and values that you will need to code and trouble-shoot code, allows you to spot failed or problematic samples and informs your decisions on quality control.\nIn this session, you should examine all three data sets because the comparisons will give you a much stronger understanding of your own project data. Compare and contrast is a very useful way to build understanding.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#session-overview",
    "href": "transcriptomics/week-3/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop you will learn what steps to take to get a good understanding of your transcriptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It gives you the deep understanding of the data structures and values that you will need to code and trouble-shoot code, allows you to spot failed or problematic samples and informs your decisions on quality control.\nIn this session, you should examine all three data sets because the comparisons will give you a much stronger understanding of your own project data. Compare and contrast is a very useful way to build understanding.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#arabidopsis",
    "href": "transcriptomics/week-3/workshop.html#arabidopsis",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis\n",
    "text": "ğŸ„ Arabidopsis\n\nImport\nImport the data for root tissue.\nğŸ¬ Import arabidopsis-root.csv\n\n# ğŸ„ import the root data\nroot &lt;- read_csv(\"data-raw/arabidopsis-root.csv\")\n\nğŸ¬ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in â€˜tidyâ€™ format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nğŸ¬ Pivot the counts (stack the columns) so all the counts are in a single column (count) labelled in sample by the column it came from and pipe into ggplot() to create a histogram:\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = count)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis data is very skewed - there are very many low counts and a very few higher numbers. It is hard to see the very low bars for the higher values. Logging the counts is a way to make the distribution more visible. You cannot take the log of 0 so we add 1 to the count before logging. The log of 1 is zero so we will be able to see how many zeros we had.\nğŸ¬ Repeat the plot of log of the counts.\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = log10(count + 1))) +\n  geom_histogram()\n\n\n\n\n\n\n\nIâ€™ve used base 10 only because it easy to convert to the original scale (1 is 10, 2 is 100, 3 is 1000 etc). Notice we have a peak at zero indicating there are many zeros. We would expect the distribution of counts to be roughly log normal because this is expression of all the genes in the genome1. The number of low counts is inflated (small peak near the low end). This suggests that these lower counts might be false positives. The removal of low counts is a common processing step in â€™omic data. We will revisit this after we have considered the distribution of counts across samples and genes.\nDistribution of values across the samples\nSummary statistics including the the number of NAs can be seen using the summary(). It is most helpful which you have up to about 25 columns. There is nothing special about the number 25, it is just that summaries of a larger number of columns are difficult to grasp.\nğŸ¬ Get a quick overview of the 14 columns:\n\n# examine all the columns quickly\n# works well with smaller numbers of column\nsummary(root)\n\n   gene_id           gene_name              CTR1               CTR2         \n Length:32833       Length:32833       Min.   :     0.0   Min.   :     0.0  \n Class :character   Class :character   1st Qu.:     0.0   1st Qu.:     0.0  \n Mode  :character   Mode  :character   Median :   100.0   Median :   131.0  \n                                       Mean   :   705.9   Mean   :   930.5  \n                                       3rd Qu.:   609.0   3rd Qu.:   811.0  \n                                       Max.   :133872.0   Max.   :256188.0  \n      CTR3               CTR4               CTR5               CTR6         \n Min.   :     0.0   Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n 1st Qu.:     0.0   1st Qu.:     0.0   1st Qu.:     0.0   1st Qu.:     0.0  \n Median :    94.0   Median :   114.0   Median :    97.0   Median :    95.0  \n Mean   :   715.2   Mean   :   786.5   Mean   :   731.4   Mean   :   703.7  \n 3rd Qu.:   609.0   3rd Qu.:   686.0   3rd Qu.:   623.0   3rd Qu.:   608.0  \n Max.   :137167.0   Max.   :121160.0   Max.   :168910.0   Max.   :134192.0  \n      LWR1               LWR2               LWR3               LWR4         \n Min.   :     0.0   Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n 1st Qu.:     0.0   1st Qu.:     0.0   1st Qu.:     0.0   1st Qu.:     0.0  \n Median :    81.0   Median :    77.0   Median :    78.0   Median :    80.0  \n Mean   :   730.1   Mean   :   622.2   Mean   :   647.1   Mean   :   662.3  \n 3rd Qu.:   606.0   3rd Qu.:   524.0   3rd Qu.:   538.0   3rd Qu.:   566.0  \n Max.   :183421.0   Max.   :139241.0   Max.   :121856.0   Max.   :139044.0  \n      LWR5               LWR6         \n Min.   :     0.0   Min.   :     0.0  \n 1st Qu.:     0.0   1st Qu.:     0.0  \n Median :    93.0   Median :    89.0  \n Mean   :   739.7   Mean   :   742.8  \n 3rd Qu.:   645.0   3rd Qu.:   625.0  \n Max.   :161872.0   Max.   :179613.0  \n\n\nNotice that:\n\nthe minimum count is 0 and the maximums are very high in all the columns\nthe medians are quite a lot lower than the means so the data are skewed (hump to the left, tail to the right) and there must be quite a lot of zeros\n\nWe want to know how many zeros there are in each a column. To achieve this, we can make use of the fact that TRUE evaluates to 1 and FALSE evaluates to 0. Consequently, summing a column of TRUE/FALSE values will give you the number of TRUE values. For example, sum(CTR1 &gt; 0) gives the number of values above zero in the CTR1 column. If you wanted the number of zeros, you could use sum(CTR1 == 0).\nğŸ¬ Find the number values above zero in all six columns:\n\nroot |&gt;\n  summarise(sum(CTR1 &gt; 0),\n            sum(CTR2 &gt; 0),\n            sum(CTR3 &gt; 0),\n            sum(CTR4 &gt; 0),\n            sum(CTR5 &gt; 0),\n            sum(CTR6 &gt; 0),\n            sum(LWR1 &gt; 0),\n            sum(LWR2 &gt; 0),\n            sum(LWR3 &gt; 0),\n            sum(LWR4 &gt; 0),\n            sum(LWR5 &gt; 0),\n            sum(LWR6 &gt; 0))\n\n# A tibble: 1 Ã— 12\n  `sum(CTR1 &gt; 0)` `sum(CTR2 &gt; 0)` `sum(CTR3 &gt; 0)` `sum(CTR4 &gt; 0)`\n            &lt;int&gt;           &lt;int&gt;           &lt;int&gt;           &lt;int&gt;\n1           23119           23378           22789           23153\n# â„¹ 8 more variables: `sum(CTR5 &gt; 0)` &lt;int&gt;, `sum(CTR6 &gt; 0)` &lt;int&gt;,\n#   `sum(LWR1 &gt; 0)` &lt;int&gt;, `sum(LWR2 &gt; 0)` &lt;int&gt;, `sum(LWR3 &gt; 0)` &lt;int&gt;,\n#   `sum(LWR4 &gt; 0)` &lt;int&gt;, `sum(LWR5 &gt; 0)` &lt;int&gt;, `sum(LWR6 &gt; 0)` &lt;int&gt;\n\n\nThere is a better way of doing this that saves you having to repeat so much code - very useful if you have a lot more than 6 columns! We can use pivot_longer() to put the data in tidy format and then use the group_by() and summarise() approach we have used extensively before.\nğŸ¬ Find the number of zeros in all columns:\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(n_above_zero = sum(count &gt; 0))\n\n# A tibble: 12 Ã— 2\n   sample n_above_zero\n   &lt;chr&gt;         &lt;int&gt;\n 1 CTR1          23119\n 2 CTR2          23378\n 3 CTR3          22789\n 4 CTR4          23153\n 5 CTR5          22941\n 6 CTR6          22982\n 7 LWR1          23103\n 8 LWR2          23111\n 9 LWR3          22955\n10 LWR4          22852\n11 LWR5          23122\n12 LWR6          23034\n\n\nYou could expand this code to get get other useful summary information\nğŸ¬ Summarise all the samples:\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(min = min(count),\n            lowerq = quantile(count, 0.25),\n            mean = mean(count),\n            median = median(count),\n            upperq = quantile(count, 0.75),\n            max = max(count),\n            n_above_zero = sum(count &gt; 0))\n\n# A tibble: 12 Ã— 8\n   sample   min lowerq  mean median upperq    max n_above_zero\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;int&gt;\n 1 CTR1       0      0  706.    100    609 133872        23119\n 2 CTR2       0      0  931.    131    811 256188        23378\n 3 CTR3       0      0  715.     94    609 137167        22789\n 4 CTR4       0      0  786.    114    686 121160        23153\n 5 CTR5       0      0  731.     97    623 168910        22941\n 6 CTR6       0      0  704.     95    608 134192        22982\n 7 LWR1       0      0  730.     81    606 183421        23103\n 8 LWR2       0      0  622.     77    524 139241        23111\n 9 LWR3       0      0  647.     78    538 121856        22955\n10 LWR4       0      0  662.     80    566 139044        22852\n11 LWR5       0      0  740.     93    645 161872        23122\n12 LWR6       0      0  743.     89    625 179613        23034\n\n\nThe mean count ranges from 704 to 931. CTR2 stands out a little - having a higher mean and maximum than the others. When we have a good number of replicates â€“ 6 is good in these experiments â€“ this is unlikely to be a problem. The potential effect of having an odd replicate when you have only two or three replicates, is reduced statistical power. Differences between genes with lower average expression and or more variable expression might be missed. Whether this matters depends on the biological question you are asking. In this case, it does not matter because a) we have 6 replicates and b) because the major differences in gene expression will be enough.\nğŸ¬ Save the summary as a dataframe, root_summary_samp (using assignment).\nWe can also plot the distribution of counts across samples. We have many values (32833) so we are not limited to using geom_histogram(). geom_density() gives us a smooth distribution.\nğŸ¬ Plot the log10 of the counts + 1 again but this time facet by the sample:\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(log10(count + 1))) +\n  geom_density() +\n  facet_wrap(. ~ sample, nrow = 3)\n\n\n\n\n\n\n\nThe key information to take from these plots is:\n\nthe peak at zero suggests quite a few counts of 1.\nwe would expect the distribution of counts in each sample to be roughly log normal so that the rise near the low end, even before the peak at zero, suggests that these lower counts might be anomalies.\n\nWe have found the distribution across samples to be similar to that over all. This is good because it means that the samples are fairly consistent with each other. We can now move on to the next step.\nDistribution of values across the genes\nThere are lots of genes in this dataset therefore we will take a slightly different approach. We would not want to use plot a distribution for each gene in the same way. Will pivot the data to tidy and then summarise the counts for each gene.\nğŸ¬ Summarise the counts for each gene and save the result as root_summary_gene. Include the same columns as we had in the by sample summary (root_summary_samp) and an additional column, total for the total number of counts for each gene.\nğŸ¬ View the root_summary_gene dataframe.\nNotice that we have:\n\na lot of genes with counts of zero in every sample\na lot of genes with zero counts in several of the samples\nsome very very low counts.\n\nGenes with very low counts should be filtered out because they are unreliable - or, at the least, uninformative. The goal of our downstream analysis will be to see if there is a significant difference in gene expression between the control and FGF-treated sibling. Since we have only three replicates in each group, having one or two unreliable, missing or zero values, makes such a determination impossible for a particular gene. We will use the total counts (total) and the number of samples with non-zero values (n_above_zero) in this dataframe to filter our genes later.\nAs we have a lot of genes, it is helpful to plot the mean counts with geom_pointrange() to get an overview of the distributions. We will again plot the log of the mean counts. We will also order the genes from lowest to highest mean count.\nğŸ¬ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\nroot_summary_gene |&gt; \n  ggplot(aes(x = reorder(gene_id, mean), y = log10(mean))) +\n  geom_pointrange(aes(ymin = log10(mean - sd), \n                      ymax = log10(mean + sd )),\n                  size = 0.1)\n\n\n\n\n\n\n\n(Note the warning is expected since we have zero means).\nYou can see we also have quite a few genes with means less than 1 (log below zero). Note that the variability between genes (average counts between 0 and 43348) is far greater than between samples (average counts from 70 to 296) which is exactly what we would expect to see.\nNow go to Filtering for QC.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#leishmania",
    "href": "transcriptomics/week-3/workshop.html#leishmania",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nImport\nImport the data for L.mexicana procyclic promastigote (pro) and the metacyclic promastigotes (meta)\nğŸ¬ Import leishmania-mex-pro.csv and leishmania-mex-meta.csv\n\n# ğŸ’‰ import the pro and meta leish data\npro &lt;- read_csv(\"data-raw/leishmania-mex-pro.csv\")\nmeta &lt;- read_csv(\"data-raw/leishmania-mex-meta.csv\")\n\nWe will need to combine the two sets of columns (datasets) so we can compare the two stages. We will join them using gene_id to match the rows. The column names differ so we donâ€™t need to worry about renaming any of them.\nğŸ¬ Combine the two datasets by gene_id and save the result as pro_meta.\n\n#  combine the two datasets\npro_meta &lt;- pro |&gt;\n  left_join(meta, \n            by = \"gene_id\")\n\nğŸ¬ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in â€˜tidyâ€™ format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nğŸ¬ Pivot the counts (stack the columns) so all the counts are in a single column (count) labelled in sample by the column it came from and pipe into ggplot() to create a histogram:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = count)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis data is very skewed - there are very many low counts and a very few higher numbers. It is hard to see the very low bars for the higher values. Logging the counts is a way to make the distribution more visible. You cannot take the log of 0 so we add 1 to the count before logging. The log of 1 is zero so we will be able to see how many zeros we had.\nğŸ¬ Repeat the plot of log of the counts.\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = log10(count + 1))) +\n  geom_histogram()\n\n\n\n\n\n\n\nIâ€™ve used base 10 only because it easy to convert to the original scale (1 is 10, 2 is 100, 3 is 1000 etc). Notice we have a peak at zero indicating there are many zeros. We would expect the distribution of counts to be roughly log normal because this is expression of all the genes in the genome2. The number of low counts is inflated (small peak near the low end). This suggests that these lower counts might be false positives. The removal of low counts is a common processing step in â€™omic data. We will revisit this after we have considered the distribution of counts across samples and genes.\nDistribution of values across the samples\nSummary statistics including the the number of NAs can be seen using the summary(). It is most helpful which you have up to about 25 columns. There is nothing special about the number 25, it is just that summaries of a larger number of columns are difficult to grasp.\nğŸ¬ Get a quick overview of the 7 columns:\n\n# examine all the columns quickly\n# works well with smaller numbers of column\nsummary(pro_meta)\n\n   gene_id             lm_pro_1           lm_pro_2           lm_pro_3       \n Length:8677        Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n Class :character   1st Qu.:    77.0   1st Qu.:    53.0   1st Qu.:    59.0  \n Mode  :character   Median :   191.0   Median :   135.0   Median :   145.0  \n                    Mean   :   364.5   Mean   :   255.7   Mean   :   281.4  \n                    3rd Qu.:   332.0   3rd Qu.:   238.0   3rd Qu.:   256.0  \n                    Max.   :442477.0   Max.   :295423.0   Max.   :411663.0  \n   lm_meta_1          lm_meta_2          lm_meta_3       \n Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n 1st Qu.:    48.0   1st Qu.:    51.0   1st Qu.:    78.0  \n Median :   110.0   Median :   120.0   Median :   187.0  \n Mean   :   220.3   Mean   :   221.9   Mean   :   355.9  \n 3rd Qu.:   197.0   3rd Qu.:   215.0   3rd Qu.:   341.0  \n Max.   :244569.0   Max.   :205203.0   Max.   :498303.0  \n\n\nNotice that:\n\nthe minimum count is 0 and the maximums are very high in all the columns\nthe medians are quite a lot lower than the means so the data are skewed (hump to the left, tail to the right) and there must be quite a lot of zeros\n\nWe want to know how many zeros there are in each a column. To achieve this, we can make use of the fact that TRUE evaluates to 1 and FALSE evaluates to 0. Consequently, summing a column of TRUE/FALSE values will give you the number of TRUE values. For example, sum(lm_pro_1 &gt; 0) gives the number of values above zero in the lm_pro_1 column. If you wanted the number of zeros, you could use sum(lm_pro_1 == 0).\nğŸ¬ Find the number values above zero in all six columns:\n\npro_meta |&gt;\n  summarise(sum(lm_pro_1 &gt; 0),\n            sum(lm_pro_2 &gt; 0),\n            sum(lm_pro_3 &gt; 0),\n            sum(lm_meta_1 &gt; 0),\n            sum(lm_meta_2 &gt; 0),\n            sum(lm_meta_3 &gt; 0))\n\n# A tibble: 1 Ã— 6\n  `sum(lm_pro_1 &gt; 0)` `sum(lm_pro_2 &gt; 0)` `sum(lm_pro_3 &gt; 0)`\n                &lt;int&gt;               &lt;int&gt;               &lt;int&gt;\n1                8549                8522                8509\n# â„¹ 3 more variables: `sum(lm_meta_1 &gt; 0)` &lt;int&gt;, `sum(lm_meta_2 &gt; 0)` &lt;int&gt;,\n#   `sum(lm_meta_3 &gt; 0)` &lt;int&gt;\n\n\nThere is a better way of doing this that saves you having to repeat so much code - very useful if you have a lot more than 6 columns! We can use pivot_longer() to put the data in tidy format and then use the group_by() and summarise() approach we have used extensively before.\nğŸ¬ Find the number of zeros in all columns:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(n_above_zero = sum(count &gt; 0))\n\n# A tibble: 6 Ã— 2\n  sample    n_above_zero\n  &lt;chr&gt;            &lt;int&gt;\n1 lm_meta_1         8535\n2 lm_meta_2         8535\n3 lm_meta_3         8530\n4 lm_pro_1          8549\n5 lm_pro_2          8522\n6 lm_pro_3          8509\n\n\nYou could expand this code to get get other useful summary information\nğŸ¬ Summarise all the samples:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(min = min(count),\n            lowerq = quantile(count, 0.25),\n            mean = mean(count),\n            median = median(count),\n            upperq = quantile(count, 0.75),\n            max = max(count),\n            n_above_zero = sum(count &gt; 0))\n\n# A tibble: 6 Ã— 8\n  sample      min lowerq  mean median upperq    max n_above_zero\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;int&gt;\n1 lm_meta_1     0     48  220.    110    197 244569         8535\n2 lm_meta_2     0     51  222.    120    215 205203         8535\n3 lm_meta_3     0     78  356.    187    341 498303         8530\n4 lm_pro_1      0     77  364.    191    332 442477         8549\n5 lm_pro_2      0     53  256.    135    238 295423         8522\n6 lm_pro_3      0     59  281.    145    256 411663         8509\n\n\nThe mean count ranges from 220 to 364. We do not appear to have any outlying (odd) replicates. The potential effect of an odd replicate is reduced statistical power. Major differences in gene expression will still be uncovered. Differences between genes with lower average expression and or more variable expression might be missed. Whether this matters depends on the biological question you are asking.\nğŸ¬ Save the summary as a dataframe, pro_meta_summary_samp (using assignment).\nWe can also plot the distribution of counts across samples. We have many values (8677) so we are not limited to using geom_histogram(). geom_density() gives us a smooth distribution.\nğŸ¬ Plot the log10 of the counts + 1 again but this time facet by the sample:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(log10(count + 1))) +\n  geom_density() +\n  facet_wrap(. ~ sample, nrow = 3)\n\n\n\n\n\n\n\nThe key information to take from these plots is:\n\nthe distributions are roughly similar\nthe peak at zero suggests quite a few counts of 1.\nwe would expect the distribution of counts in each sample to be roughly log normal so that the small rise near the low end, even before the peak at zero, suggests that these lower counts might be anomalies.\n\nWe have found the distribution across samples to be similar to that over all. This is good because it means that the samples are fairly consistent with each other. We can now move on to the next step.\nDistribution of values across the genes\nThere are lots of genes in this dataset therefore we will take a slightly different approach. We would not want to use plot a distribution for each gene in the same way. Will pivot the data to tidy and then summarise the counts for each gene.\nğŸ¬ Summarise the counts for each gene and save the result as pro_meta_summary_gene. Include the same columns as we had in the by sample summary (pro_meta_summary_samp) and an additional column, total for the total number of counts for each gene.\nğŸ¬ View the pro_meta_summary_gene dataframe.\nNotice that we have:\n\na lot of genes with counts of zero in every sample\na lot of genes with zero counts in several of the samples\nsome very very low counts.\n\nGenes with very low counts should be filtered out because they are unreliable - or, at the least, uninformative. The goal of our downstream analysis will be to see if there is a significant difference in gene expression between the stages. Since we have only three replicates in each group, having one or two unreliable, missing or zero values, makes such a determination impossible for a particular gene. We will use the total counts (total) and the number of samples with non-zero values (n_above_zero) in this dataframe to filter our genes later.\nAs we have a lot of genes, it is helpful to plot the mean counts with geom_pointrange() to get an overview of the distributions. We will again plot the log of the mean counts. We will also order the genes from lowest to highest mean count.\nğŸ¬ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\npro_meta_summary_gene |&gt; \n  ggplot(aes(x = reorder(gene_id, mean), y = log10(mean))) +\n  geom_pointrange(aes(ymin = log10(mean - sd), \n                      ymax = log10(mean + sd )),\n                  size = 0.1)\n\n\n\n\n\n\n\n(Note the warning is expected since we have zero means).\nYou can see we also have quite a few genes with means less than 1 (log below zero). Note that the variability between genes (average counts between 0 and 349606) is far greater than between samples (average counts from 220 to 364) which is exactly what we would expect to see.\nNow go to Filtering for QC.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#stem-cells",
    "href": "transcriptomics/week-3/workshop.html#stem-cells",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nImport\nImport the data for the HSPC and the Progenitor cells.\nğŸ¬ Import secretome_hspc.csv and secretome_prog.csv\n\n# ğŸ­ import the hspc and prog data\nhspc  &lt;- readr::read_csv(\"data-raw/secretome_hspc.csv\")\nprog  &lt;- readr::read_csv(\"data-raw/secretome_prog.csv\")\n\nWe will need to combine the two sets of columns (datasets) so we can compare the two stages. We will join them using ensembl_gene_id to match the rows. The column names differ so we donâ€™t need to worry about renaming any of them.\nğŸ¬ Combine the two datasets by ensembl_gene_id and save the result as hspc_prog.\n\n#  combine the two datasets\nhspc_prog &lt;- hspc |&gt;\n  left_join(prog, \n            by = \"ensembl_gene_id\")\n\nğŸ¬ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in â€˜tidyâ€™ format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nğŸ¬ Pivot the counts (stack the columns) so all the counts are in a single column (expr) labelled in cell by the column it came from and pipe into ggplot() to create a histogram:\n\nhspc_prog |&gt;\n  pivot_longer(cols = -ensembl_gene_id,\n               names_to = \"cell\",\n               values_to = \"expr\") |&gt; \n  ggplot(aes(x = expr)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis is a very striking distribution. Is it what we are expecting? Notice we have a peak at zero indicating there are low values zeros. This inflation of low values suggests some are anomalous - they will have been derived from low counts which are likely false positives. As inaccurate measures, we will want to exclude expression values below (about) 1. We will revisit this after we have considered the distribution of expression across cells and genes.\nWhat about the bimodal appearance of the the â€˜realâ€™ values? If we had the whole transcriptome we would not expect to see such a pattern - weâ€™d expect to see a roughly normal distribution3. However, this is a subset of the genome and the nature of the subsetting has had an influence here. These are a subset of cell surface proteins that show a significant difference between at least two of twelve cell subtypes. That is, all of these genes are either â€œhighâ€ or â€œlowâ€ leading to a bimodal distribution.\nUnlike the other three datasets, which count raw counts, these data are normalised and log2 transformed. We do not need to plot the log of the values to see the distribution - they are already logged.\nDistribution of values across the samples\nFor the other three datasets, we used the summary() function to get an overview of the columns. This works well when you have upto about 25 columns but it is not helpful here because we have a lot of cells! Feel free to try it!\nIn this data set, there is even more of an advantage of using the pivot_longer(), group_by() and summarise() approach. We will be able to open the dataframe in the Viewer and make plots to examine whether the distributions are similar across cells. The mean and the standard deviation are useful to see the distributions across cells in a plot but we will also examine the interquartile values, maximums and the number of non-zero values.\nğŸ¬ Summarise all the cells:\n\nhspc_prog_summary_cell &lt;- hspc_prog |&gt;\n  pivot_longer(cols = -ensembl_gene_id,\n               names_to = \"cell\",\n               values_to = \"expr\") |&gt;\n  group_by(cell) |&gt;\n  summarise(min = min(expr),\n            lowerq = quantile(expr, 0.25),\n            sd = sd(expr),\n            mean = mean(expr),\n            median = median(expr),\n            upperq = quantile(expr, 0.75),\n            max = max(expr),\n            total = sum(expr),\n            n_above_zero = sum(expr &gt; 0))\n\nğŸ¬ View the hspc_prog_summary_cell dataframe (click on it in the environment).\nNotice that: - a minimum value of 0 appears in all 1499 cells - the lower quartiles are all zero and so are many of the medians - there are no cells with above 0 expression in all 423 of the gene subset - the highest number of genes expressed is 287, the lowest is 139\nIn short, there are quite a lot of zeros.\nTo get a better understanding of the distribution of expressions in cells we can create a ggplot using the pointrange geom. Pointrange puts a dot at the mean and a line between a minimum and a maximum such as +/- one standard deviation. Not unlike a boxplot, but when you need the boxes too be very narrow!\nğŸ¬ Create a pointrange plot.\n\nhspc_prog_summary_cell |&gt; \n  ggplot(aes(x = cell, y = mean)) +\n  geom_pointrange(aes(ymin = mean - sd, \n                      ymax = mean + sd ),\n                  size = 0.1)\n\n\n\n\n\n\n\nYou will need to use the Zoom button to pop the plot window out so you can make it as wide as possible\nThe things to notice are:\n\nthe average expression in cells is similar for all cells. This is good to know - if some cells had much lower expression perhaps there is something wrong with them, or their sequencing, and they should be excluded.\nthe distributions are roughly similar in width too\n\nThe default order of cell is alphabetical. It can be easier to judge if there are unusual cells if we order the lines by the size of the mean.\nğŸ¬ Order a pointrange plot with reorder(variable_to_order, order_by).\n\nhspc_prog_summary_cell |&gt; \n  ggplot(aes(x = reorder(cell, mean), y = mean)) +\n  geom_pointrange(aes(ymin = mean - sd, \n                      ymax = mean + sd ),\n                  size = 0.1)\n\n\n\n\n\n\n\nreorder() arranges cell in increasing size of mean\nAs we thought, the distributions are similar across cells - there are not any cells that are obviously different from the others (only incrementally).\nDistribution of values across the genes\nWe will use the same approach to summarise the genes.\nğŸ¬ Summarise the expression for each gene and save the result as hspc_prog_summary_gene. Include the same columns as we had in the by cell summary (hspc_prog_summary_cell) and an additional column, total for the total expression for each gene.\nğŸ¬ View the hspc_prog_summary_gene dataframe. Remember these are normalised and logged (base 2) so we should not see very large values.\nNotice that:\n\nsome genes are expressed in every cell, and many are expressed in most cells\nquite a few genes are zero in many cells. This this matters less when we have many cells (samples) than when we have few samples.\nno genes have zeros in every cell - the lowest number of cells is is 8\n\nIt is again helpful to plot the ordered mean expression with pointrange to get an overview.\nğŸ¬ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\nhspc_prog_summary_gene |&gt; \n  ggplot(aes(x = reorder(ensembl_gene_id, mean), y = mean)) +\n  geom_pointrange(aes(ymin = mean - sd, \n                      ymax = mean + sd),\n                  size = 0.1)\n\n\n\n\n\n\n\nNote that the variability between genes (average expression between 0.020 and and 9.567) is far greater than between cells (average expression from 1.319 to 9.567) which is just what we would expect.\nNow go to Filtering for QC.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#arabidopsis-filterqc",
    "href": "transcriptomics/week-3/workshop.html#arabidopsis-filterqc",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis\n",
    "text": "ğŸ„ Arabidopsis\n\nOur samples look to be similarly well sequenced although this is difficult to determine with only two replicates. However, some genes are not expressed or the expression values are so low in for a gene that they are uninformative. We will filter the root_summary_gene dataframe to obtain a list of gene_id we can use to filter root.\nMy suggestion is to include only the genes with counts in at least 6 samples, and those with total counts above 20. I chose 6 because that would keep genes expressed only in one treatment. I chose 20 based on\nğŸ¬ Filter the summary by gene dataframe:\n\nroot_summary_gene_filtered &lt;- root_summary_gene |&gt; \n  filter(total &gt; 20) |&gt; \n  filter(n_above_zero &gt;= 6)\n\nâ“ How many genes do you have left\n\n\n\nğŸ¬ Use the list of gene_id in the filtered summary to filter the original dataset:\n\nroot_filtered &lt;- root |&gt; \n  filter(gene_id %in%  root_summary_gene_filtered$gene_id)\n\nğŸ¬ Write the filtered data to file:\n\nwrite_csv(root_filtered, \n          file = \"data-processed/root_filtered.csv\")\n\nNow go to Look after future you",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#leishmania-filterqc",
    "href": "transcriptomics/week-3/workshop.html#leishmania-filterqc",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nOur samples look to be similarly well sequenced. There are no samples we should remove. However, some genes are not expressed or the expression values are so low in for a gene that they are uninformative. We will filter the pro_meta_summary_gene dataframe to obtain a list of gene_id we can use to filter pro_meta.\nMy suggestion is to include only the genes with counts in at least 3 samples and those with total counts above 20. I chose 3 because that would keep genes expressed only in one treatment: [0, 0, 0] [#,#,#]. This is a difference we cannot test statistically, but which matters biologically.\nğŸ¬ Filter the summary by gene dataframe:\n\npro_meta_summary_gene_filtered &lt;- pro_meta_summary_gene |&gt; \n  filter(total &gt; 20) |&gt; \n  filter(n_above_zero &gt;= 3)\n\nâ“ How many genes do you have left\n\n\n\nğŸ¬ Use the list of gene_id in the filtered summary to filter the original dataset:\n\npro_meta_filtered &lt;- pro_meta |&gt; \n  filter(gene_id %in%  pro_meta_summary_gene_filtered$gene_id)\n\nğŸ¬ Write the filtered data to file:\n\nwrite_csv(pro_meta_filtered, \n          file = \"data-processed/pro_meta_filtered.csv\")\n\nNow go to Look after future you",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#stem-cells-filterqc",
    "href": "transcriptomics/week-3/workshop.html#stem-cells-filterqc",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nIn this dataset, we will not see and genes that are not expressed in any of the cells because we are using a specific subset of the transcriptome that was deliberately selected. In other words unexpressed genes have already been filtered out. However, it is good practices to verify there are no unexpressed genes before we embark on our analysis.\nWhere the sum of all the values in the rows is zero, all the entries must be zero. We can use this to find any the genes that are not expressed in any of the cells. To do row wise aggregates such as the sum across rows we can use the rowwise() function. c_across() allows us to use the colon notation to select columns. This is very useful when you have a lot of columns because it would be annoying to have to list all of them HSPC_001:Prog_852 means all the columns from HSPC_001 to Prog_852.\nğŸ¬ Find the genes that are 0 in every column of the hspc_prog dataframe:\n\nhspc_prog |&gt; \n  rowwise() |&gt; \n  filter(sum(c_across(HSPC_001:Prog_852)) == 0)\n\n# A tibble: 0 Ã— 1,500\n# Rowwise: \n# â„¹ 1,500 variables: ensembl_gene_id &lt;chr&gt;, HSPC_001 &lt;dbl&gt;, HSPC_002 &lt;dbl&gt;,\n#   HSPC_003 &lt;dbl&gt;, HSPC_004 &lt;dbl&gt;, HSPC_006 &lt;dbl&gt;, HSPC_008 &lt;dbl&gt;,\n#   HSPC_009 &lt;dbl&gt;, HSPC_011 &lt;dbl&gt;, HSPC_012 &lt;dbl&gt;, HSPC_014 &lt;dbl&gt;,\n#   HSPC_015 &lt;dbl&gt;, HSPC_016 &lt;dbl&gt;, HSPC_017 &lt;dbl&gt;, HSPC_018 &lt;dbl&gt;,\n#   HSPC_020 &lt;dbl&gt;, HSPC_021 &lt;dbl&gt;, HSPC_022 &lt;dbl&gt;, HSPC_023 &lt;dbl&gt;,\n#   HSPC_024 &lt;dbl&gt;, HSPC_025 &lt;dbl&gt;, HSPC_026 &lt;dbl&gt;, HSPC_027 &lt;dbl&gt;,\n#   HSPC_028 &lt;dbl&gt;, HSPC_030 &lt;dbl&gt;, HSPC_031 &lt;dbl&gt;, HSPC_033 &lt;dbl&gt;, â€¦\n\n\nNotice that we have summed across all the columns.\nâ“ What do you conclude?\n\n\n\nğŸ¬ Write combined data to file:\n\nwrite_csv(hspc_prog, \n          file = \"data-processed/hspc_prog.csv\")\n\nNow go to Look after future you",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#arabidopsis-and-future-you",
    "href": "transcriptomics/week-3/workshop.html#arabidopsis-and-future-you",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis and future you",
    "text": "ğŸ„ Arabidopsis and future you\nğŸ¬ Create a new Project, arab-88H, populated with folders and your data. Make a script file called cont-low-root.R. This will a be commented analysis of comparison between control and low Ni root tissue. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#leishmania-and-future-you",
    "href": "transcriptomics/week-3/workshop.html#leishmania-and-future-you",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania and future you",
    "text": "ğŸ’‰ Leishmania and future you\nğŸ¬ Create a new Project, leish-88H, populated with folders and your data. Make a script file called pro_meta.R. This will a be commented analysis of comparison procyclic promastigote and metacyclic promastigotes. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#stem-cells-and-future-you",
    "href": "transcriptomics/week-3/workshop.html#stem-cells-and-future-you",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells and future you",
    "text": "ğŸ­ Stem cells and future you\nğŸ¬ Create a new Project, mice-88H, populated with folders and your data. Make a script file called hspc-prog.R. This will a be commented analysis of the hspc cells vs the prog cells. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#footnotes",
    "href": "transcriptomics/week-3/workshop.html#footnotes",
    "title": "Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.â†©ï¸\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.â†©ï¸\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.â†©ï¸",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_after_workshop.html",
    "href": "transcriptomics/week-3/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "You need only do the section for your own project data",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_after_workshop.html#arabidopisis",
    "href": "transcriptomics/week-3/study_after_workshop.html#arabidopisis",
    "title": "Independent Study to consolidate this week",
    "section": "ğŸ„ Arabidopisis",
    "text": "ğŸ„ Arabidopisis\nğŸ¬ Open your arab-88H Project. Make a new script, cont-low-aerial.R, and, using cont-low-root.R as a template, repeat the analysis on the aerial tissue.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_after_workshop.html#leishmania",
    "href": "transcriptomics/week-3/study_after_workshop.html#leishmania",
    "title": "Independent Study to consolidate this week",
    "section": "ğŸ’‰ Leishmania",
    "text": "ğŸ’‰ Leishmania\nğŸ¬ Open your leish-88H Project. Make a new script, pro_ama.R, and, using pro_meta.R as a template, repeat the analysis on the procyclic promastigotes (pro) and amastigotes (ama).",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_after_workshop.html#stem-cells",
    "href": "transcriptomics/week-3/study_after_workshop.html#stem-cells",
    "title": "Independent Study to consolidate this week",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nğŸ¬ Open your mice-88H Project. Make a new script, hspc-lthsc.R and, using hspc-prog.R as a template, repeat the analysis on the HSPC and LT-HSC cells.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html",
    "href": "transcriptomics/week-3/workshop-orig.html",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop you will learn what steps to take to get a good understanding of your transcriptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It gives you the deep understanding of the data structures and values that you will need to code and trouble-shoot code, allows you to spot failed or problematic samples and informs your decisions on quality control.\nIn this session, you should examine all three data sets because the comparisons will give you a much stronger understanding of your own project data. Compare and contrast is a very useful way to build understanding."
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#session-overview",
    "href": "transcriptomics/week-3/workshop-orig.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop you will learn what steps to take to get a good understanding of your transcriptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It gives you the deep understanding of the data structures and values that you will need to code and trouble-shoot code, allows you to spot failed or problematic samples and informs your decisions on quality control.\nIn this session, you should examine all three data sets because the comparisons will give you a much stronger understanding of your own project data. Compare and contrast is a very useful way to build understanding."
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#arabidopsis",
    "href": "transcriptomics/week-3/workshop-orig.html#arabidopsis",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis\n",
    "text": "ğŸ„ Arabidopsis\n\nImport\nImport the data for root tissue.\nğŸ¬ Import arabidopsis-root.csv\n\n# ğŸ„ import the root data\nroot &lt;- read_csv(\"data-raw/arabidopsis-root.csv\")\n\nğŸ¬ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in â€˜tidyâ€™ format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nğŸ¬ Pivot the counts (stack the columns) so all the counts are in a single column (count) labelled in sample by the column it came from and pipe into ggplot() to create a histogram:\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = count)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis data is very skewed - there are very many low counts and a very few higher numbers. It is hard to see the very low bars for the higher values. Logging the counts is a way to make the distribution more visible. You cannot take the log of 0 so we add 1 to the count before logging. The log of 1 is zero so we will be able to see how many zeros we had.\nğŸ¬ Repeat the plot of log of the counts.\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = log10(count + 1))) +\n  geom_histogram()\n\n\n\n\n\n\n\nIâ€™ve used base 10 only because it easy to convert to the original scale (1 is 10, 2 is 100, 3 is 1000 etc). Notice we have a peak at zero indicating there are many zeros. We would expect the distribution of counts to be roughly log normal because this is expression of all the genes in the genome1. The number of low counts is inflated (small peak near the low end). This suggests that these lower counts might be false positives. The removal of low counts is a common processing step in â€™omic data. We will revisit this after we have considered the distribution of counts across samples and genes.\nDistribution of values across the samples\nSummary statistics including the the number of NAs can be seen using the summary(). It is most helpful which you have up to about 25 columns. There is nothing special about the number 25, it is just that summaries of a larger number of columns are difficult to grasp.\nğŸ¬ Get a quick overview of the 14 columns:\n\n# examine all the columns quickly\n# works well with smaller numbers of column\nsummary(root)\n\n   gene_id           gene_name              CTR1               CTR2         \n Length:32833       Length:32833       Min.   :     0.0   Min.   :     0.0  \n Class :character   Class :character   1st Qu.:     0.0   1st Qu.:     0.0  \n Mode  :character   Mode  :character   Median :   100.0   Median :   131.0  \n                                       Mean   :   705.9   Mean   :   930.5  \n                                       3rd Qu.:   609.0   3rd Qu.:   811.0  \n                                       Max.   :133872.0   Max.   :256188.0  \n      CTR3               CTR4               CTR5               CTR6         \n Min.   :     0.0   Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n 1st Qu.:     0.0   1st Qu.:     0.0   1st Qu.:     0.0   1st Qu.:     0.0  \n Median :    94.0   Median :   114.0   Median :    97.0   Median :    95.0  \n Mean   :   715.2   Mean   :   786.5   Mean   :   731.4   Mean   :   703.7  \n 3rd Qu.:   609.0   3rd Qu.:   686.0   3rd Qu.:   623.0   3rd Qu.:   608.0  \n Max.   :137167.0   Max.   :121160.0   Max.   :168910.0   Max.   :134192.0  \n      LWR1               LWR2               LWR3               LWR4         \n Min.   :     0.0   Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n 1st Qu.:     0.0   1st Qu.:     0.0   1st Qu.:     0.0   1st Qu.:     0.0  \n Median :    81.0   Median :    77.0   Median :    78.0   Median :    80.0  \n Mean   :   730.1   Mean   :   622.2   Mean   :   647.1   Mean   :   662.3  \n 3rd Qu.:   606.0   3rd Qu.:   524.0   3rd Qu.:   538.0   3rd Qu.:   566.0  \n Max.   :183421.0   Max.   :139241.0   Max.   :121856.0   Max.   :139044.0  \n      LWR5               LWR6         \n Min.   :     0.0   Min.   :     0.0  \n 1st Qu.:     0.0   1st Qu.:     0.0  \n Median :    93.0   Median :    89.0  \n Mean   :   739.7   Mean   :   742.8  \n 3rd Qu.:   645.0   3rd Qu.:   625.0  \n Max.   :161872.0   Max.   :179613.0  \n\n\nNotice that:\n\nthe minimum count is 0 and the maximums are very high in all the columns\nthe medians are quite a lot lower than the means so the data are skewed (hump to the left, tail to the right) and there must be quite a lot of zeros\n\nWe want to know how many zeros there are in each a column. To achieve this, we can make use of the fact that TRUE evaluates to 1 and FALSE evaluates to 0. Consequently, summing a column of TRUE/FALSE values will give you the number of TRUE values. For example, sum(CTR1 &gt; 0) gives the number of values above zero in the CTR1 column. If you wanted the number of zeros, you could use sum(CTR1 == 0).\nğŸ¬ Find the number values above zero in all six columns:\n\nroot |&gt;\n  summarise(sum(CTR1 &gt; 0),\n            sum(CTR2 &gt; 0),\n            sum(CTR3 &gt; 0),\n            sum(CTR4 &gt; 0),\n            sum(CTR5 &gt; 0),\n            sum(CTR6 &gt; 0),\n            sum(LWR1 &gt; 0),\n            sum(LWR2 &gt; 0),\n            sum(LWR3 &gt; 0),\n            sum(LWR4 &gt; 0),\n            sum(LWR5 &gt; 0),\n            sum(LWR6 &gt; 0))\n\n# A tibble: 1 Ã— 12\n  `sum(CTR1 &gt; 0)` `sum(CTR2 &gt; 0)` `sum(CTR3 &gt; 0)` `sum(CTR4 &gt; 0)`\n            &lt;int&gt;           &lt;int&gt;           &lt;int&gt;           &lt;int&gt;\n1           23119           23378           22789           23153\n# â„¹ 8 more variables: `sum(CTR5 &gt; 0)` &lt;int&gt;, `sum(CTR6 &gt; 0)` &lt;int&gt;,\n#   `sum(LWR1 &gt; 0)` &lt;int&gt;, `sum(LWR2 &gt; 0)` &lt;int&gt;, `sum(LWR3 &gt; 0)` &lt;int&gt;,\n#   `sum(LWR4 &gt; 0)` &lt;int&gt;, `sum(LWR5 &gt; 0)` &lt;int&gt;, `sum(LWR6 &gt; 0)` &lt;int&gt;\n\n\nThere is a better way of doing this that saves you having to repeat so much code - very useful if you have a lot more than 6 columns! We can use pivot_longer() to put the data in tidy format and then use the group_by() and summarise() approach we have used extensively before.\nğŸ¬ Find the number of zeros in all columns:\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(n_above_zero = sum(count &gt; 0))\n\n# A tibble: 12 Ã— 2\n   sample n_above_zero\n   &lt;chr&gt;         &lt;int&gt;\n 1 CTR1          23119\n 2 CTR2          23378\n 3 CTR3          22789\n 4 CTR4          23153\n 5 CTR5          22941\n 6 CTR6          22982\n 7 LWR1          23103\n 8 LWR2          23111\n 9 LWR3          22955\n10 LWR4          22852\n11 LWR5          23122\n12 LWR6          23034\n\n\nYou could expand this code to get get other useful summary information\nğŸ¬ Summarise all the samples:\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(min = min(count),\n            lowerq = quantile(count, 0.25),\n            mean = mean(count),\n            median = median(count),\n            upperq = quantile(count, 0.75),\n            max = max(count),\n            n_above_zero = sum(count &gt; 0))\n\n# A tibble: 12 Ã— 8\n   sample   min lowerq  mean median upperq    max n_above_zero\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;int&gt;\n 1 CTR1       0      0  706.    100    609 133872        23119\n 2 CTR2       0      0  931.    131    811 256188        23378\n 3 CTR3       0      0  715.     94    609 137167        22789\n 4 CTR4       0      0  786.    114    686 121160        23153\n 5 CTR5       0      0  731.     97    623 168910        22941\n 6 CTR6       0      0  704.     95    608 134192        22982\n 7 LWR1       0      0  730.     81    606 183421        23103\n 8 LWR2       0      0  622.     77    524 139241        23111\n 9 LWR3       0      0  647.     78    538 121856        22955\n10 LWR4       0      0  662.     80    566 139044        22852\n11 LWR5       0      0  740.     93    645 161872        23122\n12 LWR6       0      0  743.     89    625 179613        23034\n\n\nThe mean count ranges from 704 to 931. CTR2 stands out a little - having a higher mean and maximum than the others. When we have a good number of replicates â€“ 6 is good in these experiments â€“ this is unlikely to be a problem. The potential effect of having an odd replicate when you have only two or three replicates, is reduced statistical power. Differences between genes with lower average expression and or more variable expression might be missed. Whether this matters depends on the biological question you are asking. In this case, it does not matter because a) we have 6 replicates and b) because the major differences in gene expression will be enough.\nğŸ¬ Save the summary as a dataframe, root_summary_samp (using assignment).\nWe can also plot the distribution of counts across samples. We have many values (32833) so we are not limited to using geom_histogram(). geom_density() gives us a smooth distribution.\nğŸ¬ Plot the log10 of the counts + 1 again but this time facet by the sample:\n\nroot |&gt;\n  pivot_longer(cols = c(-gene_id, -gene_name),\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(log10(count + 1))) +\n  geom_density() +\n  facet_wrap(. ~ sample, nrow = 3)\n\n\n\n\n\n\n\nThe key information to take from these plots is:\n\nthe peak at zero suggests quite a few counts of 1.\nwe would expect the distribution of counts in each sample to be roughly log normal so that the rise near the low end, even before the peak at zero, suggests that these lower counts might be anomalies.\n\nWe have found the distribution across samples to be similar to that over all. This is good because it means that the samples are fairly consistent with each other. We can now move on to the next step.\nDistribution of values across the genes\nThere are lots of genes in this dataset therefore we will take a slightly different approach. We would not want to use plot a distribution for each gene in the same way. Will pivot the data to tidy and then summarise the counts for each gene.\nğŸ¬ Summarise the counts for each gene and save the result as root_summary_gene. Include the same columns as we had in the by sample summary (root_summary_samp) and an additional column, total for the total number of counts for each gene.\nğŸ¬ View the root_summary_gene dataframe.\nNotice that we have:\n\na lot of genes with counts of zero in every sample\na lot of genes with zero counts in several of the samples\nsome very very low counts.\n\nGenes with very low counts should be filtered out because they are unreliable - or, at the least, uninformative. The goal of our downstream analysis will be to see if there is a significant difference in gene expression between the control and FGF-treated sibling. Since we have only three replicates in each group, having one or two unreliable, missing or zero values, makes such a determination impossible for a particular gene. We will use the total counts (total) and the number of samples with non-zero values (n_above_zero) in this dataframe to filter our genes later.\nAs we have a lot of genes, it is helpful to plot the mean counts with geom_pointrange() to get an overview of the distributions. We will again plot the log of the mean counts. We will also order the genes from lowest to highest mean count.\nğŸ¬ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\nroot_summary_gene |&gt; \n  ggplot(aes(x = reorder(gene_id, mean), y = log10(mean))) +\n  geom_pointrange(aes(ymin = log10(mean - sd), \n                      ymax = log10(mean + sd )),\n                  size = 0.1)\n\n\n\n\n\n\n\n(Note the warning is expected since we have zero means).\nYou can see we also have quite a few genes with means less than 1 (log below zero). Note that the variability between genes (average counts between 0 and 43348) is far greater than between samples (average counts from 70 to 296) which is exactly what we would expect to see.\nNow go to Filtering for QC."
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#leishmania",
    "href": "transcriptomics/week-3/workshop-orig.html#leishmania",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nImport\nImport the data for L.mexicana procyclic promastigote (pro) and the metacyclic promastigotes (meta)\nğŸ¬ Import leishmania-mex-pro.csv and leishmania-mex-meta.csv\n\n# ğŸ’‰ import the pro and meta leish data\npro &lt;- read_csv(\"data-raw/leishmania-mex-pro.csv\")\nmeta &lt;- read_csv(\"data-raw/leishmania-mex-meta.csv\")\n\nWe will need to combine the two sets of columns (datasets) so we can compare the two stages. We will join them using gene_id to match the rows. The column names differ so we donâ€™t need to worry about renaming any of them.\nğŸ¬ Combine the two datasets by gene_id and save the result as pro_meta.\n\n#  combine the two datasets\npro_meta &lt;- pro |&gt;\n  left_join(meta, \n            by = \"gene_id\")\n\nğŸ¬ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in â€˜tidyâ€™ format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nğŸ¬ Pivot the counts (stack the columns) so all the counts are in a single column (count) labelled in sample by the column it came from and pipe into ggplot() to create a histogram:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = count)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis data is very skewed - there are very many low counts and a very few higher numbers. It is hard to see the very low bars for the higher values. Logging the counts is a way to make the distribution more visible. You cannot take the log of 0 so we add 1 to the count before logging. The log of 1 is zero so we will be able to see how many zeros we had.\nğŸ¬ Repeat the plot of log of the counts.\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = log10(count + 1))) +\n  geom_histogram()\n\n\n\n\n\n\n\nIâ€™ve used base 10 only because it easy to convert to the original scale (1 is 10, 2 is 100, 3 is 1000 etc). Notice we have a peak at zero indicating there are many zeros. We would expect the distribution of counts to be roughly log normal because this is expression of all the genes in the genome2. The number of low counts is inflated (small peak near the low end). This suggests that these lower counts might be false positives. The removal of low counts is a common processing step in â€™omic data. We will revisit this after we have considered the distribution of counts across samples and genes.\nDistribution of values across the samples\nSummary statistics including the the number of NAs can be seen using the summary(). It is most helpful which you have up to about 25 columns. There is nothing special about the number 25, it is just that summaries of a larger number of columns are difficult to grasp.\nğŸ¬ Get a quick overview of the 7 columns:\n\n# examine all the columns quickly\n# works well with smaller numbers of column\nsummary(pro_meta)\n\n   gene_id             lm_pro_1           lm_pro_2           lm_pro_3       \n Length:8677        Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n Class :character   1st Qu.:    77.0   1st Qu.:    53.0   1st Qu.:    59.0  \n Mode  :character   Median :   191.0   Median :   135.0   Median :   145.0  \n                    Mean   :   364.5   Mean   :   255.7   Mean   :   281.4  \n                    3rd Qu.:   332.0   3rd Qu.:   238.0   3rd Qu.:   256.0  \n                    Max.   :442477.0   Max.   :295423.0   Max.   :411663.0  \n   lm_meta_1          lm_meta_2          lm_meta_3       \n Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n 1st Qu.:    48.0   1st Qu.:    51.0   1st Qu.:    78.0  \n Median :   110.0   Median :   120.0   Median :   187.0  \n Mean   :   220.3   Mean   :   221.9   Mean   :   355.9  \n 3rd Qu.:   197.0   3rd Qu.:   215.0   3rd Qu.:   341.0  \n Max.   :244569.0   Max.   :205203.0   Max.   :498303.0  \n\n\nNotice that:\n\nthe minimum count is 0 and the maximums are very high in all the columns\nthe medians are quite a lot lower than the means so the data are skewed (hump to the left, tail to the right) and there must be quite a lot of zeros\n\nWe want to know how many zeros there are in each a column. To achieve this, we can make use of the fact that TRUE evaluates to 1 and FALSE evaluates to 0. Consequently, summing a column of TRUE/FALSE values will give you the number of TRUE values. For example, sum(lm_pro_1 &gt; 0) gives the number of values above zero in the lm_pro_1 column. If you wanted the number of zeros, you could use sum(lm_pro_1 == 0).\nğŸ¬ Find the number values above zero in all six columns:\n\npro_meta |&gt;\n  summarise(sum(lm_pro_1 &gt; 0),\n            sum(lm_pro_2 &gt; 0),\n            sum(lm_pro_3 &gt; 0),\n            sum(lm_meta_1 &gt; 0),\n            sum(lm_meta_2 &gt; 0),\n            sum(lm_meta_3 &gt; 0))\n\n# A tibble: 1 Ã— 6\n  `sum(lm_pro_1 &gt; 0)` `sum(lm_pro_2 &gt; 0)` `sum(lm_pro_3 &gt; 0)`\n                &lt;int&gt;               &lt;int&gt;               &lt;int&gt;\n1                8549                8522                8509\n# â„¹ 3 more variables: `sum(lm_meta_1 &gt; 0)` &lt;int&gt;, `sum(lm_meta_2 &gt; 0)` &lt;int&gt;,\n#   `sum(lm_meta_3 &gt; 0)` &lt;int&gt;\n\n\nThere is a better way of doing this that saves you having to repeat so much code - very useful if you have a lot more than 6 columns! We can use pivot_longer() to put the data in tidy format and then use the group_by() and summarise() approach we have used extensively before.\nğŸ¬ Find the number of zeros in all columns:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(n_above_zero = sum(count &gt; 0))\n\n# A tibble: 6 Ã— 2\n  sample    n_above_zero\n  &lt;chr&gt;            &lt;int&gt;\n1 lm_meta_1         8535\n2 lm_meta_2         8535\n3 lm_meta_3         8530\n4 lm_pro_1          8549\n5 lm_pro_2          8522\n6 lm_pro_3          8509\n\n\nYou could expand this code to get get other useful summary information\nğŸ¬ Summarise all the samples:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(min = min(count),\n            lowerq = quantile(count, 0.25),\n            mean = mean(count),\n            median = median(count),\n            upperq = quantile(count, 0.75),\n            max = max(count),\n            n_above_zero = sum(count &gt; 0))\n\n# A tibble: 6 Ã— 8\n  sample      min lowerq  mean median upperq    max n_above_zero\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;int&gt;\n1 lm_meta_1     0     48  220.    110    197 244569         8535\n2 lm_meta_2     0     51  222.    120    215 205203         8535\n3 lm_meta_3     0     78  356.    187    341 498303         8530\n4 lm_pro_1      0     77  364.    191    332 442477         8549\n5 lm_pro_2      0     53  256.    135    238 295423         8522\n6 lm_pro_3      0     59  281.    145    256 411663         8509\n\n\nThe mean count ranges from 220 to 364. We do not appear to have any outlying (odd) replicates. The potential effect of an odd replicate is reduced statistical power. Major differences in gene expression will still be uncovered. Differences between genes with lower average expression and or more variable expression might be missed. Whether this matters depends on the biological question you are asking.\nğŸ¬ Save the summary as a dataframe, pro_meta_summary_samp (using assignment).\nWe can also plot the distribution of counts across samples. We have many values (8677) so we are not limited to using geom_histogram(). geom_density() gives us a smooth distribution.\nğŸ¬ Plot the log10 of the counts + 1 again but this time facet by the sample:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(log10(count + 1))) +\n  geom_density() +\n  facet_wrap(. ~ sample, nrow = 3)\n\n\n\n\n\n\n\nThe key information to take from these plots is:\n\nthe distributions are roughly similar\nthe peak at zero suggests quite a few counts of 1.\nwe would expect the distribution of counts in each sample to be roughly log normal so that the small rise near the low end, even before the peak at zero, suggests that these lower counts might be anomalies.\n\nWe have found the distribution across samples to be similar to that over all. This is good because it means that the samples are fairly consistent with each other. We can now move on to the next step.\nDistribution of values across the genes\nThere are lots of genes in this dataset therefore we will take a slightly different approach. We would not want to use plot a distribution for each gene in the same way. Will pivot the data to tidy and then summarise the counts for each gene.\nğŸ¬ Summarise the counts for each gene and save the result as pro_meta_summary_gene. Include the same columns as we had in the by sample summary (pro_meta_summary_samp) and an additional column, total for the total number of counts for each gene.\nğŸ¬ View the pro_meta_summary_gene dataframe.\nNotice that we have:\n\na lot of genes with counts of zero in every sample\na lot of genes with zero counts in several of the samples\nsome very very low counts.\n\nGenes with very low counts should be filtered out because they are unreliable - or, at the least, uninformative. The goal of our downstream analysis will be to see if there is a significant difference in gene expression between the stages. Since we have only three replicates in each group, having one or two unreliable, missing or zero values, makes such a determination impossible for a particular gene. We will use the total counts (total) and the number of samples with non-zero values (n_above_zero) in this dataframe to filter our genes later.\nAs we have a lot of genes, it is helpful to plot the mean counts with geom_pointrange() to get an overview of the distributions. We will again plot the log of the mean counts. We will also order the genes from lowest to highest mean count.\nğŸ¬ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\npro_meta_summary_gene |&gt; \n  ggplot(aes(x = reorder(gene_id, mean), y = log10(mean))) +\n  geom_pointrange(aes(ymin = log10(mean - sd), \n                      ymax = log10(mean + sd )),\n                  size = 0.1)\n\n\n\n\n\n\n\n(Note the warning is expected since we have zero means).\nYou can see we also have quite a few genes with means less than 1 (log below zero). Note that the variability between genes (average counts between 0 and 349606) is far greater than between samples (average counts from 220 to 364) which is exactly what we would expect to see.\nNow go to Filtering for QC."
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#stem-cells",
    "href": "transcriptomics/week-3/workshop-orig.html#stem-cells",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nImport\nImport the data for the HSPC and the Progenitor cells.\nğŸ¬ Import surfaceome_hspc.csv and surfaceome_hspc.csv\n\n# ğŸ­ import the hspc and prog data\nhspc &lt;- read_csv(\"data-raw/surfaceome_hspc.csv\")\nprog &lt;- read_csv(\"data-raw/surfaceome_prog.csv\")\n\nWe will need to combine the two sets of columns (datasets) so we can compare the two stages. We will join them using ensembl_gene_id to match the rows. The column names differ so we donâ€™t need to worry about renaming any of them.\nğŸ¬ Combine the two datasets by ensembl_gene_id and save the result as hspc_prog.\n\n#  combine the two datasets\nhspc_prog &lt;- hspc |&gt;\n  left_join(prog, \n            by = \"ensembl_gene_id\")\n\nğŸ¬ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in â€˜tidyâ€™ format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nğŸ¬ Pivot the counts (stack the columns) so all the counts are in a single column (expr) labelled in cell by the column it came from and pipe into ggplot() to create a histogram:\n\nhspc_prog |&gt;\n  pivot_longer(cols = -ensembl_gene_id,\n               names_to = \"cell\",\n               values_to = \"expr\") |&gt; \n  ggplot(aes(x = expr)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis is a very striking distribution. Is it what we are expecting? Notice we have a peak at zero indicating there are low values zeros. This inflation of low values suggests some are anomalous - they will have been derived from low counts which are likely false positives. As inaccurate measures, we will want to exclude expression values below (about) 1. We will revisit this after we have considered the distribution of expression across cells and genes.\nWhat about the bimodal appearance of the the â€˜realâ€™ values? If we had the whole transcriptome we would not expect to see such a pattern - weâ€™d expect to see a roughly normal distribution3. However, this is a subset of the genome and the nature of the subsetting has had an influence here. These are a subset of cell surface proteins that show a significant difference between at least two of twelve cell subtypes. That is, all of these genes are either â€œhighâ€ or â€œlowâ€ leading to a bimodal distribution.\nUnlike the other three datasets, which count raw counts, these data are normalised and log2 transformed. We do not need to plot the log of the values to see the distribution - they are already logged.\nDistribution of values across the samples\nFor the other three datasets, we used the summary() function to get an overview of the columns. This works well when you have upto about 25 columns but it is not helpful here because we have a lot of cells! Feel free to try it!\nIn this data set, there is even more of an advantage of using the pivot_longer(), group_by() and summarise() approach. We will be able to open the dataframe in the Viewer and make plots to examine whether the distributions are similar across cells. The mean and the standard deviation are useful to see the distributions across cells in a plot but we will also examine the interquartile values, maximums and the number of non-zero values.\nğŸ¬ Summarise all the cells:\n\nhspc_prog_summary_cell &lt;- hspc_prog |&gt;\n  pivot_longer(cols = -ensembl_gene_id,\n               names_to = \"cell\",\n               values_to = \"expr\") |&gt;\n  group_by(cell) |&gt;\n  summarise(min = min(expr),\n            lowerq = quantile(expr, 0.25),\n            sd = sd(expr),\n            mean = mean(expr),\n            median = median(expr),\n            upperq = quantile(expr, 0.75),\n            max = max(expr),\n            total = sum(expr),\n            n_above_zero = sum(expr &gt; 0))\n\nğŸ¬ View the hspc_prog_summary_cell dataframe (click on it in the environment).\nNotice that: - a minimum value of 0 appears in all 1499 cells - the lower quartiles are all zero and so are many of the medians - there are no cells with above 0 expression in all 280 of the gene subset - the highest number of genes expressed is 208, the lowest is 94\nIn short, there are quite a lot of zeros.\nTo get a better understanding of the distribution of expressions in cells we can create a ggplot using the pointrange geom. Pointrange puts a dot at the mean and a line between a minimum and a maximum such as +/- one standard deviation. Not unlike a boxplot, but when you need the boxes too be very narrow!\nğŸ¬ Create a pointrange plot.\n\nhspc_prog_summary_cell |&gt; \n  ggplot(aes(x = cell, y = mean)) +\n  geom_pointrange(aes(ymin = mean - sd, \n                      ymax = mean + sd ),\n                  size = 0.1)\n\n\n\n\n\n\n\nYou will need to use the Zoom button to pop the plot window out so you can make it as wide as possible\nThe things to notice are:\n\nthe average expression in cells is similar for all cells. This is good to know - if some cells had much lower expression perhaps there is something wrong with them, or their sequencing, and they should be excluded.\nthe distributions are roughly similar in width too\n\nThe default order of cell is alphabetical. It can be easier to judge if there are unusual cells if we order the lines by the size of the mean.\nğŸ¬ Order a pointrange plot with reorder(variable_to_order, order_by).\n\nhspc_prog_summary_cell |&gt; \n  ggplot(aes(x = reorder(cell, mean), y = mean)) +\n  geom_pointrange(aes(ymin = mean - sd, \n                      ymax = mean + sd ),\n                  size = 0.1)\n\n\n\n\n\n\n\nreorder() arranges cell in increasing size of mean\nAs we thought, the distributions are similar across cells - there are not any cells that are obviously different from the others (only incrementally).\nDistribution of values across the genes\nWe will use the same approach to summarise the genes.\nğŸ¬ Summarise the expression for each gene and save the result as hspc_prog_summary_gene. Include the same columns as we had in the by cell summary (hspc_prog_summary_cell) and an additional column, total for the total expression for each gene.\nğŸ¬ View the hspc_prog_summary_gene dataframe. Remember these are normalised and logged (base 2) so we should not see very large values.\nNotice that we have:\n\nsome genes (7) expressed in every cell, and many expressed in most cells\nquite a few genes with zero in many cells but this matters less when we have many cells (samples) than when we have few samples.\nno genes with zeros in every cell - the lowest number of cells is\n\n\n\n\n\nIt is again helpful to plot the ordered mean expression with pointrange to get an overview.\nğŸ¬ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\nhspc_prog_summary_gene |&gt; \n  ggplot(aes(x = reorder(ensembl_gene_id, mean), y = mean)) +\n  geom_pointrange(aes(ymin = mean - sd, \n                      ymax = mean + sd),\n                  size = 0.1)\n\n\n\n\n\n\n\nNote that the variability between genes (average expression between 0.020 and and 9.567) is far greater than between cells (average expression from 1.319 to 9.567) which is just what we would expect.\nNow go to Filtering for QC."
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#arabidopsis-filterqc",
    "href": "transcriptomics/week-3/workshop-orig.html#arabidopsis-filterqc",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis\n",
    "text": "ğŸ„ Arabidopsis\n\nOur samples look to be similarly well sequenced although this is difficult to determine with only two replicates. However, some genes are not expressed or the expression values are so low in for a gene that they are uninformative. We will filter the root_summary_gene dataframe to obtain a list of gene_id we can use to filter root.\nMy suggestion is to include only the genes with counts in at least 6 samples, and those with total counts above 20. I chose 6 because that would keep genes expressed only in one treatment. I chose 20 based on\nğŸ¬ Filter the summary by gene dataframe:\n\nroot_summary_gene_filtered &lt;- root_summary_gene |&gt; \n  filter(total &gt; 20) |&gt; \n  filter(n_above_zero &gt;= 6)\n\nâ“ How many genes do you have left\n\n\n\nğŸ¬ Use the list of gene_id in the filtered summary to filter the original dataset:\n\nroot_filtered &lt;- root |&gt; \n  filter(gene_id %in%  root_summary_gene_filtered$gene_id)\n\nğŸ¬ Write the filtered data to file:\n\nwrite_csv(root_filtered, \n          file = \"data-processed/root_filtered.csv\")\n\nNow go to Look after future you"
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#leishmania-filterqc",
    "href": "transcriptomics/week-3/workshop-orig.html#leishmania-filterqc",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nOur samples look to be similarly well sequenced. There are no samples we should remove. However, some genes are not expressed or the expression values are so low in for a gene that they are uninformative. We will filter the pro_meta_summary_gene dataframe to obtain a list of gene_id we can use to filter pro_meta.\nMy suggestion is to include only the genes with counts in at least 3 samples and those with total counts above 20. I chose 3 because that would keep genes expressed only in one treatment: [0, 0, 0] [#,#,#]. This is a difference we cannot test statistically, but which matters biologically.\nğŸ¬ Filter the summary by gene dataframe:\n\npro_meta_summary_gene_filtered &lt;- pro_meta_summary_gene |&gt; \n  filter(total &gt; 20) |&gt; \n  filter(n_above_zero &gt;= 3)\n\nâ“ How many genes do you have left\n\n\n\nğŸ¬ Use the list of gene_id in the filtered summary to filter the original dataset:\n\npro_meta_filtered &lt;- pro_meta |&gt; \n  filter(gene_id %in%  pro_meta_summary_gene_filtered$gene_id)\n\nğŸ¬ Write the filtered data to file:\n\nwrite_csv(pro_meta_filtered, \n          file = \"data-processed/pro_meta_filtered.csv\")\n\nNow go to Look after future you"
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#stem-cells-filterqc",
    "href": "transcriptomics/week-3/workshop-orig.html#stem-cells-filterqc",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nIn this dataset, we will not see and genes that are not expressed in any of the cells because we are using a specific subset of the transcriptome that was deliberately selected. In other words unexpressed genes have already been filtered out. However, it is good practices to verify there are no unexpressed genes before we embark on our analysis.\nWhere the sum of all the values in the rows is zero, all the entries must be zero. We can use this to find any the genes that are not expressed in any of the cells. To do row wise aggregates such as the sum across rows we can use the rowwise() function. c_across() allows us to use the colon notation to select columns. This is very useful when you have a lot of columns because it would be annoying to have to list all of them HSPC_001:Prog_852 means all the columns from HSPC_001 to Prog_852.\nğŸ¬ Find the genes that are 0 in every column of the hspc_prog dataframe:\n\nhspc_prog |&gt; \n  rowwise() |&gt; \n  filter(sum(c_across(HSPC_001:Prog_852)) == 0)\n\n# A tibble: 0 Ã— 1,500\n# Rowwise: \n# â„¹ 1,500 variables: ensembl_gene_id &lt;chr&gt;, HSPC_001 &lt;dbl&gt;, HSPC_002 &lt;dbl&gt;,\n#   HSPC_003 &lt;dbl&gt;, HSPC_004 &lt;dbl&gt;, HSPC_006 &lt;dbl&gt;, HSPC_008 &lt;dbl&gt;,\n#   HSPC_009 &lt;dbl&gt;, HSPC_011 &lt;dbl&gt;, HSPC_012 &lt;dbl&gt;, HSPC_014 &lt;dbl&gt;,\n#   HSPC_015 &lt;dbl&gt;, HSPC_016 &lt;dbl&gt;, HSPC_017 &lt;dbl&gt;, HSPC_018 &lt;dbl&gt;,\n#   HSPC_020 &lt;dbl&gt;, HSPC_021 &lt;dbl&gt;, HSPC_022 &lt;dbl&gt;, HSPC_023 &lt;dbl&gt;,\n#   HSPC_024 &lt;dbl&gt;, HSPC_025 &lt;dbl&gt;, HSPC_026 &lt;dbl&gt;, HSPC_027 &lt;dbl&gt;,\n#   HSPC_028 &lt;dbl&gt;, HSPC_030 &lt;dbl&gt;, HSPC_031 &lt;dbl&gt;, HSPC_033 &lt;dbl&gt;, â€¦\n\n\nNotice that we have summed across all the columns.\nâ“ What do you conclude?\n\n\nğŸ¬ Write combined data to file:\n\nwrite_csv(hspc_prog, \n          file = \"data-processed/hspc_prog.csv\")\n\nNow go to Look after future you"
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#arabidopsis-and-future-you",
    "href": "transcriptomics/week-3/workshop-orig.html#arabidopsis-and-future-you",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis and future you",
    "text": "ğŸ„ Arabidopsis and future you\nğŸ¬ Create a new Project, arab-88H, populated with folders and your data. Make a script file called cont-low-root.R. This will a be commented analysis of comparison between control and low Ni root tissue. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again."
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#leishmania-and-future-you",
    "href": "transcriptomics/week-3/workshop-orig.html#leishmania-and-future-you",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania and future you",
    "text": "ğŸ’‰ Leishmania and future you\nğŸ¬ Create a new Project, leish-88H, populated with folders and your data. Make a script file called pro_meta.R. This will a be commented analysis of comparison procyclic promastigote and metacyclic promastigotes. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again."
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#stem-cells-and-future-you",
    "href": "transcriptomics/week-3/workshop-orig.html#stem-cells-and-future-you",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells and future you",
    "text": "ğŸ­ Stem cells and future you\nğŸ¬ Create a new Project, mice-88H, populated with folders and your data. Make a script file called hspc-prog.R. This will a be commented analysis of the hspc cells vs the prog cells. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again."
  },
  {
    "objectID": "transcriptomics/week-3/workshop-orig.html#footnotes",
    "href": "transcriptomics/week-3/workshop-orig.html#footnotes",
    "title": "Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.â†©ï¸\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.â†©ï¸\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.â†©ï¸"
  },
  {
    "objectID": "core/core.html",
    "href": "core/core.html",
    "title": "Core: Supporting Information",
    "section": "",
    "text": "futureself, CC-BY-NC, by Julen Colomb\n\n\nThere are two workshops taken by everyone on BIO00088H. These are in weeks 2 and 6. These are important in understanding both how to assemble, curate and document your â€œSupporting Informationâ€ and how to work reproducibly so future you (Spring semester you) can painlessly work with past you and your work is demonstrably repeatable. This is essential because you will want to be able to set work aside for holidays and assessment periods and then restart easily. The Supporting Information you submit with your Report will be be assessed on its organisation, reproducibility and documentation.\nBIO00070M students do week 1 and 6 of the core workshops along with weeks 3, 4 and 5 of transcriptomics.\n\n\nWhy reproducibility matters, project-oriented workflow, organisation and naming things. You will also learn how to recognise and write cool ğŸ˜ code, not ğŸ˜© ugly code and code algorithmically and discover some awesome short cuts to help you write cool ğŸ˜ code.\n\n\n\nDocumenting your Supporting Information with a read me and appropriate code commenting, curating code, non-coded processing",
    "crumbs": [
      "Core Supporting Info",
      "Core: Supporting Information"
    ]
  },
  {
    "objectID": "core/core.html#week-1-core-supporting-information-1",
    "href": "core/core.html#week-1-core-supporting-information-1",
    "title": "Core: Supporting Information",
    "section": "",
    "text": "Why reproducibility matters, project-oriented workflow, organisation and naming things. You will also learn how to recognise and write cool ğŸ˜ code, not ğŸ˜© ugly code and code algorithmically and discover some awesome short cuts to help you write cool ğŸ˜ code.",
    "crumbs": [
      "Core Supporting Info",
      "Core: Supporting Information"
    ]
  },
  {
    "objectID": "core/core.html#week-6-core-supporting-information-2",
    "href": "core/core.html#week-6-core-supporting-information-2",
    "title": "Core: Supporting Information",
    "section": "",
    "text": "Documenting your Supporting Information with a read me and appropriate code commenting, curating code, non-coded processing",
    "crumbs": [
      "Core Supporting Info",
      "Core: Supporting Information"
    ]
  },
  {
    "objectID": "core/week-2/overview.html",
    "href": "core/week-2/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week you will revise some essential concepts for scientific computing: file system organisation, file types, working directories and paths. You will also ensure you know how to use the virtual desktop service (VDS) to access the software you need for your work. The workshop will cover a rationale for working reproducibly, project oriented workflow, naming things and documenting your work.\n\nLearning objectives\nThe successful student will be able to:\n\nexplain the organisation of files and directories in a file systems including root, home and working directories\nexplain absolute and relative file paths\nexplain why working reproducibly is important\nknow how to use a project-oriented workflow to organise work\nbe able to give files human- and machine-readable names\nwrite cool ğŸ˜ code not ğŸ˜© ugly code\nexplain the value of code which expresses the structure of the problem/solution.\nuse some useful shortcuts to help write cool ğŸ˜ code\nensure they can use the VDS to access files and software required for the work\n\n\n\nInstructions\n\nPrepare\n\nğŸ“– Read Understanding file systems\nğŸ“– Read Workflow in RStudio\nğŸ–¥ï¸Set up the VDS and know how to use it.\n\nWorkshop\nConsolidate",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "About"
    ]
  },
  {
    "objectID": "core/week-2/study_before_workshop.html",
    "href": "core/week-2/study_before_workshop.html",
    "title": "Independent Study to prepare for workshop",
    "section": "",
    "text": "ğŸ“– Read Understanding file systems. This is an approximately 15 - 20 minute read revising file types and file systems. It covers concepts of working directories and paths. We learned these ideas in stage 1 and but if you donâ€™t feel completely confident with revisting might help. For BIO00070M students, this is part of the work you will also be asked to complete for BIO00052M Data Analysis in R.\nğŸ“– Read Workflow in RStudio. You may find it helpful to remind yourself about RStudio Projects. In previous years, BIO00088H students have submitted an â€œRStudio Projectâ€ as part of your BABS work. Here, you will submit â€œSupporting Informationâ€ for your Project Report. The Supporting Information is a documented and organised collection of all the digital parts of your research project. This includes data (or instructions for accessing data), code and/or non-coded processing, instructions for use, computational requirements and outputs. The Supporting Information could be a single RStudio Project (like you have done previously but with better documentation) or a folder that includes an RStudio Project and other material/scripts. BIO00070M students do not need to submit â€œSupporting Informationâ€.\nSet up the Virtual Desktop Service. I very strongly recommend working on the University computers for this work. You will be using more specialised R packages than you might be used to. This is especially important if you often have difficulty updating and or installing software on your own machine, wouldnâ€™t know what what version of R you are using or donâ€™t realise there is a difference between R and RStudio. The uni machines always have up-to-date R and R packages and all the packages that appear in teaching materials. It is our responsibility to ensure everything works on here. The Virtual Desktop Service allows you to log on to a university computer from your home computer. It means you can access all software and filestores. When using the VDS for R and RStudio, it usually makes sense to use other software - such as a browser or file explorer - also through the VDS.\n\nIf you are confident in your ability to set up your own machine, you need:\n\nto know the difference between R and RStudio\nto use R 4.5 and RStudio 2025.05.0+496 â€œMariposa Orchidâ€ Release which includes Quarto 1.7.30 (C:/Program Files/Quarto/bin/quarto.exe)\nbe certain you are actually using R 4.5 - it is written in the top edge of the console window. By default RStudio uses the latest version on R on your machine. However, windows users are able to change this to a â€œspecific versionâ€. You might have done that previously. Change it back using Tools | Global Options R version â€œUse your machineâ€™s default 64-bit version of Râ€\nto make sure you do the independent study where it tells you what steps you need to take to get packages (and versions that are unlikely to cause issues) used in the workshop\n\nIt is possible to access all your files on your university account without using the VDS. For example, if you want to work on uni machines at uni and your machine at home. You can best do this by mapping a drive: https://support.york.ac.uk/s/topic/0TO4K000000lA5ZWAU/filestores. If you store everything on google drive you can also read/write to that like any other drive using google drive app.\nEven if you plan to use your own machine I really recommend you take the time to set the VDS up now while youâ€™re not time pressured so you always have that option ready.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Prepare!"
    ]
  },
  {
    "objectID": "core/week-6/overview.html",
    "href": "core/week-6/overview.html",
    "title": "Overview",
    "section": "",
    "text": "We considered how to organise reproducible data analyses in Core: Supporting Information 1. This week we will consider how to document and curate reproducible data analyses. You will add a README to your project and discover all the software you are using in R. The workshop will also include a questions and answers section.\n\nLearning objectives\nThe successful student will be able to:\n\nDescribe the purpose of a README file\nList the key components of a README file\nUse packages to discover the packages used in a project\nWrite a README file for a project\n\n\n\nInstructions\n\nPrepare\n\nRevise Core: Supporting Information 1 and make a note of queries you have\n\nWorkshop\nConsolidate",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "About"
    ]
  },
  {
    "objectID": "core/week-6/study_before_workshop.html",
    "href": "core/week-6/study_before_workshop.html",
    "title": "Independent Study to prepare for workshop",
    "section": "",
    "text": "Revise Core: Supporting Information 1 Organising Reproducible Data Analyses.\n\nDo you know your Supporting information will most likely be be a structured folder which is either an RStudio Project or contains an RStudio Project?\nAre you following the best practices code formatting and style? If not, go through your scripts and edit.\nDo you have numbers hard coded where they could be variables?\nAre you using a sensible naming convention for files and variables? Have you written it down?\nMake a note of queries you have. Take some time to formulate and write down your questions. The more specific and clear your question is, the better answer I will be able to provide.\nPost your questions here: Menti Code: 3306 3222. QR:",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Prepare!"
    ]
  },
  {
    "objectID": "images/images.html",
    "href": "images/images.html",
    "title": "Image Data Analysis for Group Project",
    "section": "",
    "text": "The following ImageJ workflow uses the processing steps you used in workshop 3 with one change. That change is to save the results to file rather than having the results window pop up and saving from there. Or maybe two changes: it also tells you to use meaning systematic file names that will be easy to process when importing data. The RStudio workflow shows you how to import multiple files into one dataframe with columns indicating the treatment.\n\nSave files with systematic names: ev_0.avi 343_0.avi ev_1.avi 343_1.avi ev_2.5.avi 343_2.5.avi\nOpen ImageJ\nOpen video file eg ev_2.5.avi\n\nConvert to 8-bit: Image | Type | 8-bit\nCrop to petri dish: Select then Image | Crop\nCalculate average pixel intensity: Image | Stacks | Z Project\n\nProjection type: Average Intensity to create AVG_ev_2.5.avi\n\n\n\nSubtract average from image: Process | Image Calculator\n\nImage 1: ev_2.5.avi\n\nOperation: Subtract\nImage 2: AVG_ev_2.5.avi\n\nCreate new window: checked\nOK, Yes to Process all\n\n\nInvert: Edit | Invert\nAdjust threshold: Image | Adjust | Threshold\n\nMethod: Default\nThresholding: Default, B&W\nDark background: checked\nAuto or adjust a little but make sure the larvae do not disappear at later points in the video (use the slider)\nApply\n\n\nInvert: Edit | Invert\nTrack: Plugins | wrMTrck\n\nSet minSize: 10\nSet maxSize: 400\nSet maxVelocity: 10\nSet maxAreaChange: 200\nSet bendThreshold: 1\n\nImportant: check Save Results File This is different to what you did in the workshop. It will help because the results will be saved automatically rather than to saving from the Results window that other pops up. Consequently, you will be able to save the results files with systematic names relating to their treatments and then read them into R simultaneously. That will also allow you to add information from the name of the file (which has the treatment information) to the resulting dataframes\n\n\nwrMTrck window with the settings listed above shown\n\n\nClick OK. Save to a folder for all the tracking data files. I recommend deleting the â€œResults of..â€ part of the name\n\n\nCheck that the Summary window indicates 3 tracks and that the 3 larvae are what is tracked by using the slider on the Result image\nRepeat for all videos\n\nThis is the code you need to import multiple csv files into a single dataframe and add a column with the treatment information from the file name. This is why systematic file names are good.\nIt assumes\n\nyour files are called type_concentration.txt for example: ev_0.txt 343_0.txt ev_1.txt 343_1.txt ev_2.5.txt 343_2.5.txt.\nthe .txt datafile are in a folder called track inside your working directory\nyou have installed the following packages: tidyverse, janitor\n\n\nğŸ¬ Load the tidyverse\n\nlibrary(tidyverse)\n\nğŸ¬ Put the file names into a vector we will iterate through\n\n# get a vector of the file names\nfiles &lt;- list.files(path = \"track\", full.names = TRUE )\n\nWe can use map_df() from the purrr package which is one of the tidyverse gems loaded with tidyvserse. map_df() will iterate through files and read them into a dataframe with a specified import function. We are using read_table(). map_df() keeps track of the file by adding an index column called file to the resulting dataframe. Instead of this being a number (1 - 6 here) we can use set_names() to use the file names instead. The clean_names() function from the janitor package will clean up the column names (make them lower case, replace spaces with _ remove special characters etc)\nğŸ¬ Import multiple csv files into one dataframe called tracking\n\n# import multiple data files into one dataframe called tracking\n# using map_df() from purrr package\n# clean the column names up using janitor::clean_names()\ntracking &lt;- files |&gt; \n  set_names() |&gt;\n  map_dfr(read_table, .id = \"file\") |&gt;\n  janitor::clean_names()\n\nYou will get a warning Duplicated column names deduplicated: 'avgX' =&gt; 'avgX_1' [15] for each of the files because the csv files each have two columns called avgX. If you click on the tracking dataframe you see is contains the data from all the files.\nNow we can add columns for the type and the concentration by processing the values in the file. The values are like track/343_0.txt so we need to remove .txt and track/ and separate the remaining words into two columns.\nğŸ¬ Process the file column to add columns for the type and the concentration\n\n# extract type and concentration from file name\n# and put them into additopnal separate columns\ntracking &lt;- tracking |&gt; \n  mutate(file = str_remove(file, \".txt\")) |&gt;\n  mutate(file = str_remove(file, \"track/\")) |&gt;\n  extract(file, remove = \n            FALSE,\n          into = c(\"type\", \"conc\"), \n          regex = \"([^_]{2,3})_(.+)\") \n\n[^_]{2,3} matches two or three characters that are not _ at the start of the string (^)\n.+ matches one or more characters. The extract() function puts the first match into the first column, type, and the second match into the second column, conc. The remove = FALSE argument means the original column is kept.\nYou now have a dataframe with all the tracking data which is relatively easy to summarise and plot using tools you know.\nThere is an example RStudio project containing this code here: tips. You can also download the project as a zip file from there but there is some code that will do that automatically for you. Since this is an RStudio Project, do not run the code from inside a project. You may want to navigate to a particular directory or edit the destdir:\n\nusethis::use_course(url = \"3mmaRand/tips\", destdir = \".\")\n\nYou can agree to deleting the zip. You should find RStudio restarts and you have a new project called tips-xxxxxx. The xxxxxx is a commit reference - you do not need to worry about that, it is just a way to tell you which version of the repo you downloaded. You can now run the code in the project.",
    "crumbs": [
      "Image Analysis",
      "Image Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "images/images.html#worm-tracking",
    "href": "images/images.html#worm-tracking",
    "title": "Image Data Analysis for Group Project",
    "section": "",
    "text": "The following ImageJ workflow uses the processing steps you used in workshop 3 with one change. That change is to save the results to file rather than having the results window pop up and saving from there. Or maybe two changes: it also tells you to use meaning systematic file names that will be easy to process when importing data. The RStudio workflow shows you how to import multiple files into one dataframe with columns indicating the treatment.\n\nSave files with systematic names: ev_0.avi 343_0.avi ev_1.avi 343_1.avi ev_2.5.avi 343_2.5.avi\nOpen ImageJ\nOpen video file eg ev_2.5.avi\n\nConvert to 8-bit: Image | Type | 8-bit\nCrop to petri dish: Select then Image | Crop\nCalculate average pixel intensity: Image | Stacks | Z Project\n\nProjection type: Average Intensity to create AVG_ev_2.5.avi\n\n\n\nSubtract average from image: Process | Image Calculator\n\nImage 1: ev_2.5.avi\n\nOperation: Subtract\nImage 2: AVG_ev_2.5.avi\n\nCreate new window: checked\nOK, Yes to Process all\n\n\nInvert: Edit | Invert\nAdjust threshold: Image | Adjust | Threshold\n\nMethod: Default\nThresholding: Default, B&W\nDark background: checked\nAuto or adjust a little but make sure the larvae do not disappear at later points in the video (use the slider)\nApply\n\n\nInvert: Edit | Invert\nTrack: Plugins | wrMTrck\n\nSet minSize: 10\nSet maxSize: 400\nSet maxVelocity: 10\nSet maxAreaChange: 200\nSet bendThreshold: 1\n\nImportant: check Save Results File This is different to what you did in the workshop. It will help because the results will be saved automatically rather than to saving from the Results window that other pops up. Consequently, you will be able to save the results files with systematic names relating to their treatments and then read them into R simultaneously. That will also allow you to add information from the name of the file (which has the treatment information) to the resulting dataframes\n\n\nwrMTrck window with the settings listed above shown\n\n\nClick OK. Save to a folder for all the tracking data files. I recommend deleting the â€œResults of..â€ part of the name\n\n\nCheck that the Summary window indicates 3 tracks and that the 3 larvae are what is tracked by using the slider on the Result image\nRepeat for all videos\n\nThis is the code you need to import multiple csv files into a single dataframe and add a column with the treatment information from the file name. This is why systematic file names are good.\nIt assumes\n\nyour files are called type_concentration.txt for example: ev_0.txt 343_0.txt ev_1.txt 343_1.txt ev_2.5.txt 343_2.5.txt.\nthe .txt datafile are in a folder called track inside your working directory\nyou have installed the following packages: tidyverse, janitor\n\n\nğŸ¬ Load the tidyverse\n\nlibrary(tidyverse)\n\nğŸ¬ Put the file names into a vector we will iterate through\n\n# get a vector of the file names\nfiles &lt;- list.files(path = \"track\", full.names = TRUE )\n\nWe can use map_df() from the purrr package which is one of the tidyverse gems loaded with tidyvserse. map_df() will iterate through files and read them into a dataframe with a specified import function. We are using read_table(). map_df() keeps track of the file by adding an index column called file to the resulting dataframe. Instead of this being a number (1 - 6 here) we can use set_names() to use the file names instead. The clean_names() function from the janitor package will clean up the column names (make them lower case, replace spaces with _ remove special characters etc)\nğŸ¬ Import multiple csv files into one dataframe called tracking\n\n# import multiple data files into one dataframe called tracking\n# using map_df() from purrr package\n# clean the column names up using janitor::clean_names()\ntracking &lt;- files |&gt; \n  set_names() |&gt;\n  map_dfr(read_table, .id = \"file\") |&gt;\n  janitor::clean_names()\n\nYou will get a warning Duplicated column names deduplicated: 'avgX' =&gt; 'avgX_1' [15] for each of the files because the csv files each have two columns called avgX. If you click on the tracking dataframe you see is contains the data from all the files.\nNow we can add columns for the type and the concentration by processing the values in the file. The values are like track/343_0.txt so we need to remove .txt and track/ and separate the remaining words into two columns.\nğŸ¬ Process the file column to add columns for the type and the concentration\n\n# extract type and concentration from file name\n# and put them into additopnal separate columns\ntracking &lt;- tracking |&gt; \n  mutate(file = str_remove(file, \".txt\")) |&gt;\n  mutate(file = str_remove(file, \"track/\")) |&gt;\n  extract(file, remove = \n            FALSE,\n          into = c(\"type\", \"conc\"), \n          regex = \"([^_]{2,3})_(.+)\") \n\n[^_]{2,3} matches two or three characters that are not _ at the start of the string (^)\n.+ matches one or more characters. The extract() function puts the first match into the first column, type, and the second match into the second column, conc. The remove = FALSE argument means the original column is kept.\nYou now have a dataframe with all the tracking data which is relatively easy to summarise and plot using tools you know.\nThere is an example RStudio project containing this code here: tips. You can also download the project as a zip file from there but there is some code that will do that automatically for you. Since this is an RStudio Project, do not run the code from inside a project. You may want to navigate to a particular directory or edit the destdir:\n\nusethis::use_course(url = \"3mmaRand/tips\", destdir = \".\")\n\nYou can agree to deleting the zip. You should find RStudio restarts and you have a new project called tips-xxxxxx. The xxxxxx is a commit reference - you do not need to worry about that, it is just a way to tell you which version of the repo you downloaded. You can now run the code in the project.",
    "crumbs": [
      "Image Analysis",
      "Image Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html",
    "href": "core/week-6/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop we will consider how to document and curate reproducible data analyses. You will add a README to your project and start to populate it. This will include finding out how to find the software you are using.",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#session-overview",
    "href": "core/week-6/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop we will consider how to document and curate reproducible data analyses. You will add a README to your project and start to populate it. This will include finding out how to find the software you are using.",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#code-comments",
    "href": "core/week-6/workshop.html#code-comments",
    "title": "Workshop",
    "section": "Code comments",
    "text": "Code comments\nComments are notes in the code which are not executed. They are ignored by the computer but are read by humans. They are used to explain what the code is doing and why. You do not need to teach a person to program in the comments, but you should explain the logic of the code and the decisions you make based on what you see in your analysis. If you remove the code, the comments should still make sense. Good commenting can be a solid basis for your methods section. You need not interpret the results in any depth, but you should comment on interpretation where it impacts what you do in the code.",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#non-scripted-parts-of-the-analysis",
    "href": "core/week-6/workshop.html#non-scripted-parts-of-the-analysis",
    "title": "Workshop",
    "section": "Non-scripted parts of the analysis",
    "text": "Non-scripted parts of the analysis\nNon-scripted parts of the analysis such as manual data cleaning or using software that does not have a script to analyse data presented in the report should be documented. This includes the steps taken, parameter settings and the decisions made. It need not be written as a narrative - you can use bullet points and be concise.\nğŸ¬ Go through any code you have written and edit your comments.\nğŸ¬ Make a text file for each non-scripted part of the analysis and document the steps taken, parameter settings and decisions made. You likely will not be able to complete now where you will be analysing data in semester 2, but you can write down â€œplace holdersâ€ where possible. For example, in using Fiji to analyse images:\n\n\n\n\n\n\nimage_processing_in_fiji.txt\n\n\n\nImages were converted to:\nManual cropping:\nProjection type used:\netc",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#readme-files",
    "href": "core/week-6/workshop.html#readme-files",
    "title": "Workshop",
    "section": "README files",
    "text": "README files\nREADMEs are a form of documentation which have been widely used for a long time. They contain all the information about the other files in a Project. They are the first thing that someone will see when they open a project. They are written in plain text and are usually called README.md or README.txt. .md stands for markdown, a lightweight markup language which is human readable (plain text) and can be converted to HTML.",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#what-should-be-in-a-readme",
    "href": "core/week-6/workshop.html#what-should-be-in-a-readme",
    "title": "Workshop",
    "section": "What should be in a README?",
    "text": "What should be in a README?\n\nProject title and description\nStart date, last updated date, contact information\nDescription of Project organisation\nSoftware requirements\nData description\nInstructions for use\n\n\nProject title and description\nThe title should be descriptive and concise. It does not have to match the title of the report but why give yourself more thinking!\nThe description should be a brief summary of the project. It should include the aim of the project and the methods used. It should be written in plain language so that someone who is not familiar with the field can understand it. It is likely to be similar to a report abstract but with less emphasis on introduction/background and discussion and more detail on the the methods.\n\n\nStart date, last updated date, contact information\nWe date our work so others know how old it is and when it was last updated. The very best READMEs are updated every time the project is updated so there is a â€œchange logâ€ at the top of the file. This is a list of changes made to the project with the date they were made. This especially useful in collaborative projects when you are not using a version control system. Sometimes, the change log is in a separate file called CHANGELOG.md. or similar.\n\n\nDescription of Project organisation\nYour aim is to organise your project and name files and files so that the organisation is clear to someone who has never seen it before. You are are aiming for â€œspeaks for itselfâ€ organisation. However, you still need to give an overview of the organisation in the README. This should include the names of the folders and what is in them. You can also include a diagram of the project organisation if you like. Make sure it is easy for readers to distinguish between code, data, and outputs. Also explain your file naming convention along with any abbreviations or codes you have used.\n\n\nSoftware requirements\nYou should list the software you used in your project. This should include the version of the software you used. This is important for reproducibility.\nThere are packages that give session information in R and Python\nR: sessioninfo (Wickham et al. 2021)\nsessioninfo::session_info()\nPython: session_info (Ostblom, Joel 2019)\nimport session_info\nsession_info.show()\nFiji:\nCite\n\n\nData description\nYou should describe all the data used in the project. This should include where the data came from, how it was collected, and any processing that was done to it. You may need to include links to the data sources. Include a â€œdata dictionaryâ€ which describes the variables in the data and what they mean. This is especially important since most variable names are abbreviations or codes.\n\n\nInstructions for use\nThe last part of the README should be instructions for how to use it. You want to describe to reader what they need to do to recreate the results you present in your report. This should include:\n\nBrief description of the code structure especially if you have used a a mix of languages and scripts\nInstructions to run the code and reproduce the figures or other outputs in the report\nAny other information that needed to understand and recreate the work\n\nğŸ¬ Write a README file for your project.",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#what-should-be-in-supporting-information",
    "href": "core/week-6/workshop.html#what-should-be-in-supporting-information",
    "title": "Workshop",
    "section": "What should be in Supporting information",
    "text": "What should be in Supporting information\nAll the inputs and outputs of the work\n\nOriginal data or instructions/links to obtain the data. This includes all data in the report such as images, not just the numerical data\nCode\nInstructions for non-scripted parts of the analysis.\nAnything generated from the code such as processed data and figures.\nAnything generated from non-scripted parts of the analysis such as processed data and figures.\nA copy of the report it supports.\na README file",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/study_after_workshop.html",
    "href": "core/week-2/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "Before starting work on your strand specific data analysis. Ensure you have EITHER:\n\nğŸ¬ Set up the Virtual Desktop Service and been able to use it\n\nOR\n\nğŸ¬ Updated R\nğŸ¬ Updated RStudio.\nInstall package building tools\nğŸ¬ Windows Install Rtools\nğŸ¬ Mac install Xcode from Mac App Store\nğŸ¬ Installed packages: devtools, tidyverse, BiocManager, readxl\n\nYou might want to consider getting a GitHub account and applying for student benefits so that you can use GitHub co-pilot. GitHub copilot is an â€œAI pair programmer that offers autocomplete-style suggestions as you codeâ€. GitHub Copilot is available as an opt-in integration with RStudio.\n\nRead more about GitHub Copilot integration with RStudio\nğŸ¬ Create a GitHub account\nğŸ¬ Apply for student benefits. You will need to upload an image of your student id, use your institutional email address and you will be required to use two factor authentication.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Consolidate!"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#session-overview",
    "href": "core/week-2/workshop.html#session-overview",
    "title": "Workshop",
    "section": "Session overview",
    "text": "Session overview\nIn this workshop we will discuss why reproducibility matters and how to organise your work to make it reproducible. We will cover:\n\n\nWhat is reproducibility\nHow to achieve reproducibility\nRationale for scripting\nProject-oriented workflow\nCode formatting and style\nCoding algorithmically\nNaming things\nAnd some handy workflow tips",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#what-is-reproducibility",
    "href": "core/week-2/workshop.html#what-is-reproducibility",
    "title": "Workshop",
    "section": "What is reproducibility?",
    "text": "What is reproducibility?\n\n\nThe Turing Wayâ€™s definitions of reproducible research",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#definitions",
    "href": "core/week-2/workshop.html#definitions",
    "title": "Workshop",
    "section": "Definitions",
    "text": "Definitions\n\n\nThe Turing Wayâ€™s definitions of reproducible research\n\nReproducible: Same data + same analysis = identical results. â€œâ€¦ obtaining consistent results using the same input data; computational steps, methods, and code; and conditions of analysis. This definition is synonymous withâ€computational reproducibilityâ€ (National Academies of Sciences et al. 2019). This is what we are concentrating on in the Supporting Information.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#definitions-1",
    "href": "core/week-2/workshop.html#definitions-1",
    "title": "Workshop",
    "section": "Definitions",
    "text": "Definitions\n\n\nThe Turing Wayâ€™s definitions of reproducible research\n\nReplicable: Different data + same analysis = qualitatively similar results. The work is not dependent on the specificities of the data.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#definitions-2",
    "href": "core/week-2/workshop.html#definitions-2",
    "title": "Workshop",
    "section": "Definitions",
    "text": "Definitions\n\n\nThe Turing Wayâ€™s definitions of reproducible research\n\nRobust: Same data + different analysis = qualitatively similar or identical results. The work is not dependent on the specificities of the analysis.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#definitions-3",
    "href": "core/week-2/workshop.html#definitions-3",
    "title": "Workshop",
    "section": "Definitions",
    "text": "Definitions\n\n\nThe Turing Wayâ€™s definitions of reproducible research\n\nGeneralisable: Different data + different analysis = qualitatively similar results and same conclusions.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#why-does-it-matter",
    "href": "core/week-2/workshop.html#why-does-it-matter",
    "title": "Workshop",
    "section": "Why does it matter?",
    "text": "Why does it matter?\n\nMany high profile cases of work which did not reproduce e.g.Â Anil Potti unravelled by Baggerly and Coombes (2009)\nFive selfish reasons to work reproducibly (Markowetz 2015). Alternatively, see the very entertaining talk\nWill become standard in Science and publishing e.g OECD Global Science Forum Building digital workforce capacity and skills for data-intensive science (OECD Global Science Forum 2020)",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#how-to-achieve-reproducibility",
    "href": "core/week-2/workshop.html#how-to-achieve-reproducibility",
    "title": "Workshop",
    "section": "How to achieve reproducibility",
    "text": "How to achieve reproducibility\n\nReproducibility is a continuum. Some is better than none!\nScript everything\nOrganisation: Project-oriented workflows with file and folder structure, naming things\nCode: follow a consistent style, organise into sections and scripts (be modular), Code algorithmically\nDocumentation: Readme files, code comments, metadata,\nMore advanced: version, control, continuous integration and testing (not required for Supporting Information)",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rationale-for-scripting",
    "href": "core/week-2/workshop.html#rationale-for-scripting",
    "title": "Workshop",
    "section": "Rationale for scripting",
    "text": "Rationale for scripting\n\nScience is the generation of ideas, designing work to test them and reporting the results.\nWe ensure laboratory and field work is replicable, robust and generalisable by planning and recording in lab books and using standard protocols. Repeating results is still hard.\nWorkflows for computational projects, and the data analysis and reporting of other work can, and should, be 100% reproducible!\nScripting is the way to achieve this.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#project-oriented-workflow",
    "href": "core/week-2/workshop.html#project-oriented-workflow",
    "title": "Workshop",
    "section": "Project-oriented workflow",
    "text": "Project-oriented workflow\n\nuse folders to organise your work\nyou are aiming for structured, systematic and repeatable.\ninputs and outputs should be clearly identifiable from structure and/or naming",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#example-si-itself-is-an-rsp",
    "href": "core/week-2/workshop.html#example-si-itself-is-an-rsp",
    "title": "Workshop",
    "section": "Example: SI itself is an RSP",
    "text": "Example: SI itself is an RSP\n\n-- stem_cell_rna\n   |__stem_cell_rna.Rproj   \n   |__raw_ data/            \n      |__2019-03-21_donor_1.csv\n      |__2019-03-21_donor_2.csv\n      |__2019-03-21_donor_3.csv\n   |__README.md\n   |__R/\n      |__01_data_processing.R\n      |__02_exploratory.R\n      |__functions/\n         |__theme_volcano.R\n         |__normalise.R",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#example-si-includes-an-rsp",
    "href": "core/week-2/workshop.html#example-si-includes-an-rsp",
    "title": "Workshop",
    "section": "Example: SI includes an RSP",
    "text": "Example: SI includes an RSP\n\n-- stem_cell_rna\n   |__data_processing/\n      |__01_data_processing.py\n      |__02_exploratory.py\n      |__raw_data/\n         |__2019-03-21_donor_1.csv\n         |__2019-03-21_donor_2.csv\n         |__2019-03-21_donor_3.csv\n   |__README.md\n   |__statistical_analysis\n      |__statistical_analysis.Rproj   \n      |__processed_data/\n      |__R/\n         |__01_DGE.R\n         |__02_visualisation.R\n         |__functions/\n            |__theme_volcano.R\n            |__normalise.R",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects",
    "href": "core/week-2/workshop.html#rstudio-projects",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\nRStudio Projects make it easy to manage working directories and paths because they set the working directory to the RStudio Projects directory automatically.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects-1",
    "href": "core/week-2/workshop.html#rstudio-projects-1",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\n\n-- stem_cell_rna\n   |__stem_cell_rna.Rproj   \n   |__raw_ data/            \n      |__2019-03-21_donor_1.csv\n   |__README. md\n   |__R/\n      |__01_data_processing.R\n      |__02_exploratory.R\n      |__functions/\n         |__theme_volcano.R\n         |__normalise.R\n\n\nThe project directory is the folder at the top",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects-2",
    "href": "core/week-2/workshop.html#rstudio-projects-2",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\n\n-- stem_cell_rna\n   |__stem_cell_rna.Rproj   \n   |__raw_ data/            \n      |__2019-03-21_donor_1.csv\n   |__README. md\n   |__R/\n      |__01_data_processing.R\n      |__02_exploratory.R\n      |__functions/\n         |__theme_volcano.R\n         |__normalise.R\n\n\nthe .RProj file is directly under the project folder1. Its presence is what makes the folder an RStudio Project\n\nThanks to Mine Ã‡etinkaya-Rundel who helped me work out how to highlight a line https://gist.github.com/mine-cetinkaya-rundel/3af3415eab70a65be3791c3dcff6e2e3. Note to futureself: the engine: knitr matters.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects-3",
    "href": "core/week-2/workshop.html#rstudio-projects-3",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\nWhen you open an RStudio Project, the working directory is set to the Project directory (i.e., the location of the .Rproj file).\nWhen you use an RStudio Project you do not need to use setwd()\nWhen someone, including future you, opens the project on another machine, all the paths just work.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects-4",
    "href": "core/week-2/workshop.html#rstudio-projects-4",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\nJenny BryanIn the words of Jenny Bryan:\n\nâ€œIf the first line of your R script is setwd(â€C:/Users/jenny/path/that/only/I/haveâ€) I will come into your office and SET YOUR COMPUTER ON FIREâ€",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#creating-an-rstudio-project",
    "href": "core/week-2/workshop.html#creating-an-rstudio-project",
    "title": "Workshop",
    "section": "Creating an RStudio Project",
    "text": "Creating an RStudio Project\nThere are two menus options:\n\nTop left, File menu\nTop Right, drop-down indicated by the .RProj icon\n\nThey both do the same thing.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#creating-an-rstudio-project-1",
    "href": "core/week-2/workshop.html#creating-an-rstudio-project-1",
    "title": "Workshop",
    "section": "Creating an RStudio Project",
    "text": "Creating an RStudio Project\nThen Choose: New Project | New Directory | New Project\nMake sure you â€œBrowseâ€ to the folder you want to create the project.\nâ” Is your working directory a good place to create a Project folder?",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#creating-an-rstudio-project-2",
    "href": "core/week-2/workshop.html#creating-an-rstudio-project-2",
    "title": "Workshop",
    "section": "Creating an RStudio Project",
    "text": "Creating an RStudio Project\nWhen you create a new RStudio Project\n\nA folder called bananas/ is created\nRStudio starts a new session in bananas/ i.e., your working directory is now bananas/\n\nA file called bananas.Rproj is created\nthe .Rproj file is what makes the directory an RStudio Project",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#opening-and-closing",
    "href": "core/week-2/workshop.html#opening-and-closing",
    "title": "Workshop",
    "section": "Opening and closing",
    "text": "Opening and closing\nYou can close an RStudio Project with ONE of:\n\nFile | Close Project\nUsing the drop-down option on the far right of the tool bar where you see the Project name",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#opening-and-closing-1",
    "href": "core/week-2/workshop.html#opening-and-closing-1",
    "title": "Workshop",
    "section": "Opening and closing",
    "text": "Opening and closing\nYou can open an RStudio Project with ONE of:\n\nFile | Open Project or File | Recent Projects\n\nUsing the drop-down option on the far right of the tool bar where you see the Project name\n\nDouble-clicking an .Rproj file from your file explorer/finder\n\nWhen you open project, a new R session starts.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#code-formatting-and-style-1",
    "href": "core/week-2/workshop.html#code-formatting-and-style-1",
    "title": "Workshop",
    "section": "Code formatting and style",
    "text": "Code formatting and style\n\nâ€œGood coding style is like correct punctuation: you can manage without it butitsuremakesthingseasiertoread.â€\n\nThe tidyverse style guide\n\nCode is not write only.\nCode is communication!",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#code-formatting-and-style-2",
    "href": "core/week-2/workshop.html#code-formatting-and-style-2",
    "title": "Workshop",
    "section": "Code formatting and style",
    "text": "Code formatting and style\nWe have all written code which is hard to read!\nWe all improve over time.\n\n\n\nThe only way to write good code is to write tons of shitty code first. Feeling shame about bad code stops you from getting to good codeâ€” Hadley Wickham (@hadleywickham) April 17, 2015",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#code-formatting-and-style-3",
    "href": "core/week-2/workshop.html#code-formatting-and-style-3",
    "title": "Workshop",
    "section": "Code formatting and style",
    "text": "Code formatting and style\nSome keys points:\n\n\nbe consistent, emulate experienced coders\n\nuse snake_case for variable names (not CamelCase, dot.case)\n\nuse &lt;- (not =) for assignment\n\nuse spacing around most operators and after commas\n\nuse indentation\n\navoid long lines, break up code blocks with new lines\n\nuse \" for quoting text (not ') unless the text contains double quotes\n\nspace after # for comments",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#ugly-code",
    "href": "core/week-2/workshop.html#ugly-code",
    "title": "Workshop",
    "section": "ğŸ˜© Ugly code ğŸ˜©",
    "text": "ğŸ˜© Ugly code ğŸ˜©\n\n\ndata&lt;-read_csv('../data-raw/Y101_Y102_Y201_Y202_Y101-5.csv',skip=2)\nlibrary(janitor);sol&lt;-clean_names(data)\ndata=data|&gt;filter(str_detect(description,\"OS=Homo sapiens\"))|&gt;filter(x1pep=='x')\ndata=data|&gt;\nmutate(g=str_extract(description,\n\"GN=[^\\\\s]+\")|&gt;str_replace(\"GN=\",''))\ndata&lt;-data|&gt;mutate(id=str_extract(accession,\"1::[^;]+\")|&gt;str_replace(\"1::\",\"\"))",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#ugly-code-1",
    "href": "core/week-2/workshop.html#ugly-code-1",
    "title": "Workshop",
    "section": "ğŸ˜© Ugly code ğŸ˜©",
    "text": "ğŸ˜© Ugly code ğŸ˜©\n\nno spacing or indentation\ninconsistent splitting of code blocks over lines\ninconsistent use of quote characters\nno comments\nvariable names convey no meaning\nuse of = for assignment and inconsistently\nmultiple commands on a line\nlibrary statement in the middle of the analysis",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#cool-code",
    "href": "core/week-2/workshop.html#cool-code",
    "title": "Workshop",
    "section": "ğŸ˜ Cool code ğŸ˜",
    "text": "ğŸ˜ Cool code ğŸ˜\n\n\n# Packages ----------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Import ------------------------------------------------------------------\n\n# define file name\nfile &lt;- \"../data-raw/Y101_Y102_Y201_Y202_Y101-5.csv\"\n\n# import: column headers and data are from row 3\nsolu_protein &lt;- read_csv(file, skip = 2) |&gt;\n  clean_names()\n\n# Tidy data ----------------------------------------------------------------\n\n# filter out the bovine proteins and those proteins \n# identified from fewer than 2 peptides\nsolu_protein &lt;- solu_protein |&gt;\n  filter(str_detect(description, \"OS=Homo sapiens\")) |&gt;\n  filter(x1pep == \"x\")\n\n# Extract the genename from description column to a column\n# of its own\nsolu_protein &lt;- solu_protein |&gt;\n  mutate(genename =  str_extract(description,\"GN=[^\\\\s]+\") |&gt;\n           str_replace(\"GN=\", \"\"))\n\n# Extract the top protein identifier from accession column (first\n# Uniprot ID after \"1::\") to a column of its own\nsolu_protein &lt;- solu_protein |&gt;\n  mutate(protid =  str_extract(accession, \"1::[^;]+\") |&gt;\n           str_replace(\"1::\", \"\"))",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#cool-code-1",
    "href": "core/week-2/workshop.html#cool-code-1",
    "title": "Workshop",
    "section": "ğŸ˜ Cool code ğŸ˜",
    "text": "ğŸ˜ Cool code ğŸ˜\n\nlibrary() calls collected\nUses code sections to make it easier to navigate\nUses white space and proper indentation\nCommented\nUses more informative name for the dataframe",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#code-algorithmically-1",
    "href": "core/week-2/workshop.html#code-algorithmically-1",
    "title": "Workshop",
    "section": "Code â€˜algorithmicallyâ€™",
    "text": "Code â€˜algorithmicallyâ€™\n\nWrite code which expresses the structure of the problem/solution.\nAvoid hard coding numbers if at all possible - declare variables instead\nDeclare frequently used values as variables at the start e.g., colour schemes, figure saving settings",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#hard-coding-numbers.",
    "href": "core/week-2/workshop.html#hard-coding-numbers.",
    "title": "Workshop",
    "section": "ğŸ˜© Hard coding numbers.",
    "text": "ğŸ˜© Hard coding numbers.\n\nSuppose we want to calculate the sums of squares, \\(SS(x)\\), for the number of eggs in five nests.\nThe formula is given by: \\(\\sum (x_i- \\bar{x})^2\\)\nWe could calculate the mean and copy it, and the individual numbers into the formula",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#hard-coding-numbers.-1",
    "href": "core/week-2/workshop.html#hard-coding-numbers.-1",
    "title": "Workshop",
    "section": "ğŸ˜© Hard coding numbers.",
    "text": "ğŸ˜© Hard coding numbers.\n\n# mean number of eggs per nest\nsum(3, 5, 6, 7, 8) / 5\n\n[1] 5.8\n\n# ss(x) of number of eggs\n(3 - 5.8)^2 + (5 - 5.8)^2 + (6 - 5.8)^2 + (7 - 5.8)^2 + (8 - 5.8)^2\n\n[1] 14.8\n\n\nI am coding the calculation of the mean rather using the mean() function only to explain what â€˜coding algorithmicallyâ€™ means using a simple example.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#hard-coding-numbers",
    "href": "core/week-2/workshop.html#hard-coding-numbers",
    "title": "Workshop",
    "section": "ğŸ˜© Hard coding numbers",
    "text": "ğŸ˜© Hard coding numbers\n\nif any of the sample numbers must be altered, all the code needs changing\nit is hard to tell that the output of the first line is a mean\nits hard to recognise that the numbers in the mean calculation correspond to those in the next calculation\nit is hard to tell that 5 is just the number of nests\nno way of know if numbers are the same by coincidence or they refer to the same thing",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#better",
    "href": "core/week-2/workshop.html#better",
    "title": "Workshop",
    "section": "ğŸ˜ Better",
    "text": "ğŸ˜ Better\n\n# eggs each nest\neggs &lt;- c(3, 5, 6, 7, 8)\n\n# mean eggs per nest\nmean_eggs &lt;- sum(eggs) / length(eggs)\n\n# ss(x) of number of eggs\nsum((eggs - mean_eggs)^2)\n\n[1] 14.8",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#better-1",
    "href": "core/week-2/workshop.html#better-1",
    "title": "Workshop",
    "section": "ğŸ˜ Better",
    "text": "ğŸ˜ Better\n\nthe commenting is similar but it is easier to follow\nif any of the sample numbers must be altered, only that number needs changing\nassigning a value you will later use to a variable with a meaningful name allows us to understand the first and second calculations\nmakes use of Râ€™s elementwise calculation which resembles the formula (i.e., is expressed as the general rule)",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#naming-things",
    "href": "core/week-2/workshop.html#naming-things",
    "title": "Workshop",
    "section": "Naming things",
    "text": "Naming things\n\n\n\n\ndocuments, CC-BY-NC, https://xkcd.com/1459/\n\n\nGuiding principle - Have a convention! Good file names are:\n\nmachine readable\nhuman readable\nplay nicely with sorting",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#naming-suggestions",
    "href": "core/week-2/workshop.html#naming-suggestions",
    "title": "Workshop",
    "section": "Naming suggestions",
    "text": "Naming suggestions\n\nno spaces in names\nuse snake_case or kebab-case rather than CamelCase or dot.case\nuse all lower case except very occasionally where convention is otherwise, e.g., README, LICENSE\nordering: use left-padded numbers e.g., 01, 02â€¦.99 or 001, 002â€¦.999\ndates ISO 8601 format: 2020-10-16\nwrite down your conventions",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#summary",
    "href": "core/week-2/workshop.html#summary",
    "title": "Workshop",
    "section": "Summary",
    "text": "Summary\n\nUse an RStudio project for any R work (you can also incorporate other languages)\nWrite Cool code not Ugly code: space, consistency, indentation, comments, meaningful variable names\nWrite code which expresses the structure of the problem/solution.\nAvoid hard coding numbers if at all possible - declare variables instead",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#reading",
    "href": "core/week-2/workshop.html#reading",
    "title": "Workshop",
    "section": "Reading",
    "text": "Reading\nCompletely optional suggestions for further reading\n\n\n\nProject-oriented workflow | What They Forgot to Teach You About R (Bryan et al., n.d.). Recommended if you still need convincing to use RStudio Projects\nTen simple rules for reproducible computational research (Sandve et al. 2013)\n\nGood enough practices in scientific computing (Wilson et al. 2017)\n\nExcuse Me, Do You Have a Moment to Talk About Version Control? (Bryan 2018)\n\n\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr (Xie 2024, 2015, 2014), kableExtra (Zhu 2021)",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#references",
    "href": "core/week-2/workshop.html#references",
    "title": "Workshop",
    "section": "References",
    "text": "References\n\n\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2024. â€œQuarto.â€ https://doi.org/10.5281/zenodo.5960048.\n\n\nBaggerly, Keith A, and Kevin R Coombes. 2009. â€œDERIVING CHEMOSENSITIVITY FROM CELL LINES: FORENSIC BIOINFORMATICS AND REPRODUCIBLE RESEARCH IN HIGH-THROUGHPUT BIOLOGY.â€ Ann. Appl. Stat. 3 (4): 1309â€“34. http://www.jstor.org/stable/27801549.\n\n\nBryan, Jennifer. 2018. â€œExcuse Me, Do You Have a Moment to Talk about Version Control?â€ Am. Stat. 72 (1): 20â€“27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\nBryan, Jennifer, Jim Hester, Shannon Pileggi, and E. David Aja. n.d. What They Forgot to Teach You about r. https://rstats.wtf/.\n\n\nMarkowetz, Florian. 2015. â€œFive Selfish Reasons to Work Reproducibly.â€ Genome Biol. 16 (December): 274. https://doi.org/10.1186/s13059-015-0850-7.\n\n\nNational Academies of Sciences, Engineering, Medicine, Policy, Global Affairs, Engineering, Medicine Committee on Science, Public Policy, Board on Research Data, et al. 2019. Understanding Reproducibility and Replicability. National Academies Press (US). https://www.ncbi.nlm.nih.gov/books/NBK547546/.\n\n\nOECD Global Science Forum. 2020. â€œBuilding Digital Workforce Capacity and Skills for Data-Intensive Science.â€ http://www.oecd.org/officialdocuments/publicdisplaydocumentpdf/?cote=DSTI/STP/GSF(2020)6/FINAL&docLanguage=En.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig. 2013. â€œTen Simple Rules for Reproducible Computational Research.â€ PLoS Comput. Biol. 9 (10): e1003285. https://doi.org/10.1371/journal.pcbi.1003285.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K Teal. 2017. â€œGood Enough Practices in Scientific Computing.â€ PLoS Comput. Biol. 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nXie, Yihui. 2014. â€œKnitr: A Comprehensive Tool for Reproducible Research in R.â€ In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\nâ€”â€”â€”. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nâ€”â€”â€”. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. â€œkableExtra: Construct Complex Table with â€™Kableâ€™ and Pipe Syntax.â€ https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "structures/structures.html",
    "href": "structures/structures.html",
    "title": "Structure Data Analysis for Group Project",
    "section": "",
    "text": "There is an RStudio project containing a Quarto version of the the Antibody Mimetics Workshop by Michael Plevin & Jon Agirre. Instructions to obtain the RStudio project are at the bottom of this document after the set up instructions.\nYou might find RStudio useful for Python because you are already familiar with it. It is also a good way to create Quarto documents with code chunks in more than one language. Quarto documents can be used in RStudio, VS Code or Jupyter notebooks\nSome set up is required before you will be able to execute code in antibody_mimetics_workshop_3.qmd. This in contrast to the Colab notebook which is a cloud-based Jupyter notebook and does not require any set up (except installing packages).\n\nğŸ¬ If using your own machine, install Python from https://www.python.org/downloads/. This should not be necessary if you are using a university machine where Python is already installed.\nğŸ¬ If using your own machine and you did not install Quarto in the Core 1 workshop, install it now from https://quarto.org/docs/get-started/. This should not be necessary if you are using a university machine where quarto is already installed.\nğŸ¬ Open RStudio and check you are using a â€œGit bashâ€ Terminal: Tools | Global Options| Terminal | New Terminal opens withâ€¦ . If the option to choose Git bash, you will need to install Git from https://git-scm.com/downloads. Quit RStudio first. This should not be necessary if you are using a university machine where Git bash is already installed.\nğŸ¬ If on your own machine: In RStudio, install the quarto and the recticulate packages. This should not be necessary if you are using a university machine where these packages are already installed.\nğŸ¬ Whether you are using your own machine or a university machine, you need to install some python packages. In RStudio and go to the Terminal window (behind the Console window). Run the following commands in the Terminal window:\npython -m pip install --upgrade pip setuptools wheel\nYou may get these warnings about scripts not being on the path. You can ignore these.\n  WARNING: The script wheel.exe is installed in 'C:\\Users\\er13\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The scripts pip.exe, pip3.11.exe, pip3.9.exe and pip3.exe are installed in 'C:\\Users\\er13\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspyder 5.1.5 requires pyqt5&lt;5.13, which is not installed.\nspyder 5.1.5 requires pyqtwebengine&lt;5.13, which is not installed.\nconda-repo-cli 1.0.4 requires pathlib, which is not installed.\nanaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\nSuccessfully installed pip-23.3.1 setuptools-69.0.2 wheel-0.41.3\npython -m pip install session_info\npython -m pip install wget\npython -m pip install gemmi\nNote: On my windows laptop at home, I also had to install C++ Build Tools to be able to install the gemmi python package. If this is true for you, you will get a fail message telling you to install C++ build tools if you need them. These are from https://visualstudio.microsoft.com/visual-cpp-build-tools/ You need to check the Workloads tab and select C++ build tools.\n\nYou can then install the gemmi package again.\nI think thatâ€™s it! You can now download the RStudio project and run each chunk in the quarto document.\nThere is an example RStudio project here: structure-analysis. You can also download the project as a zip file from there but there is some code that will do that automatically for you. Since this is an RStudio Project, do not run the code from inside a project. You may want to navigate to a particular directory or edit the destdir:\n\nusethis::use_course(url = \"3mmaRand/structure-analysis\", destdir = \".\")\n\nYou can agree to deleting the zip. You should find RStudio restarts and you have a new project called structure-analysis-xxxxxx. The xxxxxx is a commit reference - you do not need to worry about that, it is just a way to tell you which version of the repo you downloaded.\nYou should be able to open the antibody_mimetics_workshop_3.qmd file and run each chunk. You can also knit the document to html.",
    "crumbs": [
      "Structure Analysis",
      "Structure Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "structures/structures.html#programmatic-protein-structure-analysis",
    "href": "structures/structures.html#programmatic-protein-structure-analysis",
    "title": "Structure Data Analysis for Group Project",
    "section": "",
    "text": "There is an RStudio project containing a Quarto version of the the Antibody Mimetics Workshop by Michael Plevin & Jon Agirre. Instructions to obtain the RStudio project are at the bottom of this document after the set up instructions.\nYou might find RStudio useful for Python because you are already familiar with it. It is also a good way to create Quarto documents with code chunks in more than one language. Quarto documents can be used in RStudio, VS Code or Jupyter notebooks\nSome set up is required before you will be able to execute code in antibody_mimetics_workshop_3.qmd. This in contrast to the Colab notebook which is a cloud-based Jupyter notebook and does not require any set up (except installing packages).\n\nğŸ¬ If using your own machine, install Python from https://www.python.org/downloads/. This should not be necessary if you are using a university machine where Python is already installed.\nğŸ¬ If using your own machine and you did not install Quarto in the Core 1 workshop, install it now from https://quarto.org/docs/get-started/. This should not be necessary if you are using a university machine where quarto is already installed.\nğŸ¬ Open RStudio and check you are using a â€œGit bashâ€ Terminal: Tools | Global Options| Terminal | New Terminal opens withâ€¦ . If the option to choose Git bash, you will need to install Git from https://git-scm.com/downloads. Quit RStudio first. This should not be necessary if you are using a university machine where Git bash is already installed.\nğŸ¬ If on your own machine: In RStudio, install the quarto and the recticulate packages. This should not be necessary if you are using a university machine where these packages are already installed.\nğŸ¬ Whether you are using your own machine or a university machine, you need to install some python packages. In RStudio and go to the Terminal window (behind the Console window). Run the following commands in the Terminal window:\npython -m pip install --upgrade pip setuptools wheel\nYou may get these warnings about scripts not being on the path. You can ignore these.\n  WARNING: The script wheel.exe is installed in 'C:\\Users\\er13\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The scripts pip.exe, pip3.11.exe, pip3.9.exe and pip3.exe are installed in 'C:\\Users\\er13\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspyder 5.1.5 requires pyqt5&lt;5.13, which is not installed.\nspyder 5.1.5 requires pyqtwebengine&lt;5.13, which is not installed.\nconda-repo-cli 1.0.4 requires pathlib, which is not installed.\nanaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\nSuccessfully installed pip-23.3.1 setuptools-69.0.2 wheel-0.41.3\npython -m pip install session_info\npython -m pip install wget\npython -m pip install gemmi\nNote: On my windows laptop at home, I also had to install C++ Build Tools to be able to install the gemmi python package. If this is true for you, you will get a fail message telling you to install C++ build tools if you need them. These are from https://visualstudio.microsoft.com/visual-cpp-build-tools/ You need to check the Workloads tab and select C++ build tools.\n\nYou can then install the gemmi package again.\nI think thatâ€™s it! You can now download the RStudio project and run each chunk in the quarto document.\nThere is an example RStudio project here: structure-analysis. You can also download the project as a zip file from there but there is some code that will do that automatically for you. Since this is an RStudio Project, do not run the code from inside a project. You may want to navigate to a particular directory or edit the destdir:\n\nusethis::use_course(url = \"3mmaRand/structure-analysis\", destdir = \".\")\n\nYou can agree to deleting the zip. You should find RStudio restarts and you have a new project called structure-analysis-xxxxxx. The xxxxxx is a commit reference - you do not need to worry about that, it is just a way to tell you which version of the repo you downloaded.\nYou should be able to open the antibody_mimetics_workshop_3.qmd file and run each chunk. You can also knit the document to html.",
    "crumbs": [
      "Structure Analysis",
      "Structure Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#overview",
    "href": "transcriptomics/week-3/study_before_workshop.html#overview",
    "title": "Independent Study to prepare for workshop",
    "section": "Overview",
    "text": "Overview\n\nConcise summary of the experimental design and aims\nWhat the raw data consist of\nWhat has been done to the data so far\nWhat steps we will take in the workshop",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#the-data",
    "href": "transcriptomics/week-3/study_before_workshop.html#the-data",
    "title": "Independent Study to prepare for workshop",
    "section": "The Data",
    "text": "The Data\nThere are 3 transcriptomic datasets\n\nğŸ„ bulk RNA-seq from Arabidopsis thaliana\nğŸ’‰ bulk RNA-seq from Leishmania mexicana\nğŸ­ single cell RNA-seq from mouse stemcells",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-1",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ Experimental design",
    "text": "ğŸ„ Experimental design\n\nSchematic of arabidopsis experiment",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-2",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-2",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ Experimental design",
    "text": "ğŸ„ Experimental design\n\nSchematic of arabidopsis experiment\n2 plant tissues\n2 nickel conditions\n6 replicates\n2 x 2 x 6 = 24 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-3",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-3",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ Experimental design",
    "text": "ğŸ„ Experimental design\n\nSchematic of arabidopsis experiment\n2 plant tissues: root and aerial. This is the tissue treatment\n2 nickel conditions: control and low Ni. This is the Ni treatment\n6 replicates. These are the replicates\n2 x 2 x 6 = 24 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#aim",
    "href": "transcriptomics/week-3/study_before_workshop.html#aim",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ Aim",
    "text": "ğŸ„ Aim\n\nFind genes that are â€œdifferentially expressedâ€ between tissue types and nickel conditions e.g.Â root tissue grown under control and low Ni conditions\nDifferentially expressed means the expression in one group is significantly higher than in the other",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#guided-analysis",
    "href": "transcriptomics/week-3/study_before_workshop.html#guided-analysis",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ Guided analysis",
    "text": "ğŸ„ Guided analysis\n\nThe workshops will take you through comparing the root tissue grown under control and low Ni conditions\nYou will make other comparisons independently\nYou will be guided to carefully document your work so you can apply the same methods to other comparisons\nDo the independent study before and after the workshop!",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-4",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-4",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ Experimental design",
    "text": "ğŸ’‰ Experimental design\n\nSchematic of leishmania experiment",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-5",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-5",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ Experimental design",
    "text": "ğŸ’‰ Experimental design\n\nSchematic of leishmania experiment\n3 stages\n3 samples\n3 x 3 = 9 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-6",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-6",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ Experimental design",
    "text": "ğŸ’‰ Experimental design\n\nSchematic of leishmania experiment\nthree stages: procyclic promastigotes, metacyclic promastigotes and amastigotes. This is the stage treatment\nthree samples. These are the replicates\n3 x 3 = 9 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#aim-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#aim-1",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ Aim",
    "text": "ğŸ’‰ Aim\n\nFind genes that are â€œdifferentially expressedâ€ between stages e.g., procyclic promastigotes and the metacyclic promastigotes\nDifferentially expressed means the expression in one group is significantly higher than in the other",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-1",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ Guided analysis",
    "text": "ğŸ’‰ Guided analysis\n\nThe workshops will take you through comparing the procyclic promastigotes and the metacyclic promastigotes\nYou will make other comparisons independently\nYou will be guided to carefully document your work so you can apply the same methods to other comparisons\nDo the independent study before and after the workshop!",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-7",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-7",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Experimental design",
    "text": "ğŸ­ Experimental design\n\nSchematic of stem cell experiment",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-8",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-8",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Experimental design",
    "text": "ğŸ­ Experimental design\n\nSchematic of stem cell experiment\nCells were sorted using flow cytometry on the basis of cell surface markers\nThere are 3 cell types\nMany cells of each cell type were sequenced",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-9",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-9",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Experimental design",
    "text": "ğŸ­ Experimental design\n\nSchematic of stem cell experiment\nThere are three cell types: LT-HSCs, HSPCs, Progs This is the cell â€œtreatmentâ€\nMany cells of each type were sequenced: These are the replicates\n155 LT-HSCs, 701 HSPCs, 798 Progs",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#aim-2",
    "href": "transcriptomics/week-3/study_before_workshop.html#aim-2",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Aim",
    "text": "ğŸ­ Aim\n\nfind genes that are â€œdifferentially expressedâ€ between at least two cell types\nDifferentially expressed means the expression in one group is significantly higher than in the other",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-2",
    "href": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-2",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Guided analysis",
    "text": "ğŸ­ Guided analysis\n\nThe workshops will take you through comparing the HSPC and Prog cells\nYou will make other comparisons independently\nYou will be guided to carefully document your work so you can apply the same methods to other comparisons\nDo the independent study before and after the workshop!",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#raw-sequence-data",
    "href": "transcriptomics/week-3/study_before_workshop.html#raw-sequence-data",
    "title": "Independent Study to prepare for workshop",
    "section": "Raw Sequence data",
    "text": "Raw Sequence data\n\nThe raw data are â€œreadsâ€ from a sequencing machine in FASTQ files\nA read is sequence of RNA which is shorter than the whole transcriptome\nThe length of the reads depends on the type of sequencing machine\n\nShort-read technologies (e.g.Â Illumina) have higher base accuracy but are harder to align\nLong-read technologies (e.g.Â Nanopore) have lower base accuracy but are easier to align",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#raw-sequence-data-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#raw-sequence-data-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Raw Sequence data",
    "text": "Raw Sequence data\nOptional\nYou can read more about Sequencing technologies in Statistically useful experimental design(Rand and Forrester 2022)",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#general-steps",
    "href": "transcriptomics/week-3/study_before_workshop.html#general-steps",
    "title": "Independent Study to prepare for workshop",
    "section": "General steps",
    "text": "General steps\n\nReads are filtered and trimmed on the basis of a quality score\nThey are then aligned/pseudo-aligned to a reference genome/transcriptome (or assembled de novo)\nAnd then counted to quantify the expression\nCounts need to be normalised to account for differences in sequencing depth and transcript length before, or as part of, statistical analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#data",
    "href": "transcriptomics/week-3/study_before_workshop.html#data",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ Data",
    "text": "ğŸ„ Data\n\nBrand spanking new! Provided by Alex Marks (Marks and Rylott, n.d.)\nExpression for the whole transcriptome ENSEMBL Arabidopsis TAIR10(Yates et al. 2022)\nValues are raw counts\nThe statistical analysis method we will use is DESeq2 (Love, Huber, and Anders 2014). It requires raw counts and performs the normalisation itself.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#data-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#data-1",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ Data",
    "text": "ğŸ’‰ Data\n\nBrand spanking new!\nExpression for the whole transcriptome L. mexicana MHOM/GT/2001/U1103(Rogers et al. 2011)\nValues are raw counts\nThe statistical analysis method we will use is DESeq2 (Love, Huber, and Anders 2014). It requires raw counts and performs the normalisation itself.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#data-2",
    "href": "transcriptomics/week-3/study_before_workshop.html#data-2",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Data",
    "text": "ğŸ­ Data\n\nPublished in Nestorowa et al. (2016)\n\n\nExpression for a subset of genes, the secretome\n\n\nValues are log2 normalised values\nThe statistical analysis method we will use is scran (Lun, McCarthy, and Marioni 2016) and it requires normalised values",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#workshops-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#workshops-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Workshops",
    "text": "Workshops\n\nTranscriptomics 1: Hello data Getting to know the data. Checking the distributions of values overall, across rows and columns to check things are as we expect and detect rows/columns that need to be removed\nTranscriptomics 2: Statistical Analysis. Identifying which genes are differentially expressed between treatments. This is the main analysis step. We will use different methods for bulk and single cell data.\nTranscriptomics 3: Visualising. Principal Component Analysis (PCA) and volcano plots to visualise the results of the analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#references",
    "href": "transcriptomics/week-3/study_before_workshop.html#references",
    "title": "Independent Study to prepare for workshop",
    "section": "References",
    "text": "References\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr (Xie 2024, 2015, 2014), kableExtra (Zhu 2021)\n\n\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2024. â€œQuarto.â€ https://doi.org/10.5281/zenodo.5960048.\n\n\nLove, Michael I., Wolfgang Huber, and Simon Anders. 2014. â€œModerated Estimation of Fold Change and Dispersion for RNA-Seq Data with DESeq2.â€ Genome Biology 15: 550. https://doi.org/10.1186/s13059-014-0550-8.\n\n\nLun, Aaron T. L., Davis J. McCarthy, and John C. Marioni. 2016. â€œA Step-by-Step Workflow for Low-Level Analysis of Single-Cell RNA-Seq Data with Bioconductor.â€ F1000Res. 5: 2122. https://doi.org/10.12688/f1000research.9501.2.\n\n\nMarks, Alex, and Elizabeth Rylott. n.d. â€œNickel Dosing Arabidopsis Thaliana RNA-Seq.â€\n\n\nNestorowa, Sonia, Fiona K. Hamey, Blanca Pijuan Sala, Evangelia Diamanti, Mairi Shepherd, Elisa Laurenti, Nicola K. Wilson, David G. Kent, and Berthold GÃ¶ttgens. 2016. â€œA Single-Cell Resolution Map of Mouse Hematopoietic Stem and Progenitor Cell Differentiation.â€ Blood 128 (8): e20â€“31. https://doi.org/10.1182/blood-2016-05-716480.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRand, Emma, and Sarah Forrester. 2022. â€œStatistically Useful Experimental Design.â€ https://cloud-span.github.io/experimental_design00-overview/.\n\n\nRogers, Matthew B., James D. Hilley, Nicholas J. Dickens, Jon Wilkes, Paul A. Bates, Daniel P. Depledge, David Harris, et al. 2011. â€œChromosome and Gene Copy Number Variation Allow Major Structural Change Between Species and Strains of Leishmania.â€ Genome Research 21 (12): 2129â€“42. https://doi.org/10.1101/gr.122945.111.\n\n\nXie, Yihui. 2014. â€œKnitr: A Comprehensive Tool for Reproducible Research in R.â€ In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\nâ€”â€”â€”. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nâ€”â€”â€”. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nYates, Andrew D, James Allen, Ridwan M Amode, Andrey G Azov, Matthieu Barba, AndrÃ©s Becerra, Jyothish Bhai, et al. 2022. â€œEnsembl Genomes 2022: An Expanding Genome Resource for Non-Vertebrates.â€ Nucleic Acids Research 50 (D1): D996â€“1003. https://doi.org/10.1093/nar/gkab1007.\n\n\nZhu, Hao. 2021. â€œkableExtra: Construct Complex Table with â€™Kableâ€™ and Pipe Syntax.â€ https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/overview.html",
    "href": "transcriptomics/week-3/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week you will meet your data. The independent study will summarise how these data were generated and how they have been processed before being given to you. There will also be an overview of the analysis we will carry out over three workshops. In the workshop, you will learn what steps to take to get a good understanding of transcriptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It gives you the deep understanding of the data structures and values that you will need to code and trouble-shoot code, allows you to spot failed or problematic samples and informs your decisions on quality control.\nWe suggest you sit together with your group in the workshop.\n\nLearning objectives\nThe successful student will be able to:\n\nexplore transcriptomics data to find the number of rows and columns and know how these correspond to samples and variables\nexplore the distribution of expression measures across whole data sets, across variables and across samples by summarising and plotting\nexplain what distributions are expected and interpret the distributions they have\nexplain on what basis we might filter out variables or samples\nimport, explore and filter transcriptomics data reproducibly so they can understand and reuse their code in the future\n\n\n\nInstructions\n\nPrepare\n\nğŸ“– Read how the data were generated and how they have been processed so far and a summary of the analysis we will carry out over three workshops.\n\nWorkshop\n\nğŸ’» Set up a Project\nğŸ’» Import data\nğŸ’» Explore the distribution of values across rows and columns\nğŸ’» Look after future you!\n\nConsolidate\n\nğŸ’» Use the work you completed in the workshop as a template to apply to a new case.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "About"
    ]
  },
  {
    "objectID": "transcriptomics/semester-2/stem_cell_variant.html",
    "href": "transcriptomics/semester-2/stem_cell_variant.html",
    "title": "Self study",
    "section": "",
    "text": "We have analysed the Nestorowa et al. (2016) stem cells categorised as LTHSC, HSPC, or Prog. The cell types were determined from flowcytometry markers. Additional markers allow the cells to be categorised into 12 other types (including unknown): CMP, GMP, LMPP, LTHSC, LTHSC-ESLAM, MEP, MPP_broad, MPP1, MPP2, MPP3, STHSC, unknown. These cell types are mutually exclusive, meaning that a cell can only be one of these types.\nIn this self-study, you will:\n\nimport all the expression data and gene information for the three cell types\nimport the data that gives the additional labels\ncombine the data and pivot it to have the expression values in a single column\nplot the expression of one gene, Emb, in each cell type for the 3-type categorisation and the 12-type categorisation\nconduct a PCA on all the data and plot the first two components using colour/shapes for the 12-type categorisation, the 3-type categorisation and the expression of Emb.",
    "crumbs": [
      "Transcriptomics",
      "Semester 2: Extras",
      "Additional ğŸ­ stem cell labels"
    ]
  },
  {
    "objectID": "transcriptomics/semester-2/stem_cell_variant.html#the-raw-data",
    "href": "transcriptomics/semester-2/stem_cell_variant.html#the-raw-data",
    "title": "Self study",
    "section": "The raw data",
    "text": "The raw data\nImport the data for the LT-HSC, HSPC and the Progenitor cells.\nğŸ¬ Import surfaceome_lthsc.csv, surfaceome_hspc.csv and surfaceome_hspc.csv\n\n# ğŸ­ import the three datasets\nlthsc &lt;- read_csv(\"data-raw/surfaceome_lthsc.csv\")\nhspc &lt;- read_csv(\"data-raw/surfaceome_hspc.csv\")\nprog &lt;- read_csv(\"data-raw/surfaceome_prog.csv\")\n\nNote there are 155 LT-HSC + 701 HSPC + 798 Progenitor = 1654 total cells.\nWe will need to join the three datasets using ensembl_gene_id to match the rows.\nğŸ¬ Combine the three datasets by ensembl_gene_id and save the result as cell_expr.\n\n#  combine the three datasets\ncell_expr &lt;- hspc |&gt;\n  left_join(prog, \n            by = \"ensembl_gene_id\") |&gt; \n  left_join(lthsc, \n            by = \"ensembl_gene_id\")",
    "crumbs": [
      "Transcriptomics",
      "Semester 2: Extras",
      "Additional ğŸ­ stem cell labels"
    ]
  },
  {
    "objectID": "transcriptomics/semester-2/stem_cell_variant.html#gene-information",
    "href": "transcriptomics/semester-2/stem_cell_variant.html#gene-information",
    "title": "Self study",
    "section": "Gene information",
    "text": "Gene information\nYou will likely want the gene information in the dataframe along with the ensembl_gene_id. Rather the connecting again to Ensembl (Martin et al. 2023; Birney et al. 2004) and BioMart (Smedley et al. 2009) using the R package biomaRt (Durinck et al. 2009, 2005), which means loading more packages, managing conflicts and using more compute, we can use the information we already have in the results/hspc_prog_results.csv file. This works because the same 280 genes are in the three raw data files and this results file.\nWe need only the ensembl_gene_id, (to join the information), the external_gene_name and the description.\nğŸ¬ Import the gene information from results/hspc_prog_results.csv and save as gene_info.\n\n# ğŸ­ import the gene information\ngene_info &lt;- read_csv(\"results/hspc_prog_results.csv\") |&gt; \n  select(ensembl_gene_id, external_gene_name, description)\n\nğŸ¬ Join the gene_info to the cell_expr data.\n\n# ğŸ­ join the gene information to the cell_expr data\ncell_expr &lt;- cell_expr |&gt; \n  left_join(gene_info, \n            by = \"ensembl_gene_id\")",
    "crumbs": [
      "Transcriptomics",
      "Semester 2: Extras",
      "Additional ğŸ­ stem cell labels"
    ]
  },
  {
    "objectID": "transcriptomics/semester-2/stem_cell_variant.html#type-categorisation-data",
    "href": "transcriptomics/semester-2/stem_cell_variant.html#type-categorisation-data",
    "title": "Self study",
    "section": "12-type categorisation data",
    "text": "12-type categorisation data\nThe information in the er_cell_types.csv has 14 columns. The first column gives the cell_id (e.g., HSPC_001) the next 12 columns are named with the cell type or unknown and contain a 0 or 1. These columns are useful if you want to compare (plot or DE), for example, CMP cells vs others. The last column gives the cell type which is useful to include all the labels in a plot.\nğŸ¬ Import er_cell_types.csv\n\ncell &lt;- read_csv(\"data-raw/er_cell_types.csv\")\n\nğŸ¬ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nğŸ¬ Examine the number of cells of each type:\n\ncell |&gt; \n  group_by(cell_type_12) |&gt;\n  count()\n\n# A tibble: 12 Ã— 2\n# Groups:   cell_type_12 [12]\n   cell_type_12     n\n   &lt;chr&gt;        &lt;int&gt;\n 1 CMP            317\n 2 GMP            120\n 3 LMPP           246\n 4 LTHSC          233\n 5 LTHSC-ESLAM     38\n 6 MEP            354\n 7 MPP1            27\n 8 MPP2            11\n 9 MPP3            60\n10 MPP_broad      202\n11 STHSC           36\n12 unknown         10",
    "crumbs": [
      "Transcriptomics",
      "Semester 2: Extras",
      "Additional ğŸ­ stem cell labels"
    ]
  },
  {
    "objectID": "transcriptomics/semester-2/stem_cell_variant.html#pivot-to-long-form",
    "href": "transcriptomics/semester-2/stem_cell_variant.html#pivot-to-long-form",
    "title": "Self study",
    "section": "Pivot to long form",
    "text": "Pivot to long form\nThe cell_expr data has the expression of each gene in each cell. The cell_ids are in columns and the gene are in rows.\nWe will need to pivot the cell_expr data to have the expression values in a single column with additional columns giving the cell_id and gene.\n\n# ğŸ­ pivot the cell_expr data\ncell_expr_long &lt;- cell_expr |&gt; \n  pivot_longer(cols = -c(ensembl_gene_id, \n                         external_gene_name, \n                         description),\n               names_to = \"cell_id\", \n               values_to = \"expression\")\n\nNote that this data frame will have: [280 genes * 1654 cells = 463120] rows.",
    "crumbs": [
      "Transcriptomics",
      "Semester 2: Extras",
      "Additional ğŸ­ stem cell labels"
    ]
  },
  {
    "objectID": "transcriptomics/semester-2/stem_cell_variant.html#add-the-two-types-of-categorisation-to-the-expression-data",
    "href": "transcriptomics/semester-2/stem_cell_variant.html#add-the-two-types-of-categorisation-to-the-expression-data",
    "title": "Self study",
    "section": "Add the two types of categorisation to the expression data",
    "text": "Add the two types of categorisation to the expression data\nWe also want to add the two types of categorisation (the 3-type and the 12-types) to the cell_expr_long data. To add the 3-type categorisation we extract the information in the cell_id column into two columns: one with the cell type and one with the number. We did this before in making a PCA plot\nğŸ¬ Extract the cell type and cell number from the cell_id column (keeping the cell_id column):\n\ncell_expr_long &lt;- cell_expr_long |&gt; \n  extract(cell_id, \n          remove = FALSE,\n          c(\"cell_type_3\", \"cell_number\"),\n          \"([a-zA-Z.]{4,6})_([0-9]{3})\")\n\nLetâ€™s just check we have the right number of cells in each of the three groups:\n\ncell_expr_long |&gt; \n  group_by(cell_type_3) |&gt; \n  summarise(n()/ 280)\n\n# A tibble: 3 Ã— 2\n  cell_type_3 `n()/280`\n  &lt;chr&gt;           &lt;dbl&gt;\n1 HSPC              701\n2 LT.HSC            155\n3 Prog              798\n\n\nTo add the 12-type categorisation we need to join the cell_expr_long data to the cell data on the cell_id\nğŸ¬ Add the 12-type categorisation to the cell_expr_long data.\n\ncell_expr_long &lt;- cell_expr_long |&gt; \n  left_join(cell |&gt; select(cell_id, cell_type_12), \n            by = \"cell_id\")\n\nPlot gene expression under 3-type categorisation\nI have chosen the gene Emb to plot the expression in each cell type. I prefer the use of violin plots and points because multimodal distributions are more obvious than in boxplots\nğŸ¬ Plot the expression of Emb in each cell type for the 3-type categorisation.\n\n# ğŸ­ plot the expression of Emb in each cell type\ncell_expr_long |&gt; \n  filter(external_gene_name == \"Emb\") |&gt; \n  ggplot(aes(x = cell_type_3, y = expression)) +\n  geom_violin(linewidth = 1) +\n  geom_jitter(width = 0.4, alpha = 0.2, pch = 16) +\n  theme_classic()\n\n\n\n\n\n\n\nPlot expression under 12-type categorisation\n\n# ğŸ­ plot the expression of Emb in each cell type\ncell_expr_long |&gt; \n  filter(external_gene_name == \"Emb\") |&gt; \n  ggplot(aes(x = cell_type_12, y = expression)) +\n  geom_violin(linewidth = 1) +\n  geom_jitter(width = 0.2, alpha = 0.2, pch = 16) +\n  theme_classic()",
    "crumbs": [
      "Transcriptomics",
      "Semester 2: Extras",
      "Additional ğŸ­ stem cell labels"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#overview",
    "href": "transcriptomics/week-4/study_before_workshop.html#overview",
    "title": "Independent Study to prepare for workshop",
    "section": "Overview",
    "text": "Overview\nIn these slides we will:\n\nCheck where you are following week 3\n\nlearn some concepts in differential expression\n\nlog2 fold changes\nMultiple correction\nnormalisation\nstatistical model\n\n\nFind out what packages we will use",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#what-we-did-in-transcriptomics-1-hello-data",
    "href": "transcriptomics/week-4/study_before_workshop.html#what-we-did-in-transcriptomics-1-hello-data",
    "title": "Independent Study to prepare for workshop",
    "section": "What we did in Transcriptomics 1: ğŸ‘‹ Hello data!",
    "text": "What we did in Transcriptomics 1: ğŸ‘‹ Hello data!\n\n\nDiscovered how many rows and columns we had in our datasets and what these were.\nExamined the distribution of values\n\nacross the whole dataset\nacross the samples/cells (i.e., averaged over genes) to see variation between samples/cells\nacross the genes (i.e., averaged over samples/cells) to see variation between genes\n\n\nFiltered data for quality control and wrote to file (except ğŸ­)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#where-should-you-be-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#where-should-you-be-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Where should you be?",
    "text": "Where should you be?\nAfter the Transcriptomics 1: ğŸ‘‹ Hello data! Workshop including:\n\nğŸ¤— Look after future you! and\nthe Independent Study to consolidate, you should have:",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#arabidopsis",
    "href": "transcriptomics/week-4/study_before_workshop.html#arabidopsis",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ Arabidopsis\n",
    "text": "ğŸ„ Arabidopsis\n\nAn RStudio Project called arab-88H which contains:\n\ndata-raw: arabidopsis-root.csv, arabidopsis-aerial.csv\ndata-processed: root_filtered.csv, aerial_filtered.csv\nTwo scripts: cont-low-root.R, cont-low-aerial.R",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#leishmania",
    "href": "transcriptomics/week-4/study_before_workshop.html#leishmania",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nAn RStudio Project called leish-88H which contains:\n-   data-raw: `leishmania-mex-ama.csv`, `leishmania-mex-pro.csv`,\n    `leishmania-mex-meta.csv`\n-   data-processed: `pro_meta_filtered.csv`, `pro_ama_filtered.csv`\n-   Two scripts: `pro_meta.R`, `pro_ama.R`",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#stem-cells",
    "href": "transcriptomics/week-4/study_before_workshop.html#stem-cells",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nAn RStudio Project called mice-88H which contains\n\ndata-raw: secretome_hspc.csv, secretome_prog.csv, secretome_lthsc.csv\n\n\n\ndata-processed: hspc_prog.csv, hspc_lthsc.csv\n\nTwo scripts: hspc-prog.R, hspc-lthsc.R",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#additionally",
    "href": "transcriptomics/week-4/study_before_workshop.html#additionally",
    "title": "Independent Study to prepare for workshop",
    "section": "Additionallyâ€¦",
    "text": "Additionallyâ€¦\nFiles should be organised into folders. Code should well commented and easy to read. You should have curated your code to remove unnecessary commands that were useful to troubleshoot or understand objects in your environment but which are not needed for the final analysis.\nIf you are missing files, go through:\n\nTranscriptomics 1: ğŸ‘‹ Hello data! Workshop including:\nğŸ¤— Look after future you! and\nthe Independent Study to consolidate",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#differential-expression-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#differential-expression-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Differential expression",
    "text": "Differential expression\n\nThe goal of differential expression is to test whether there is a significant difference in gene expression between groups.\nA large number of computational methods have been developed for differential expression analysis\nR is the leading language for differential expression analysis",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#differential-expression-2",
    "href": "transcriptomics/week-4/study_before_workshop.html#differential-expression-2",
    "title": "Independent Study to prepare for workshop",
    "section": "Differential expression",
    "text": "Differential expression\n\nthe statistical concepts are very similar to those you have already encountered in stages 1 and 2\nyou are essentially doing two-sample tests (independent samples)\nbut you are doing a lot of them! One for every gene\ndata need normalisation before comparison",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#statistical-concepts",
    "href": "transcriptomics/week-4/study_before_workshop.html#statistical-concepts",
    "title": "Independent Study to prepare for workshop",
    "section": "Statistical concepts",
    "text": "Statistical concepts\nLike familiar tests:\n\nthe type of test (the function) you use depends on the type of data you have and the type of assumptions you want to make\nthe tests work by comparing the variation between groups to the variation within groups.\nyou will get: the difference between groups, a test statistic, and a p-value\nyou also get an adjusted p-value which is the â€˜correctionâ€™ for multiple testing",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#the-difference-between-groups",
    "href": "transcriptomics/week-4/study_before_workshop.html#the-difference-between-groups",
    "title": "Independent Study to prepare for workshop",
    "section": "The difference between groups",
    "text": "The difference between groups\n\nThe difference between groups is given as the log2 fold change in expression between groups\nA fold change is the expression in one group divided by the expression in the other group: \\(\\frac{A}{B}\\)\nwe use fold changes because the absolute expression values may not be accurate and relative changes are what matters\nwe use log2 fold changes because they are symmetrical around 0",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#why-log2-fold-change",
    "href": "transcriptomics/week-4/study_before_workshop.html#why-log2-fold-change",
    "title": "Independent Study to prepare for workshop",
    "section": "Why log2 fold change?",
    "text": "Why log2 fold change?\n\nlog2 means log to the base 2\nSuppose the expression in group A is 5 and the expression in group B is 8\n\\(\\frac{A}{B} = \\frac{5}{8}\\) = 0.625 and \\(\\frac{B}{A} = \\frac{8}{5}\\) = 1.6\nIf B &gt; A the range of \\(\\frac{A}{B}\\) is 0 - 1 but the range of \\(\\frac{B}{A}\\) is 1 - \\(\\infty\\)\nHowever, if we take the log2 of \\(\\frac{A}{B}\\) we get -0.678 and the log2 of \\(\\frac{B}{A}\\) is 0.678.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#adjusted-p-value",
    "href": "transcriptomics/week-4/study_before_workshop.html#adjusted-p-value",
    "title": "Independent Study to prepare for workshop",
    "section": "Adjusted p-value",
    "text": "Adjusted p-value\n\nThe p-value has to be adjusted because of the number of tested being done\nPreviously we have used Tukeyâ€™s HSD to adjust for multiple testing following a significant ANOVA (using the emmeans package) and Dunnâ€™s test of multiple comparisons following a significant Kruskal-Wallis test\nHere the Benjamini-Hochberg procedure (Benjamini and Hochberg 1995) is used to adjust for multiple testing\nBH controls the False Discovery Rate (FDR)\nThe FDR is the proportion of false positives among the genes called significant",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#normalisation",
    "href": "transcriptomics/week-4/study_before_workshop.html#normalisation",
    "title": "Independent Study to prepare for workshop",
    "section": "Normalisation",
    "text": "Normalisation\n\nNormalisation adjusts raw counts to account for factors that prevent direct comparisons\nNormalisation usually influences the experimental design as well as the analysis",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#normalisation-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#normalisation-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Normalisation",
    "text": "Normalisation\n\nğŸ­ mice data are normalised\nğŸ„ Arabidopisis and ğŸ’‰ Leishmania data are raw counts (not normalised) because the differential expression method will do this.\nNormalisation is a big topic. See DÃ¼ren, Lederer, and Qin (2022); Bullard et al. (2010); Lytal, Ran, and An (2020); Abrams et al. (2019); Vallejos et al. (2017); Evans, Hardin, and Stoebel (2017)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#type-of-de-tests",
    "href": "transcriptomics/week-4/study_before_workshop.html#type-of-de-tests",
    "title": "Independent Study to prepare for workshop",
    "section": "Type of DE tests",
    "text": "Type of DE tests\n\nA large number of computational methods have been developed for differential expression analysis\nMethods vary in the types of normalisation they do, the statistical model they use, and the assumptions they make\nSome of the most well-known methods are provided by: DESeq2 (Love, Huber, and Anders 2014), edgeR (Robinson, McCarthy, and Smyth 2010; McCarthy, Chen, and Smyth 2012; Chen, Lun, and Smyth 2016), limma (Ritchie et al. 2015) and scran (Lun, McCarthy, and Marioni 2016)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#type-of-test-the-function",
    "href": "transcriptomics/week-4/study_before_workshop.html#type-of-test-the-function",
    "title": "Independent Study to prepare for workshop",
    "section": "Type of test (the function)",
    "text": "Type of test (the function)\n\n\nDESeq2 and edgeR\n\nboth require raw counts as input\nboth assume that most genes are not DE\nboth use a negative binomial distribution1 to model the data\nuse slightly different normalisation methods: DESeq2 uses the median of ratios method; edgeR uses the trimmed mean of M values (TMM) method\n\n\nA discrete distribution for counts, similar to the Poisson distribution",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#type-of-test-the-function-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#type-of-test-the-function-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Type of test (the function)",
    "text": "Type of test (the function)\n\n\nscran\n\nworks on normalized log-expression values\nperforms Welch t-tests",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#meta-data",
    "href": "transcriptomics/week-4/study_before_workshop.html#meta-data",
    "title": "Independent Study to prepare for workshop",
    "section": "Meta data",
    "text": "Meta data\n\nDE methods require two types of data: the expression data and the meta data\nThe meta data gives the information about the samples\nIt says which samples (which columns of data) are in which treatment group (s)\nMeta data is usually stored in a separate file",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#arabidopisis",
    "href": "transcriptomics/week-4/study_before_workshop.html#arabidopisis",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ Arabidopisis\n",
    "text": "ğŸ„ Arabidopisis\n\n\nExpression for the whole transcriptome ENSEMBL Arabidopsis TAIR10(Yates et al. 2022)\nValues are raw counts\nThe statistical analysis method we will use is DESeq2 which requires raw counts and performs the normalisation as part of the analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#leishmania-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#leishmania-1",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\n\nExpression for the whole transcriptome L. mexicana MHOM/GT/2001/U1103(Rogers et al. 2011)\nValues are raw counts\nThe statistical analysis method we will use is DESeq2 which requires raw counts and performs the normalisation as part of the analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#stem-cells-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#stem-cells-1",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\n\nExpression for a subset of the transcriptome, the secretome\n\n\nValues are log2 normalised values\nThe statistical analysis method we will use scran (Lun, McCarthy, and Marioni 2016) requires normalised values",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#adding-gene-information-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#adding-gene-information-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Adding gene information",
    "text": "Adding gene information\n\nThe gene id is difficult to interpret\nTherefore we need to add information such as the gene name and a description to the results\n\n\n\nğŸ„ Arabidopisis information comes from TAIR10 (Yates et al. 2022)\nğŸ’‰ Leishmania information comes TriTrypDB (Rogers et al. 2011)\nğŸ­ Stem cell information comes from Ensembl (Birney et al. 2004)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#tair10-through-ensembl",
    "href": "transcriptomics/week-4/study_before_workshop.html#tair10-through-ensembl",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ„ TAIR10 through Ensembl",
    "text": "ğŸ„ TAIR10 through Ensembl\n\nEnsembl creates, integrates and distributes reference datasets and analysis tools that enable genomics\nBioMart (Smedley et al. 2009) provides uniform access to these large datasets\nbiomaRt (Durinck et al. 2009, 2005) is a Bioconductor package gives you programmatic access to BioMart.\nIn the workshop you use this package to get information you can merge with the results file",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#tritrypdb",
    "href": "transcriptomics/week-4/study_before_workshop.html#tritrypdb",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ’‰ TriTrypDB",
    "text": "ğŸ’‰ TriTrypDB\n\n\nI got the information from TriTrypDB\nwhich is a functional genomic resource for the Trypanosomatidae and Plasmodidae\nhttps://tritrypdb.org/tritrypdb/app/downloads section\nI downloaded the L. mexicana MHOM/GT/2001/U1103 Full GFF and extracted the gene information and saved it as leishmania_mex.xlsx\nIn the workshop you will import this file and merge the information with the results file",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#ensembl",
    "href": "transcriptomics/week-4/study_before_workshop.html#ensembl",
    "title": "Independent Study to prepare for workshop",
    "section": "ğŸ­ Ensembl",
    "text": "ğŸ­ Ensembl\n\nEnsembl creates, integrates and distributes reference datasets and analysis tools that enable genomics\nBioMart (Smedley et al. 2009) provides uniform access to these large datasets\nbiomaRt (Durinck et al. 2009, 2005) is a Bioconductor package gives you programmatic access to BioMart.\nIn the workshop you use this package to get information you can merge with the results file",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#packages",
    "href": "transcriptomics/week-4/study_before_workshop.html#packages",
    "title": "Independent Study to prepare for workshop",
    "section": "Packages",
    "text": "Packages\nThese packages are all on the University computers which you can access on campus or remotely using the VDS\nIf you want to use your own machine you will need to install the packages.\n\nInstall BiocManager from CRAN in the the normal way:\n\ninstall.packages(\"BiocManager\")\n\nInstall DESeq2 from Bioconductor using BiocManager:\n\nBiocManager::install(\"DESeq2\")\n\nInstall scran from Bioconductor using BiocManager:\n\nBiocManager::install(\"scran\")\n\nInstall biomaRt from Bioconductor using BiocManager:\n\nBiocManager::install(\"biomaRt\")",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#workshops-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#workshops-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Workshops",
    "text": "Workshops\n\nTranscriptomics 1: Hello data. Getting to know the data. Checking the distributions of values overall, across rows and columns to check things are as we expect and detect rows/columns that need to be removed\nTranscriptomics 2: Statistical Analysis. Identifying which genes are differentially expressed between treatments. This is the main analysis step. We will use different methods for bulk and single cell data.\nTranscriptomics 3: Visualising. Principal Component Analysis (PCA) volcano plots to visualise the results of the",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#references",
    "href": "transcriptomics/week-4/study_before_workshop.html#references",
    "title": "Independent Study to prepare for workshop",
    "section": "References",
    "text": "References\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr (Xie 2024, 2015, 2014), kableExtra (Zhu 2021)\n\n\n\n\nAbrams, Zachary B., Travis S. Johnson, Kun Huang, Philip R. O. Payne, and Kevin Coombes. 2019. â€œA Protocol to Evaluate RNA Sequencing Normalization Methods.â€ BMC Bioinformatics 20 (24): 679. https://doi.org/10.1186/s12859-019-3247-x.\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2024. â€œQuarto.â€ https://doi.org/10.5281/zenodo.5960048.\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. â€œControlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.â€ J. R. Stat. Soc. Series B Stat. Methodol. 57 (1): 289â€“300. http://www.jstor.org/stable/2346101.\n\n\nBirney, Ewan, T. Daniel Andrews, Paul Bevan, Mario Caccamo, Yuan Chen, Laura Clarke, Guy Coates, et al. 2004. â€œAn Overview of Ensembl.â€ Genome Research 14 (5): 925â€“28. https://doi.org/10.1101/gr.1860604.\n\n\nBullard, James H., Elizabeth Purdom, Kasper D. Hansen, and Sandrine Dudoit. 2010. â€œEvaluation of Statistical Methods for Normalization and Differential Expression in mRNA-Seq Experiments.â€ BMC Bioinformatics 11 (1): 94. https://doi.org/10.1186/1471-2105-11-94.\n\n\nChen, Yunshun, Aaron T. L. Lun, and Gordon K. Smyth. 2016. â€œFrom Reads to Genes to Pathways: Differential Expression Analysis of RNA-Seq Experiments Using Rsubread and the edgeR Quasi-Likelihood Pipeline.â€ https://doi.org/10.12688/f1000research.8987.2.\n\n\nDÃ¼ren, Yannick, Johannes Lederer, and Li-Xuan Qin. 2022. â€œDepth Normalization of Small RNA Sequencing: Using Data and Biology to Select a Suitable Method.â€ Nucleic Acids Research 50 (10): e56. https://doi.org/10.1093/nar/gkac064.\n\n\nDurinck, Steffen, Yves Moreau, Arek Kasprzyk, Sean Davis, Bart De Moor, Alvis Brazma, and Wolfgang Huber. 2005. â€œBioMart and Bioconductor: A Powerful Link Between Biological Databases and Microarray Data Analysis.â€ Bioinformatics 21: 3439â€“40.\n\n\nDurinck, Steffen, Paul T. Spellman, Ewan Birney, and Wolfgang Huber. 2009. â€œMapping Identifiers for the Integration of Genomic Datasets with the r/Bioconductor Package biomaRt.â€ Nature Protocols 4: 1184â€“91.\n\n\nEvans, Ciaran, Johanna Hardin, and Daniel M Stoebel. 2017. â€œSelecting Between-Sample RNA-Seq Normalization Methods from the Perspective of Their Assumptions.â€ Briefings in Bioinformatics 19 (5): 776â€“92. https://doi.org/10.1093/bib/bbx008.\n\n\nLove, Michael I., Wolfgang Huber, and Simon Anders. 2014. â€œModerated Estimation of Fold Change and Dispersion for RNA-Seq Data with DESeq2.â€ Genome Biology 15: 550. https://doi.org/10.1186/s13059-014-0550-8.\n\n\nLun, Aaron T. L., Davis J. McCarthy, and John C. Marioni. 2016. â€œA Step-by-Step Workflow for Low-Level Analysis of Single-Cell RNA-Seq Data with Bioconductor.â€ F1000Res. 5: 2122. https://doi.org/10.12688/f1000research.9501.2.\n\n\nLytal, Nicholas, Di Ran, and Lingling An. 2020. â€œNormalization Methods on Single-Cell RNA-Seq Data: An Empirical Survey.â€ Frontiers in Genetics 11. https://www.frontiersin.org/articles/10.3389/fgene.2020.00041.\n\n\nMcCarthy, Davis J., Yunshun Chen, and Gordon K. Smyth. 2012. â€œDifferential Expression Analysis of Multifactor RNA-Seq Experiments with Respect to Biological Variation.â€ Nucleic Acids Research 40 (10): 4288â€“97. https://doi.org/10.1093/nar/gks042.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRitchie, Matthew E., Belinda Phipson, Di Wu, Yifang Hu, Charity W. Law, Wei Shi, and Gordon K. Smyth. 2015. â€œLimma Powers Differential Expression Analyses for RNA-Sequencing and Microarray Studies.â€ Nucleic Acids Research 43 (7): e47. https://doi.org/10.1093/nar/gkv007.\n\n\nRobinson, Mark D., Davis J. McCarthy, and Gordon K. Smyth. 2010. â€œedgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.â€ Bioinformatics 26 (1): 139â€“40. https://doi.org/10.1093/bioinformatics/btp616.\n\n\nRogers, Matthew B., James D. Hilley, Nicholas J. Dickens, Jon Wilkes, Paul A. Bates, Daniel P. Depledge, David Harris, et al. 2011. â€œChromosome and Gene Copy Number Variation Allow Major Structural Change Between Species and Strains of Leishmania.â€ Genome Research 21 (12): 2129â€“42. https://doi.org/10.1101/gr.122945.111.\n\n\nSmedley, Damian, Syed Haider, Benoit Ballester, Richard Holland, Darin London, Gudmundur Thorisson, and Arek Kasprzyk. 2009. â€œBioMart  Biological Queries Made Easy.â€ BMC Genomics 10 (1): 22. https://doi.org/10.1186/1471-2164-10-22.\n\n\nVallejos, Catalina A., Davide Risso, Antonio Scialdone, Sandrine Dudoit, and John C. Marioni. 2017. â€œNormalizing Single-Cell RNA Sequencing Data: Challenges and Opportunities.â€ Nature Methods 14 (6): 565â€“71. https://doi.org/10.1038/nmeth.4292.\n\n\nXie, Yihui. 2014. â€œKnitr: A Comprehensive Tool for Reproducible Research in R.â€ In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\nâ€”â€”â€”. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\nâ€”â€”â€”. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nYates, Andrew D, James Allen, Ridwan M Amode, Andrey G Azov, Matthieu Barba, AndrÃ©s Becerra, Jyothish Bhai, et al. 2022. â€œEnsembl Genomes 2022: An Expanding Genome Resource for Non-Vertebrates.â€ Nucleic Acids Research 50 (D1): D996â€“1003. https://doi.org/10.1093/nar/gkab1007.\n\n\nZhu, Hao. 2021. â€œkableExtra: Construct Complex Table with â€™Kableâ€™ and Pipe Syntax.â€ https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/overview.html",
    "href": "transcriptomics/week-4/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week we cover differential expression analysis on raw counts or log normalised values. The independent study will allow you to check you have what you should have following the Transcriptomics 1: Hello Data workshop and Consolidation study. It will also summarise the concepts and methods we will use in the workshop. In the workshop, you will learn how to perform differential expression analysis on raw counts using DESeq2 (Love, Huber, and Anders 2014) or on logged normalised expression values using scran (Lun, McCarthy, and Marioni 2016) or both. You will also add information about genes programmatically.\nWe suggest you sit together with your group in the workshop.\n\nLearning objectives\nThe successful student will be able to:\n\nverify they have the required RStudio Project set up and the data and code files from the previous Workshop and Consolidation study\nexplain the goal of differential expression analysis and the importance of normalisation\nexplain why and how the nature of the input values determines the analysis package used\ndescribe the metadata needed to carry out differential expression analysis and the statistical models used by DESeq2 and scran\nfind genes that are unexpressed or expressed in a just one group\nperform differential expression analysis on raw counts using DESeq2 or on logged normalised expression values using scran or both.\nexplain the output of differential expression: log fold change, p-value, adjusted p-value\nadd information about genes programmatically to their results\nprepare for a discussion with their project supervisor about genes of interest\n\n\n\nInstructions\n\nPrepare\n\nğŸ“– Check what you should have after week 3\nğŸ“– Read about concepts in differential expression analysis.\nğŸ“– Find out what packages we will use.\n\nWorkshop\n\nğŸ’» Find unexpressed genes and those expressed in a single cell type or treatment group.\nğŸ’» Set up the metadata for differential expression analysis.\nğŸ’» Perform differential expression analysis on raw counts using DESeq2 or on logged normalised expression values using scran.\nLook after future you!\n\nConsolidate\n\nğŸ’» Use the work you completed in the workshop as a template to apply to a new case.\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLove, Michael I., Wolfgang Huber, and Simon Anders. 2014. â€œModerated Estimation of Fold Change and Dispersion for RNA-Seq Data with DESeq2.â€ Genome Biology 15: 550. https://doi.org/10.1186/s13059-014-0550-8.\n\n\nLun, Aaron T. L., Davis J. McCarthy, and John C. Marioni. 2016. â€œA Step-by-Step Workflow for Low-Level Analysis of Single-Cell RNA-Seq Data with Bioconductor.â€ F1000Res. 5: 2122. https://doi.org/10.12688/f1000research.9501.2.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "About"
    ]
  },
  {
    "objectID": "transcriptomics/transcriptomics.html",
    "href": "transcriptomics/transcriptomics.html",
    "title": "Transcriptomics Data Analysis for Group Project",
    "section": "",
    "text": "This week you will meet your data. There are three datasets, one for each project in this strand. The independent study will concisely cover how each of these three data sets were generated and how they have been processed before being given to you. It will also give an overview of the analysis we will carry out over three workshops. In the workshop, you will learn what steps to take to get a good understanding of transciptomic data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It will give you the understanding of the data and R data structures that you will need to code and trouble-shoot code. It will also allow you to spot failed or problematic samples and will inform your decisions on quality control. At the end of this workshop and the following independent study you will have performed quality control by filtering out uninformative genes and samples, and saved this filtered data for use in the next workshop. You will also have a script that you can use to repeat this process on other datasets.\n\n\n\nThis week we cover differential expression analysis on your quality controlled data. The independent study will allow you to check you have what you should have following the Transcriptomics 1: Hello Data workshop and Consolidation study. It then summarises the concepts and methods used to carry out differential expression analysis in workshop. In the workshop, you will perform the differential expression and learn how to computationally annotate your genes with more information from the databases. This will include the Gene Ontology (GO) terms that describe the biological processes, molecular functions and cellular components that the gene is involved in. At the end of this workshop and the following independent study you will have files containing the genes which are differentially expressed, along with the statistical information, summary information and annotation. You will be able to consider which genes you want to investigates with your Project director and have what you need for the next workshop. You will also have a script that you can use to repeat this process on other datasets.\n\n\n\nThis week you will learn some how to do some common data visualisations for transcriptomic data. You will conduct and present a Principal Component Analysis (PCA) and a Volcano plot. The independent study will allow you to check you have what you should have following the Transcriptomics 2: Statistical Analysis workshop and Consolidation study. At the end of this workshop and the following independent study you will have at least two figures suitable for including in your report, along with an understanding of the results you can report on. You will also have a script that you can use to repeat this process on other datasets.\nReferences",
    "crumbs": [
      "Transcriptomics",
      "Transcriptomics Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "transcriptomics/transcriptomics.html#transcriptomics-1-hello-data",
    "href": "transcriptomics/transcriptomics.html#transcriptomics-1-hello-data",
    "title": "Transcriptomics Data Analysis for Group Project",
    "section": "",
    "text": "This week you will meet your data. There are three datasets, one for each project in this strand. The independent study will concisely cover how each of these three data sets were generated and how they have been processed before being given to you. It will also give an overview of the analysis we will carry out over three workshops. In the workshop, you will learn what steps to take to get a good understanding of transciptomic data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It will give you the understanding of the data and R data structures that you will need to code and trouble-shoot code. It will also allow you to spot failed or problematic samples and will inform your decisions on quality control. At the end of this workshop and the following independent study you will have performed quality control by filtering out uninformative genes and samples, and saved this filtered data for use in the next workshop. You will also have a script that you can use to repeat this process on other datasets.",
    "crumbs": [
      "Transcriptomics",
      "Transcriptomics Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "transcriptomics/transcriptomics.html#transcriptomics-2-statistical-analysis",
    "href": "transcriptomics/transcriptomics.html#transcriptomics-2-statistical-analysis",
    "title": "Transcriptomics Data Analysis for Group Project",
    "section": "",
    "text": "This week we cover differential expression analysis on your quality controlled data. The independent study will allow you to check you have what you should have following the Transcriptomics 1: Hello Data workshop and Consolidation study. It then summarises the concepts and methods used to carry out differential expression analysis in workshop. In the workshop, you will perform the differential expression and learn how to computationally annotate your genes with more information from the databases. This will include the Gene Ontology (GO) terms that describe the biological processes, molecular functions and cellular components that the gene is involved in. At the end of this workshop and the following independent study you will have files containing the genes which are differentially expressed, along with the statistical information, summary information and annotation. You will be able to consider which genes you want to investigates with your Project director and have what you need for the next workshop. You will also have a script that you can use to repeat this process on other datasets.",
    "crumbs": [
      "Transcriptomics",
      "Transcriptomics Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "transcriptomics/transcriptomics.html#transcriptomics-3-visualising-and-interpreting",
    "href": "transcriptomics/transcriptomics.html#transcriptomics-3-visualising-and-interpreting",
    "title": "Transcriptomics Data Analysis for Group Project",
    "section": "",
    "text": "This week you will learn some how to do some common data visualisations for transcriptomic data. You will conduct and present a Principal Component Analysis (PCA) and a Volcano plot. The independent study will allow you to check you have what you should have following the Transcriptomics 2: Statistical Analysis workshop and Consolidation study. At the end of this workshop and the following independent study you will have at least two figures suitable for including in your report, along with an understanding of the results you can report on. You will also have a script that you can use to repeat this process on other datasets.\nReferences",
    "crumbs": [
      "Transcriptomics",
      "Transcriptomics Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_after_workshop.html",
    "href": "transcriptomics/week-5/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "You need only do the section for one of the examples.\nYou need only do the section for your own project data\nğŸ„ Arabidopisis\nğŸ¬ Open your arab-88H RStudio Project and the cont-low-aerial.R script you began in the Consolidation study of Transcriptomics 1 and continued to work on in Transcriptomics 2. Use the code you used in the workshop (in cont-low-root.R) as a template to visualise the results for the comparison between the control and low nickel in roots.\nğŸ’‰ Leishmania\nğŸ¬ Open your leish-88H RStudio Project and the pro_ama.R script you began in the Consolidation study of Transcriptomics 1 and continued to work on in Transcriptomics 2. Use the code you used in the workshop (in pro_meta.R) as a template to visualise the results for the comparison between the procyclic promastigote and amastigote stages.\nğŸ­ Stem cells\nğŸ¬ Open your mice-88H RStudio Project and the hspc-lthsc.R script you began in the Consolidation study of Transcriptomics 1 and continued to work on in Transcriptomics 2. Use the code you used in the workshop (in hspc-prog.R) as a template to visualise the results for the comparison between HSPC and LT-HSPC cells",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html",
    "href": "transcriptomics/week-5/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In the workshop, you will learn how to conduct and plot a Principle Component Analysis (PCA) as well as how to create a nicely formatted Volcano plot. You will also save significant genes to file to make it easier to identify genes of interest. and perform Gene Ontology (GO) term enrichment analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#session-overview",
    "href": "transcriptomics/week-5/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In the workshop, you will learn how to conduct and plot a Principle Component Analysis (PCA) as well as how to create a nicely formatted Volcano plot. You will also save significant genes to file to make it easier to identify genes of interest. and perform Gene Ontology (GO) term enrichment analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#arabidopsis",
    "href": "transcriptomics/week-5/workshop.html#arabidopsis",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis\n",
    "text": "ğŸ„ Arabidopsis\n\nğŸ¬ Open the arabi-88H RStudio Project and the cont-low-root.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#leishmania",
    "href": "transcriptomics/week-5/workshop.html#leishmania",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nğŸ¬ Open the leish-88H RStudio Project and the pro-meta.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#stem-cells",
    "href": "transcriptomics/week-5/workshop.html#stem-cells",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nğŸ¬ Open the mice-88H RStudio Project and the hspc-prog.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#everyone",
    "href": "transcriptomics/week-5/workshop.html#everyone",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nğŸ¬ Make a new folder figures in the project directory.\nThis is where we will save our figure files\nğŸ¬ Load tidyverse (Wickham et al. 2019) and conflicted (Wickham 2023). You most likely have this code at the top of your script already.\n\nlibrary(tidyverse)\nlibrary(conflicted)\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.3     âœ” readr     2.1.4\nâœ” forcats   1.0.0     âœ” stringr   1.5.0\nâœ” ggplot2   3.4.3     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.0\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package to force all conflicts to become errors\nI recommend you set the dplyr versions of filter() and select() to use by default\nğŸ¬ Use the dplyr version of filter() by default:\n\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(dplyr::select)",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#everyone-1",
    "href": "transcriptomics/week-5/workshop.html#everyone-1",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nğŸ¬ Import your results data. This should be a file in the results folder called xxxx_results.csv where xxxx indicates the comparison you made.\nğŸ¬ Remind yourself what is in the rows and columns and the structure of the dataframes (perhaps using glimpse())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we do PCA we will want to label the samples with their treatment for figures. For ğŸ¸ Frog development, ğŸ„ Arabidopsis and ğŸ’‰ Leishmania, this labelling information is most easily added using the metadata. You will need to filter for only the samples in the comparison that was made in the results file.\nYou may need to refer back to the Week 4 Statistical Analysis workshop (in section 2. Create DESeqDataSet object) to remind yourself how to import and filter the metadata you need.\nğŸ¬ Import the metadata that maps the sample names to treatments. Remember to select only the samples for comparison that was made.\nFor ğŸ­ Stem cells, cell types are encoded in the column names. We will do some regular expression magic to extract the cell type from the column names.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#arabidopsis-and-leishmania",
    "href": "transcriptomics/week-5/workshop.html#arabidopsis-and-leishmania",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis and ğŸ’‰ Leishmania\n",
    "text": "ğŸ„ Arabidopsis and ğŸ’‰ Leishmania\n\nğŸ¬ Design the code to log2 transform the normalised counts using the template given:\nI recommend viewing the dataframe to see the new columns. Check you have the expected number of columns.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#stem-cells-1",
    "href": "transcriptomics/week-5/workshop.html#stem-cells-1",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nYou do not need to apply this transformation because the data is already log2 transformed.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#everyone-2",
    "href": "transcriptomics/week-5/workshop.html#everyone-2",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nWe now all have dataframes with all the information we need: normalised counts, log2 normalised counts, statistical comparisons with fold changes and p-values, and information about the gene.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#everyone-3",
    "href": "transcriptomics/week-5/workshop.html#everyone-3",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nWe will create dataframe of the significant genes and write them to file. This is subset from the results file but will make it a little easier to examine and select genes of interest.\nThe general form of the code you need is:\n\n# DO NOT DO\n# create a dataframe of genes significant at 0.05 level\nxxxx_results_sig0.05 &lt;- xxxx_results |&gt; \n  filter(pvalue &lt;= 0.05)\n\nNote that you determine the significance level using the adjusted p-values rather than the uncorrected p-values. This column is name padj in DESeq2 output and FDR in scran output.\nğŸ¬ Create a dataframe of the genes significant at the 0.05 level using filter(). You will need to know the name of column with the adjusted p-values.\nâ“How many genes are significant at the 0.05 levels?\n\n\n\n\n\n\n\n\n\nIf you have a very large number of genes significant at the 0.05 level, you may want to consider a more stringent cut-off such as 0.01.\nğŸ¬ Write the dataframe to a csv file. I recommend using the same file name as you used for the dataframe.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#arabidopsis-pca",
    "href": "transcriptomics/week-5/workshop.html#arabidopsis-pca",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis\n",
    "text": "ğŸ„ Arabidopsis\n\nğŸ¬ Transpose the log2 transformed normalised counts:\n\nroot_log2_trans &lt;- root_results |&gt; \n  select(starts_with(\"log2_\")) |&gt;\n  t() |&gt; \n  data.frame()\n\nWe have used the select() function to select all the columns that start with log2_. We then use the t() function to transpose the dataframe. We then convert the resulting matrix to a dataframe using data.frame(). If you view that dataframe youâ€™ll see it has default column name which we can fix using colnames() to set the column names to the gene ids.\nğŸ¬ Set the column names to the gene ids:\n\ncolnames(root_log2_trans) &lt;- root_results$gene_id\n\nğŸ¬ Perform PCA on the log2 transformed normalised counts:\n\npca &lt;- root_log2_trans |&gt;\n  prcomp(rank. = 4, scale = TRUE)\n\nThe scale argument tells prcomp() to scale the data before doing the PCA. This is important when the variables are on different scales to stop variables with large values dominating the PCA. The rank. argument tells prcomp() to only calculate the first 4 principal components. This is useful for visualisation as we can only plot in 2 or 3 dimensions. We can see the results of the PCA by viewing the summary() of the pca object.\n\nsummary(pca)\n\nImportance of first k=4 (out of 12) components:\n                            PC1     PC2      PC3      PC4\nStandard deviation     105.3770 48.2488 41.43172 36.25251\nProportion of Variance   0.4967  0.1041  0.07678  0.05878\nCumulative Proportion    0.4967  0.6008  0.67756  0.73634\n\n\nThe Proportion of Variance tells us how much of the variance is explained by each component. We can see that the first component explains 0.5081 of the variance, the second 0.1053, and the third 0.07615. Together the first three components explain nearly 70% of the total variance in the data. Plotting PC1 against PC2 will capture about 61% of the variance which is very likely very much better than we would get plotting any two genes against each other. To plot the PC1 against PC2 we will need to extract the PC1 and PC2 â€œscoresâ€ from the PCA object and add labels for the samples. Those labels will come from the row names of the transformed data which has the sample ids and from the metadata.\nğŸ¬ Create a vector of the sample ids from the row names. These include the log2 prefix which we can removed for labelling:\n\nsample_id &lt;- row.names(root_log2_trans) |&gt; str_remove(\"log2_\")\n\nYou might want to check the result.\nNow we will extract the PC1 and PC2 scores from the PCA object and add. Our PCA object is called pca and the scores are in pca$x. We will create a dataframe of the scores and add the sample ids.\nğŸ¬ Create a dataframe of PC1 and PC2 scores and add the sample ids:\n\npca_labelled &lt;- data.frame(pca$x,\n                           sample_id)\n\nğŸ¬ Merge with the metadata so we can label points by treatment and sibling pair:\n\npca_labelled &lt;- pca_labelled |&gt; \n  left_join(meta_wild, \n            by = \"sample_id\")\n\nSince the metadata contained the sample ids, it was especially important to remove the log2_ from the row names so that the join would work.\nThe dataframe should look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nsample_id\ntissue\nnickel\n\n\n\n-109.53392\n39.48389\n-1.6499549\n-43.461316\nCTR1\nroot\ncontrol\n\n\n-108.22301\n-41.09251\n-32.3915483\n-15.124413\nCTR2\nroot\ncontrol\n\n\n-103.81095\n-27.57453\n20.0770797\n57.879701\nCTR3\nroot\ncontrol\n\n\n-95.17176\n89.37045\n-0.2395139\n-38.571701\nCTR4\nroot\ncontrol\n\n\n-87.01308\n-42.16006\n40.1872508\n23.702486\nCTR5\nroot\ncontrol\n\n\n-90.52651\n-31.24911\n0.0354079\n-3.883498\nCTR6\nroot\ncontrol\n\n\n132.01912\n-68.32947\n-7.0032451\n-61.108905\nLWR1\nroot\nlow_ni\n\n\n118.62999\n38.76724\n102.2645361\n6.924138\nLWR2\nroot\nlow_ni\n\n\n94.16399\n62.58876\n-51.6047556\n30.623721\nLWR3\nroot\nlow_ni\n\n\n56.13000\n14.00142\n-45.5791649\n45.260438\nLWR4\nroot\nlow_ni\n\n\n78.94968\n-17.85199\n-23.7625009\n7.633547\nLWR5\nroot\nlow_ni\n\n\n114.38645\n-15.95410\n-0.3335909\n-9.874197\nLWR6\nroot\nlow_ni\n\n\n\n\n\nThe next task is to plot PC2 against PC1 and colour by nickel conditions. This is just a scatterplot so we can use geom_point(). We will use colour to indicate the copper conditions.\nğŸ¬ Plot PC2 against PC1 and colour by nickel conditions:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = nickel)) +\n  geom_point(size = 3) +\n  theme_classic()\n\n\n\n\n\n\n\nWe see particularly good separation between treatments, on PC1 â€“ samples with the same treatment cluster together. You can also try plotting PC3 or PC4.\nI prefer to customise the colours and shapes. I especially like the viridis colour scales which provide colour scales that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness. See Introduction to viridis for more information.\nggplot provides functions to access the viridis scales. Here I use scale_fill_viridis_d(). The d stands for discrete. The function scale_fill_viridis_c() would be used for continuous data. Iâ€™ve used the default â€œviridisâ€ (or â€œDâ€) option (do ?scale_fill_viridis_d for all the options) and used the begin and end arguments to control the range of colour - I have set the range to be from 0.15 to 0.95 the avoid the strongest contrast. I have also set the name argument to provide a label for the legend.\nğŸ¬ Customise the PC2 against PC1 plot:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = nickel)) +\n  geom_point(size = 3) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = \"Nickel\") +\n  theme_classic()\n\n\n\n\n\n\n\nNow go to Volcano plots for ğŸ„ Arabidopsis",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#leishmania-pca",
    "href": "transcriptomics/week-5/workshop.html#leishmania-pca",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nğŸ¬ Transpose the log2 transformed normalised counts:\n\npro_meta_log2_trans &lt;- pro_meta_results |&gt; \n  select(starts_with(\"log2_\")) |&gt;\n  t() |&gt; \n  data.frame()\n\nWe have used the select() function to select all the columns that start with log2_. We then use the t() function to transpose the dataframe. We then convert the resulting matrix to a dataframe using data.frame(). If you view that dataframe youâ€™ll see it has default column name which we can fix using colnames() to set the column names to the gene ids.\nğŸ¬ Set the column names to the gene ids:\n\ncolnames(pro_meta_log2_trans) &lt;- pro_meta_results$gene_id\n\nğŸ¬ Perform PCA on the log2 transformed normalised counts:\n\npca &lt;- pro_meta_log2_trans |&gt;\n  prcomp(rank. = 4, scale = TRUE) \n\nThe scale argument tells prcomp() to scale the data before doing the PCA. This is important when the variables are on different scales to stop variables with large values dominating the PCA. The rank. argument tells prcomp() to only calculate the first 4 principal components. This is useful for visualisation as we can only plot in 2 or 3 dimensions. We can see the results of the PCA by viewing the summary() of the pca object.\n\nsummary(pca)\n\nImportance of first k=4 (out of 6) components:\n                           PC1     PC2     PC3      PC4\nStandard deviation     66.0744 41.9741 29.4077 27.61377\nProportion of Variance  0.5175  0.2088  0.1025  0.09038\nCumulative Proportion   0.5175  0.7263  0.8288  0.91916\n\n\nThe Proportion of Variance tells us how much of the variance is explained by each component. We can see that the first component explains 0.5175 of the variance, the second 0.2088, and the third 0.1025. Together the first three components explain nearly 92% of the total variance in the data. Plotting PC1 against PC2 will capture about 66% of the variance which is likely very much better than we would get plotting any two genes against each other. To plot the PC1 against PC2 we will need to extract the PC1 and PC2 â€œscoresâ€ from the PCA object and add labels for the samples. Those labels will come from the row names of the transformed data which has the sample ids and from the metadata.\nğŸ¬ Create a vector of the sample ids from the row names. These include the log2 prefix which we can removed for labelling:\n\nsample_id &lt;- row.names(pro_meta_log2_trans) |&gt; str_remove(\"log2_\")\n\nYou might want to check the result.\nNow we will extract the PC1 and PC2 scores from the PCA object and add. Our PCA object is called pca and the scores are in pca$x. We will create a dataframe of the scores and add the sample ids.\nğŸ¬ Create a dataframe of PC1 and PC2 scores and add the sample ids:\n\npca_labelled &lt;- data.frame(pca$x,\n                           sample_id)\n\nğŸ¬ Merge with the metadata so we can label points by life cycle stage:\n\npca_labelled &lt;- pca_labelled |&gt; \n  left_join(meta_pro_meta, \n            by = \"sample_id\")\n\nSince the metadata contained the sample ids, it was especially important to remove the log2_ from the row names so that the join would work.\nThe dataframe should look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nsample_id\nstage\nreplicate\n\n\n\n-59.42107\n-2.922229\n-11.043368\n-13.4641060\nlm_pro_1\nprocyclic\n1\n\n\n-63.25818\n-15.985856\n-36.248191\n0.7025242\nlm_pro_2\nprocyclic\n2\n\n\n-57.92065\n25.679645\n47.373874\n13.7893070\nlm_pro_3\nprocyclic\n3\n\n\n56.20326\n-59.707787\n5.958727\n31.5528028\nlm_meta_1\nmetacyclic\n1\n\n\n57.41205\n-11.738157\n14.157242\n-47.2285609\nlm_meta_2\nmetacyclic\n2\n\n\n66.98459\n64.674383\n-20.198284\n14.6480329\nlm_meta_3\nmetacyclic\n3\n\n\n\n\n\nThe next task is to plot PC2 against PC1 and colour by sibling pair. This is just a scatterplot so we can use geom_point(). We will use colour to indicate the life cycle stage.\nğŸ¬ Plot PC2 against PC1 and colour by copper conditions:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = stage)) +\n  geom_point(size = 3) +\n  theme_classic()\n\n\n\n\n\n\n\nThere is a good separation between treatments on PCA1. The replicates do seem to cluster together. You can also try plotting PC3 or PC4.\nI prefer to customise the colours. I especially like the\nviridis colour scales which provide colour scales that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness. See Introduction to viridis for more information.\nggplot provides functions to access the viridis scales. Here I use scale_fill_viridis_d(). The d stands for discrete. The function scale_fill_viridis_c() would be used for continuous data. Iâ€™ve used the default â€œviridisâ€ (or â€œDâ€) option (do ?scale_fill_viridis_d for all the options) and used the begin and end arguments to control the range of colour - I have set the range to be from 0.15 to 0.95 the avoid the strongest contrast. I have also set the name argument to provide a label for the legend.\nğŸ¬ Plot PC2 against PC1 and colour by life stage:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = stage)) +\n  geom_point(size = 3) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = \"Stage\") +\n  theme_classic()\n\n\n\n\n\n\n\nNow go to Volcano plots for ğŸ’‰ Leishmania",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#stem-cells-pca",
    "href": "transcriptomics/week-5/workshop.html#stem-cells-pca",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nğŸ¬ Transpose the log2 transformed normalised counts:\n\nhspc_prog_trans &lt;- hspc_prog_results |&gt; \n  dplyr::select(starts_with(c(\"HSPC_\", \"Prog_\"))) |&gt;\n  t() |&gt; \n  data.frame()\n\nWe have used the select() function to select all the columns that start with Prog_ or HSPC_. We then use the t() function to transpose the dataframe. We then convert the resulting matrix to a dataframe using data.frame(). If you view that dataframe youâ€™ll see it has default column name which we can fix using colnames() to set the column names to the gene ids.\nğŸ¬ Set the column names to the gene ids:\n\ncolnames(hspc_prog_trans) &lt;- hspc_prog_results$ensembl_gene_id\n\nğŸ¬ Perform PCA on the log2 transformed normalised counts:\n\npca &lt;- hspc_prog_trans |&gt;\n  prcomp(rank. = 8)\n\nThe rank. argument tells prcomp() to only calculate the first 8 principal components. This is useful for visualisation as we can only plot in 2 or 3 dimensions. We can see the results of the PCA by viewing the summary() of the pca object.\n\nsummary(pca)\n\nImportance of first k=8 (out of 423) components:\n                           PC1      PC2     PC3     PC4    PC5    PC6     PC7\nStandard deviation     15.2361 10.90424 8.02110 7.13683 5.4244 4.7159 4.49501\nProportion of Variance  0.1065  0.05456 0.02952 0.02337 0.0135 0.0102 0.00927\nCumulative Proportion   0.1065  0.16108 0.19060 0.21397 0.2275 0.2377 0.24695\n                           PC8\nStandard deviation     4.13094\nProportion of Variance 0.00783\nCumulative Proportion  0.25478\n\n\nThe Proportion of Variance tells us how much of the variance is explained by each component. We can see that the first component explains 0.10652 of the variance, the second 0.05456, and the third 0.02952. Together the first three components explain 19.06% of the total variance in the data. This is not that high but it is also quite a lot better than than we would get plotting any two genes randomly chosen against each other.\nTo plot the PC1 against PC2 we will need to extract the PC1 and PC2 â€œscoresâ€ from the PCA object and add labels for the cells. Our PCA object is called pca and the scores are in pca$x. The cells labels will come from the row names of the transformed data.\nğŸ¬ Create a dataframe of the PC1 and PC2 scores (in pca$x) and add the cell ids:\n\npca_labelled &lt;- data.frame(pca$x,\n                           cell_id = row.names(hspc_prog_trans))\n\nIt will be helpful to add a column for the cell type so we can label points. One way to do this is to extract the information in the cell_id column into two columns: one with the complete cell id and one with just the cell type. This extraction is done with Regular Expression magic.\nğŸ¬ Extract the cell type and cell number from the cell_id column (keeping the cell_id column):\n\npca_labelled &lt;- pca_labelled |&gt; \n  extract(cell_id, \n          remove = FALSE,\n          c(\"cell_type\", \"cell_number\"),\n          \"([a-zA-Z]{4})_([0-9]{3})\")\n\nWhat this code does is take what is in the cell_id column (something like Prog_001 or HSPC_001) and split it into two columns (â€œcell_typeâ€ and â€œcell_numberâ€). The reason why we want to do that is to colour the points by cell type. We would not want to use cell_id to colour the points because each cell id is unique and that would be 1000+ colours. The last argument in the extract() function is the pattern to match described with a regular expression. Three patterns are being matched, and two of those are in brackets meaning they are kept to fill the two new columns.\nThe first pattern is ([a-zA-Z]{4})\n\nit is brackets because we want to keep it and put it in cell_type\n\n\n[a-zA-Z] means any lower (a-z) or upper case letter (A-Z).\nThe square brackets means any of the characters in the square brackets will be matched\n\n{4} means 4 of them.\n\nSo the first pattern inside the first (â€¦) will match exactly 4 upper or lower case letters (like Prog or HSPC)\nThe second pattern is _ to match the underscore in every cell id that separates the cell type from the number. It is not in brackets because we do not want to keep it.\nThe third pattern is ([0-9]{3})\n\n\n[0-9] means any number\n\n{3} means 3 of them.\n\nSo the second pattern inside the second (â€¦) will match exactly 3 numbers (like 001 or 851).\n\n\n\n\n\n\nImportant for applying to other comparisons\n\n\n\nProg and HPSC have 4 letters. The column names with LT.HSC_ have 6 characters and includes a dot. You will need to adjust the regex when comparing LT-HSC to other cell types. The pattern to match the LT.HSC as well as the Prog and HSPC is ([a-zA-Z.]{4,6}). Note the dot inside the square brackets and numbers meaning either 4 or 6 of the characters in the []. The pattern to match the underscore and the cell number is the same.\n\n\nThe top of the dataframe should look like this (but with more decimal places)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\ncell_id\ncell_type\ncell_number\n\n\n\nHSPC_001\n-5.16\n14.51\n-1.73\n10.23\n-8.73\n4.23\n-0.09\n-3.38\nHSPC_001\nHSPC\n001\n\n\nHSPC_002\n-3.12\n13.27\n5.52\n7.00\n-3.75\n3.33\n-6.37\n-4.62\nHSPC_002\nHSPC\n002\n\n\nHSPC_003\n-11.98\n-9.99\n-1.63\n1.16\n-0.72\n3.13\n4.35\n-3.35\nHSPC_003\nHSPC\n003\n\n\nHSPC_004\n-6.75\n7.30\n1.45\n-9.82\n-8.30\n1.31\n3.10\n-0.60\nHSPC_004\nHSPC\n004\n\n\nHSPC_006\n-6.31\n2.83\n-4.38\n-13.28\n-0.22\n-2.25\n2.34\n6.65\nHSPC_006\nHSPC\n006\n\n\nHSPC_008\n-10.80\n10.17\n-5.13\n5.54\n-3.86\n-5.99\n0.62\n-4.92\nHSPC_008\nHSPC\n008\n\n\n\n\n\nThe next task is to plot PC2 against PC1 and colour by cell type. This is just a scatterplot so we can use geom_point(). We will use colour to indicate the cell type.\nğŸ¬ Plot PC2 against PC1 and colour by cell type:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = cell_type)) +\n  geom_point(alpha = 0.4) +\n  theme_classic()\n\n\n\n\n\n\n\nThere is a good clustering of cell types but plenty of overlap. You can also try plotting PC3 or PC4 (or others).\nI prefer to customise the colours. I especially like the viridis colour scales which provide colour scales that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness. See Introduction to viridis for more information.\nggplot provides functions to access the viridis scales. Here I use scale_fill_viridis_d(). The d stands for discrete used because cell type is a discrete variable. The function scale_fill_viridis_c() would be used for continuous data. Iâ€™ve used the default â€œviridisâ€ (or â€œDâ€) option (do ?scale_fill_viridis_d for all the options) and used the begin and end arguments to control the range of colour - I have set the range to be from 0.15 to 0.95 the avoid the strongest contrast. I have also set the name argument to NULL because that the legend refers to cell types is obvious.\nğŸ¬ Plot PC2 against PC1 and colour by cell type:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = cell_type)) +\n  geom_point(alpha = 0.4) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = NULL) +\n  theme_classic()\n\n\n\n\n\n\n\nNow go to Volcano plots for ğŸ­ Stem cells",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#arabidopsis-volcano",
    "href": "transcriptomics/week-5/workshop.html#arabidopsis-volcano",
    "title": "Workshop",
    "section": "ğŸ„ Arabidopsis\n",
    "text": "ğŸ„ Arabidopsis\n\nWe will add a column to the results dataframe that contains the -log10(padj). You could perform this transformation within the plot command without adding a column to the data if you prefer.\nğŸ¬ Add a column to the results dataframe that contains the -log10(padj):\n\nroot_results &lt;- root_results |&gt; \n  mutate(log10_padj = -log10(padj)) \n\nğŸ¬ Create a volcano plot of the results:\n\nroot_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj)) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.01), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nOur dashed lines are at -log10(0.01) and log2(2) and log2(-2) to make more clear which genes (points) are significantly different between the control and low nickel conditions samples and have a fold change of at least 2.\nIn most cases, people colour the points to show each quadrant. I like to add columns to the dataframe to indicate if the gene is significant and if the fold change is large and use those variables in the plot.\nğŸ¬ Add columns to the results dataframe to indicate if the gene is significant and if the fold change is large:\n\nroot_results &lt;- root_results |&gt; \n  mutate(sig = padj &lt;= 0.01,\n         bigfc = abs(log2FoldChange) &gt;= 2) \n\nThe use of abs() (absolute) means genes with a fold change of at least 2 in either direction will be considered to have a large fold change.\nNow we can colour the points by these new columns. I use interaction() to create four categories:\n\nnot significant and not large fold change (FF)\nsignificant and not large fold change (TF)\nnot significant and large fold (FT)\nsignificant and large fold change (TT)\n\nAnd I use scale_colour_manual() to set the colours for these categories.\nğŸ¬ Create a volcano plot of the results with the points coloured by significance and fold change:\n\nroot_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.01), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFor exploring the data, I like add labels to all the significant genes with a large fold change so I can very quickly identity them. The ggrepel package has a function geom_text_repel() that is useful for adding labels so that they donâ€™t overlap.\nğŸ¬ Load the package:\n\nlibrary(ggrepel)\n\nğŸ¬ Add labels to the significant genes with a large fold change:\n\nroot_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.01), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_text_repel(data = root_results |&gt; \n                    filter(bigfc == TRUE, sig == TRUE),\n                  aes(label = external_gene_name),\n                  size = 3,\n                  max.overlaps = 50) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nNotice that I have used filter() label only the genes that are both significant and have a large fold change. In systems you are familiar with, this labelling is very informative and can help you quickly identify common themes. Key to interpreting the volcano plot is to remember that positive fold changes means the gene is up-regulated in the control conditions and negative fold changes means the gene is down-regulated (i.e., higher in the low nickel). This was determined by the order of the treatments in the contrast used in the DESeq2 analysis\nIf you do forget which way round you did the comparison, you can always examine the results dataframe to see which of the treatments seem to be higher for the positive fold changes.\nWhen you have a gene of interest, you may wish to label it, and only it, on the plot. This is done in the same way except that you filter the data to only include the gene of interest. I have used and then use geom_label_repel() rather than geom_text_repel() to put the label in a box and nudged itâ€™s position to get a line connecting the point and the label. I have also increased the size of the point.\nğŸ¬ Add a label to one gene of interest (TBP1) and :\n\nroot_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.01), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_label_repel(data = root_results |&gt; \n                    filter(external_gene_name == \"TBP1\"),\n                  aes(label = external_gene_name),\n                  size = 4,\n                  nudge_x = .5,\n                  nudge_y = 1.5) +\n  geom_point(data = root_results |&gt; \n                    filter(external_gene_name == \"TBP1\"),\n                size = 3) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nShould you want to label more than one gene, you will need to use (for example): filter(external_gene_name %in% c(\"TBP1\", \"TRPN1\"))\nNow go to Save your plots",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#leishmania-volcano",
    "href": "transcriptomics/week-5/workshop.html#leishmania-volcano",
    "title": "Workshop",
    "section": "ğŸ’‰ Leishmania\n",
    "text": "ğŸ’‰ Leishmania\n\nWe will add a column to the results dataframe that contains the -log10(padj). You could perform this transformation within the plot command without adding a column to the data if you prefer.\nğŸ¬ Add a column to the results dataframe that contains the -log10(padj):\n\npro_meta_results &lt;- pro_meta_results |&gt; \n  mutate(log10_padj = -log10(padj)) \n\nğŸ¬ Create a volcano plot of the results:\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj)) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nOur dashed lines are at -log10(0.05) and log2(2) and log2(-2) to make more clear which genes (points) are significantly different between the life stages and have a fold change of at least 2. Whilst we have very many genes that are significant, we have fewer that are significant and have a large fold change.\nIn most cases, people colour the points to show that the quadrants. I like to add columns to the dataframe to indicate if the gene is significant and if the fold change is large and use those variables in the plot.\nğŸ¬ Add columns to the results dataframe to indicate if the gene is significant and if the fold change is large:\n\npro_meta_results &lt;- pro_meta_results |&gt; \n  mutate(sig = padj &lt;= 0.05,\n         bigfc = abs(log2FoldChange) &gt;= 2) \n\nThe use of abs() (absolute) means genes with a fold change of at least 2 in either direction will be considered to have a large fold change.\nNow we can colour the points by these new columns. I use interaction() to create four categories:\n\nnot significant and not large fold change (FF)\nsignificant and not large fold change (TF)\nnot significant and large fold (FT)\nsignificant and large fold change (TT)\n\nAnd I use scale_colour_manual() to set the colours for these categories.\nğŸ¬ Create a volcano plot of the results with the points coloured by significance and fold change:\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFor exploring the data, I like add labels to all the significant genes with a large fold change so I can very quickly identity them. The ggrepel package has a function geom_text_repel() that is useful for adding labels so that they donâ€™t overlap.\nğŸ¬ Load the package:\n\nlibrary(ggrepel)\n\nğŸ¬ Add labels to the significant genes with a large fold change:\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_text_repel(data = pro_meta_results |&gt; \n                    filter(bigfc == TRUE, sig == TRUE),\n                  aes(label = description),\n                  size = 3,\n                  max.overlaps = 50) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nError in `geom_text_repel()`:\n! Problem while computing aesthetics.\nâ„¹ Error occurred in the 5th layer.\nCaused by error:\n! object 'description' not found\n\n\nNotice that I have used filter() label only the genes that are both significant and have a large fold change. In systems you are familiar with, this labelling is very informative and can help you quickly identify common themes. However, in this case, we do not have good annotation for the genes. We can label only with the gene id or the description. Many of the descriptions are\nğŸ¬ Add labels to the significant genes with a large fold change only where description doesnâ€™t contain â€œhypotheticalâ€ or â€œunspecified :\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_text_repel(data = pro_meta_results |&gt; \n                    filter(bigfc == TRUE, sig == TRUE,\n                           !str_detect(description, \"hypothetical|unspecified\")),\n                  aes(label = description),\n                  size = 3,\n                  max.overlaps = 50) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nError in `filter()`:\nâ„¹ In argument: `!str_detect(description, \"hypothetical|unspecified\")`.\nCaused by error:\n! object 'description' not found\n\n\nKey to interpreting the volcano plot is to remember that positive fold changes means the gene is up-regulated in the procyclic stage and negative fold changes means the gene is down-regulated (i.e., higher in the metacyclic). This was determined by the order of the treatments in the contrast used in the DESeq2 analysis\nIf you do forget which way round you did the comparison, you can always examine the results dataframe to see which of the treatments seem to be higher for the positive fold changes.\nWhen you have a gene of interest, you may wish to label it, and only it, on the plot. This is done in the same way except that you filter the data to only include the gene of interest. I have used and then use geom_label_repel() rather than geom_text_repel() to put the label in a box and nudged itâ€™s position to get a line connecting the point and the label. I have also increased the size of the point.\nğŸ¬ Add a label to one gene of interest (elongation factor 1-alpha) and :\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_label_repel(data = pro_meta_results |&gt; \n                    filter(description == \"elongation factor 1-alpha\"),\n                  aes(label = description),\n                  size = 4,\n                  nudge_x = .5,\n                  nudge_y = 1.5) +\n  geom_point(data = pro_meta_results |&gt; \n                    filter(description == \"elongation factor 1-alpha\"),\n                size = 3) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nError in `filter()`:\nâ„¹ In argument: `description == \"elongation factor 1-alpha\"`.\nCaused by error:\n! object 'description' not found\n\n\nShould you want to label more than one gene, you will need to use (for example): filter(description %in% c(\"elongation factor 1-alpha\", \"ADP/ATP translocase 1 putative\"))\nNow go to Save your plots",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#stem-cells-volcano",
    "href": "transcriptomics/week-5/workshop.html#stem-cells-volcano",
    "title": "Workshop",
    "section": "ğŸ­ Stem cells",
    "text": "ğŸ­ Stem cells\nWe will add a column to the results dataframe that contains the -log10(FDR). You could perform this transformation within the plot command without adding a column to the data if you prefer.\nğŸ¬ Add a column to the results dataframe that contains the -log10(FDR):\n\nhspc_prog_results &lt;- hspc_prog_results |&gt; \n  mutate(log10_FDR = -log10(FDR)) \n\nğŸ¬ Create a volcano plot of the results:\n\nhspc_prog_results |&gt; \n  ggplot(aes(x = summary.logFC, \n             y = log10_FDR)) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nOur dashed lines are at -log10(0.05) and log2(2) and log2(-2) to make more clear which genes (points) are significantly different between the cell types and have a fold change of at least 2.\nIn most cases, people colour the points to show that the quadrants. I like to add columns to the dataframe to indicate if the gene is significant and if the fold change is large and use those variables in the plot.\nğŸ¬ Add columns to the results dataframe to indicate if the gene is significant and if the fold change is large:\n\nhspc_prog_results &lt;- hspc_prog_results |&gt; \n  mutate(sig = FDR &lt;= 0.05,\n         bigfc = abs(summary.logFC) &gt;= 2) \n\nThe use of abs() (absolute) means genes with a fold change of at least 2 in either direction will be considered to have a large fold change.\nNow we can colour the points by these new columns. I use interaction() to create four categories:\n\nnot significant and not large fold change (FF)\nsignificant and not large fold change (TF)\nnot significant and large fold (FT)\nsignificant and large fold change (TT)\n\nAnd I use scale_colour_manual() to set the colours for these categories.\nNOTE: there are no â€œnot significant and large fold change (FT)â€ in this case. This means we do not need four colours. I have put the â€œgray30â€ colour in the code but commented it out.\nğŸ¬ Create a volcano plot of the results with the points coloured by significance and fold change:\n\nhspc_prog_results |&gt; \n  ggplot(aes(x = summary.logFC, \n             y = log10_FDR, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                # \"gray30\",\n                                 \"deeppink\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFor exploring the data, I like add labels to all the significant genes with a large fold change so I can very quickly identity them. The ggrepel package has a function geom_text_repel() that is useful for adding labels so that they donâ€™t overlap.\nğŸ¬ Load the package:\n\nlibrary(ggrepel)\n\nğŸ¬ Add labels to the significant genes with a large fold change:\n\nhspc_prog_results |&gt; \n  ggplot(aes(x = summary.logFC, \n             y = log10_FDR, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                              #   \"gray30\",\n                                 \"deeppink\")) +\n  geom_text_repel(data = hspc_prog_results |&gt; \n                    filter(bigfc == TRUE, sig == TRUE),\n                  aes(label = external_gene_name),\n                  size = 3,\n                  max.overlaps = 50) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nNotice that I have used filter() label only the genes that are both significant and have a large fold change. In systems you are familiar with, this labelling is very informative and can help you quickly identify common themes. Key to interpreting the volcano plot is to remember that positive fold changes means the gene is up-regulated in the Prog and negative fold changes means the gene is down-regulated (i.e., higher HSPC). This was determined by us choosing the results_hspc_prog$prog dataframe from the list object\nIf you do forget which way round you did the comparison, you can always examine the gene summary dataframe to see which of the treatments seem to be higher for the positive fold changes.\nWhen you have a gene of interest, you may wish to label it on the plot. This is done in the same way except that you filter the data to only include the gene of interest. I have used and then use geom_label_repel() rather than geom_text_repel() to put the label in a box and nudged itâ€™s position to get a line connecting the point and the label. I have also increased the size of the point.\nğŸ¬ Add a label to one gene of interest Emilin2 (I made an arbitrary choice):\n\nhspc_prog_results |&gt; \n  ggplot(aes(x = summary.logFC, \n             y = log10_FDR, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                # \"gray30\",\n                                 \"deeppink\")) +\n  geom_label_repel(data = hspc_prog_results |&gt; \n                    filter(external_gene_name == \"Emilin2\"),\n                  aes(label = external_gene_name),\n                  size = 4) +\n  geom_point(data = hspc_prog_results |&gt; \n                    filter(external_gene_name == \"Emilin2\"),\n                size = 3) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nShould you want to label more than one gene, you will need to use (for example): filter(external_gene_name %in% c(\"Emilin2\", \"Col5a1\"))\nNow go to Save your plots",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  }
]