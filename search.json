[
  {
    "objectID": "structures/structures.html",
    "href": "structures/structures.html",
    "title": "Structure Data Analysis for Group Project",
    "section": "",
    "text": "There is an RStudio project containing a Quarto version of the the Antibody Mimetics Workshop by Michael Plevin & Jon Agirre. Instructions to obtain the RStudio project are at the bottom of this document after the set up instructions.\nYou might find RStudio useful for Python because you are already familiar with it. It is also a good way to create Quarto documents with code chunks in more than one language. Quarto documents can be used in RStudio, VS Code or Jupyter notebooks\nSome set up is required before you will be able to execute code in antibody_mimetics_workshop_3.qmd. This in contrast to the Colab notebook which is a cloud-based Jupyter notebook and does not require any set up (except installing packages).\n\nüé¨ If using your own machine, install Python from https://www.python.org/downloads/. This should not be necessary if you are using a university machine where Python is already installed.\nüé¨ If using your own machine and you did not install Quarto in the Core 1 workshop, install it now from https://quarto.org/docs/get-started/. This should not be necessary if you are using a university machine where quarto is already installed.\nüé¨ Open RStudio and check you are using a ‚ÄúGit bash‚Äù Terminal: Tools | Global Options| Terminal | New Terminal opens with‚Ä¶ . If the option to choose Git bash, you will need to install Git from https://git-scm.com/downloads. Quit RStudio first. This should not be necessary if you are using a university machine where Git bash is already installed.\nüé¨ If on your own machine: In RStudio, install the quarto and the recticulate packages. This should not be necessary if you are using a university machine where these packages are already installed.\nüé¨ Whether you are using your own machine or a university machine, you need to install some python packages. In RStudio and go to the Terminal window (behind the Console window). Run the following commands in the Terminal window:\npython -m pip install --upgrade pip setuptools wheel\nYou may get these warnings about scripts not being on the path. You can ignore these.\n  WARNING: The script wheel.exe is installed in 'C:\\Users\\er13\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The scripts pip.exe, pip3.11.exe, pip3.9.exe and pip3.exe are installed in 'C:\\Users\\er13\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspyder 5.1.5 requires pyqt5&lt;5.13, which is not installed.\nspyder 5.1.5 requires pyqtwebengine&lt;5.13, which is not installed.\nconda-repo-cli 1.0.4 requires pathlib, which is not installed.\nanaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\nSuccessfully installed pip-23.3.1 setuptools-69.0.2 wheel-0.41.3\npython -m pip install session_info\npython -m pip install wget\npython -m pip install gemmi\nNote: On my windows laptop at home, I also had to install C++ Build Tools to be able to install the gemmi python package. If this is true for you, you will get a fail message telling you to install C++ build tools if you need them. These are from https://visualstudio.microsoft.com/visual-cpp-build-tools/ You need to check the Workloads tab and select C++ build tools.\n\nYou can then install the gemmi package again.\nI think that‚Äôs it! You can now download the RStudio project and run each chunk in the quarto document.\nThere is an example RStudio project here: structure-analysis. You can also download the project as a zip file from there but there is some code that will do that automatically for you. Since this is an RStudio Project, do not run the code from inside a project. You may want to navigate to a particular directory or edit the destdir:\n\nusethis::use_course(url = \"3mmaRand/structure-analysis\", destdir = \".\")\n\nYou can agree to deleting the zip. You should find RStudio restarts and you have a new project called structure-analysis-xxxxxx. The xxxxxx is a commit reference - you do not need to worry about that, it is just a way to tell you which version of the repo you downloaded.\nYou should be able to open the antibody_mimetics_workshop_3.qmd file and run each chunk. You can also knit the document to html.",
    "crumbs": [
      "Structure Analysis",
      "Structure Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "structures/structures.html#programmatic-protein-structure-analysis",
    "href": "structures/structures.html#programmatic-protein-structure-analysis",
    "title": "Structure Data Analysis for Group Project",
    "section": "",
    "text": "There is an RStudio project containing a Quarto version of the the Antibody Mimetics Workshop by Michael Plevin & Jon Agirre. Instructions to obtain the RStudio project are at the bottom of this document after the set up instructions.\nYou might find RStudio useful for Python because you are already familiar with it. It is also a good way to create Quarto documents with code chunks in more than one language. Quarto documents can be used in RStudio, VS Code or Jupyter notebooks\nSome set up is required before you will be able to execute code in antibody_mimetics_workshop_3.qmd. This in contrast to the Colab notebook which is a cloud-based Jupyter notebook and does not require any set up (except installing packages).\n\nüé¨ If using your own machine, install Python from https://www.python.org/downloads/. This should not be necessary if you are using a university machine where Python is already installed.\nüé¨ If using your own machine and you did not install Quarto in the Core 1 workshop, install it now from https://quarto.org/docs/get-started/. This should not be necessary if you are using a university machine where quarto is already installed.\nüé¨ Open RStudio and check you are using a ‚ÄúGit bash‚Äù Terminal: Tools | Global Options| Terminal | New Terminal opens with‚Ä¶ . If the option to choose Git bash, you will need to install Git from https://git-scm.com/downloads. Quit RStudio first. This should not be necessary if you are using a university machine where Git bash is already installed.\nüé¨ If on your own machine: In RStudio, install the quarto and the recticulate packages. This should not be necessary if you are using a university machine where these packages are already installed.\nüé¨ Whether you are using your own machine or a university machine, you need to install some python packages. In RStudio and go to the Terminal window (behind the Console window). Run the following commands in the Terminal window:\npython -m pip install --upgrade pip setuptools wheel\nYou may get these warnings about scripts not being on the path. You can ignore these.\n  WARNING: The script wheel.exe is installed in 'C:\\Users\\er13\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The scripts pip.exe, pip3.11.exe, pip3.9.exe and pip3.exe are installed in 'C:\\Users\\er13\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspyder 5.1.5 requires pyqt5&lt;5.13, which is not installed.\nspyder 5.1.5 requires pyqtwebengine&lt;5.13, which is not installed.\nconda-repo-cli 1.0.4 requires pathlib, which is not installed.\nanaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\nSuccessfully installed pip-23.3.1 setuptools-69.0.2 wheel-0.41.3\npython -m pip install session_info\npython -m pip install wget\npython -m pip install gemmi\nNote: On my windows laptop at home, I also had to install C++ Build Tools to be able to install the gemmi python package. If this is true for you, you will get a fail message telling you to install C++ build tools if you need them. These are from https://visualstudio.microsoft.com/visual-cpp-build-tools/ You need to check the Workloads tab and select C++ build tools.\n\nYou can then install the gemmi package again.\nI think that‚Äôs it! You can now download the RStudio project and run each chunk in the quarto document.\nThere is an example RStudio project here: structure-analysis. You can also download the project as a zip file from there but there is some code that will do that automatically for you. Since this is an RStudio Project, do not run the code from inside a project. You may want to navigate to a particular directory or edit the destdir:\n\nusethis::use_course(url = \"3mmaRand/structure-analysis\", destdir = \".\")\n\nYou can agree to deleting the zip. You should find RStudio restarts and you have a new project called structure-analysis-xxxxxx. The xxxxxx is a commit reference - you do not need to worry about that, it is just a way to tell you which version of the repo you downloaded.\nYou should be able to open the antibody_mimetics_workshop_3.qmd file and run each chunk. You can also knit the document to html.",
    "crumbs": [
      "Structure Analysis",
      "Structure Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "core/week-6-old/study_before_workshop.html",
    "href": "core/week-6-old/study_before_workshop.html",
    "title": "Independent Study to prepare for workshop",
    "section": "",
    "text": "üìñ Read materials from Core 1 Organising reproducible data analyses and make a note of questions you have\nüìñ Read materials from Core 2 File types, workflow tips and other tools and make a note of questions you have.\nüìñ Review Stage 1 and 2 (88H students) or 52M (70M students) content to see if there are areas you might benefit from revisiting. You can access these through the past VLE sites but you might find it helpful to use the latest versions, particularly for stage 1.\n\nStage 1\n\nData Analysis in R for Becoming a Bioscientist 1.Core concepts about scientific computing, types of variable, the role of variables in analysis and how to use RStudio to organise analysis and import, summarise and plot data.\nData Analysis in R for Becoming a Bioscientist 2. The logic of hypothesis testing, confidence intervals, what is meant by a statistical model, two-sample tests and one- and two-way analysis of variance (ANOVA).\n\nStage 2\n\nGet Introductory Statistical Tests as Linear models: A guide for R users\nA simple introduction to GLM for analysing Poisson and Binomial responses in R\n\n52M\n\n52M Data Analysis in R. Core concepts about scientific computing, types of variable, the role of variables in analysis and how to use RStudio to organise analysis and import, summarise and plot data, the logic of hypothesis testing, confidence intervals, what is meant by a statistical model, two-sample tests and one-way analysis of variance (ANOVA) and reproducible reports in Quarto."
  },
  {
    "objectID": "core/week-6-old/study_after_workshop.html",
    "href": "core/week-6-old/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "There is no consolidation work other than to continue revising what you have learned over the course of your degree about data analysis."
  },
  {
    "objectID": "core/week-2/study_before_workshop.html",
    "href": "core/week-2/study_before_workshop.html",
    "title": "Independent Study to prepare for workshop",
    "section": "",
    "text": "üìñ Read Understanding file systems. This is an approximately 15 - 20 minute read revising file types and file systems. It covers concepts of working directories and paths. We learned these ideas in stage 1 and you may feel completely confident with them but many students will benefit from a refresher. For BIO00070M students, this is part of the work you will also be asked to complete for BIO00052M Data Analysis in R.\nüìñ Read Workflow in RStudio. You may find it helpful to remind yourself about RStudio Projects. In previous years, you have submitted an ‚ÄúRStudio Project‚Äù as part of your BABS work. In this module, you will submit ‚ÄúSupporting Information‚Äù for your Project Report. The Supporting Information is a documented and organised collection of all the digital parts of your research project. This includes data (or instructions for accessing data), code and/or non-coded processing, instructions for use, computational requirements and outputs. The Supporting Information could be a single RStudio Project (like you have done previously but with better documentation) or a folder that includes an RStudio Project and other material/scripts.\n\n3.üíª Set up the Virtual Desktop. I very strongly recommend working on the University computers for this work. You will be using more specialised R packages than you might be used to. This is especially important if you often have difficulty updating and or installing software on your own machine, wouldn‚Äôt know what what version of R you are using or don‚Äôt realise there is a difference between R and RStudio. The uni machines always have up-to-date R and R packages and all the packages that appear in teaching materials. It is my responsibility to ensure everything works on here.\nYou can still work from home by using the Virtual Desktop Service. The VDS allows you to log on to a university computer from your own computer. It means you can access all software and filestores. When using the VDS for R and RStudio, it usually makes sense to use other software - such as a browser or file explorer - also through the VDS.\nIf you are confident in your ability to set up your own machine, you need:\n\nto know the difference between R and RStudio\nto use R 4.4 and RStudio 2024.09.0 Build 375 (‚ÄúCranberry Hibiscus‚Äù)\nbe certain you are actually using R 4.4 - it is written in the top edge of the console window. By default RStudio uses the latest version on R on your machine. However, windows users are able to change this to a ‚Äúspecific version‚Äù. You might have done that previously. Change it back using Tools | Global Options R version ‚ÄúUse your machine‚Äôs default 64-bit version of R‚Äù\nto make sure you do the independent study where it tells you what steps you need to take to get packages (and versions that are unlikely to cause issues) used in the workshop\n\nIt is possible to access all your files on your university account without using the VDS. For example, if you want to work on uni machines at uni and your machine at home. You can best do this by mapping a drive: https://support.york.ac.uk/s/topic/0TO4K000000lA5ZWAU/filestores. If you store everything on google drive you can also read/write to that like any other drive using google drive app.\nEven if you plan to use your own machine I really recommend you take the time to set the VDS up now while you‚Äôre not time pressured so you always have that option ready.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Prepare!"
    ]
  },
  {
    "objectID": "core/week-2/study_after_workshop.html",
    "href": "core/week-2/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "Before starting work on your strand specific data analysis. Ensure you have EITHER:\n\nüé¨ Set up the Virtual Desktop Service and been able to use it\n\nOR\n\nüé¨ Updated R\nüé¨ Updated RStudio.\nInstall package building tools\nüé¨ Windows Install Rtools\nüé¨ Mac install Xcode from Mac App Store\nüé¨ Installed packages: devtools, tidyverse, BiocManager, readxl\n\nYou might want to consider getting a GitHub account and applying for student benefits so that you can use GitHub co-pilot. GitHub copilot is an ‚ÄúAI pair programmer that offers autocomplete-style suggestions as you code‚Äù. GitHub Copilot is available as an opt-in integration with RStudio.\n\nRead more about GitHub Copilot integration with RStudio\nüé¨ Create a GitHub account\nüé¨ Apply for student benefits. You will need to upload an image of your student id, use your institutional email address and you will be required to use two factor authentication.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Consolidate!"
    ]
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#overview",
    "href": "core/week-2-old/study_before_workshop.html#overview",
    "title": "Independent Study to prepare for workshop",
    "section": "Overview",
    "text": "Overview\n\nRStudio Projects revisited\n\nusing usethis package\nAdding a README\n\n\nFormatting code\nCode algorithmically / algebraically."
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#reproducibility-is-a-continuum",
    "href": "core/week-2-old/study_before_workshop.html#reproducibility-is-a-continuum",
    "title": "Independent Study to prepare for workshop",
    "section": "Reproducibility is a continuum",
    "text": "Reproducibility is a continuum\nSome is better than none!\n\nOrganise your project\n\nScript everything.\n\nFormat code and follow a consistent style.\n\nCode algorithmically\nModularise your code: organise into sections and scripts\nDocument your project - commenting, READMEs\nUse literate programming e.g., R Markdown or Quarto\n\n\n\nMore advanced: Version control, continuous integration, environments, containers"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#rstudio-projects",
    "href": "core/week-2-old/study_before_workshop.html#rstudio-projects",
    "title": "Independent Study to prepare for workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\nWe used RStudio Projects in stage one but they are so useful, it is worth covering them again in case you are not yet using them.\nWe will also cover the usethisworkflow to create an RStudio Project.\nRStudio Projects make it easy to manage working directories and paths because they set the working directory to the RStudio Projects directory automatically."
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#rstudio-projects-1",
    "href": "core/week-2-old/study_before_workshop.html#rstudio-projects-1",
    "title": "Independent Study to prepare for workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\n\n-- stem_cell_rna\n   |__stem_cell_rna.Rproj   \n   |__raw_ data/            \n      |__2019-03-21_donor_1.csv\n   |__README. md\n   |__R/\n      |__01_data_processing.R\n      |__02_exploratory.R\n      |__functions/\n         |__theme_volcano.R\n         |__normalise.R\n\n\nThe project directory is the folder at the top 1\n\nThanks to Mine √áetinkaya-Rundel who helped me work out how to highlight a line https://gist.github.com/mine-cetinkaya-rundel/3af3415eab70a65be3791c3dcff6e2e3. Note to futureself: the engine: knitr matters."
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#rstudio-projects-2",
    "href": "core/week-2-old/study_before_workshop.html#rstudio-projects-2",
    "title": "Independent Study to prepare for workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\n\n-- stem_cell_rna\n   |__stem_cell_rna.Rproj   \n   |__raw_ data/            \n      |__2019-03-21_donor_1.csv\n   |__README. md\n   |__R/\n      |__01_data_processing.R\n      |__02_exploratory.R\n      |__functions/\n         |__theme_volcano.R\n         |__normalise.R\n\n\nthe .RProj file is directly under the project folder. Its presence is what makes the folder an RStudio Project"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#rstudio-projects-3",
    "href": "core/week-2-old/study_before_workshop.html#rstudio-projects-3",
    "title": "Independent Study to prepare for workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\nWhen you open an RStudio Project, the working directory is set to the Project directory (i.e., the location of the .Rproj file).\nWhen you use an RStudio Project you do not need to use setwd()\nWhen someone, including future you, opens the project on another machine, all the paths just work."
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#rstudio-projects-4",
    "href": "core/week-2-old/study_before_workshop.html#rstudio-projects-4",
    "title": "Independent Study to prepare for workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\nJenny BryanIn the words of Jenny Bryan:\n\n‚ÄúIf the first line of your R script is setwd(‚ÄùC:/Users/jenny/path/that/only/I/have‚Äù) I will come into your office and SET YOUR COMPUTER ON FIRE‚Äù"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#creating-an-rstudio-project",
    "href": "core/week-2-old/study_before_workshop.html#creating-an-rstudio-project",
    "title": "Independent Study to prepare for workshop",
    "section": "Creating an RStudio Project",
    "text": "Creating an RStudio Project\nThere are two ways to create an RStudio Project.\n\nUsing one of the two menus\nUsing the usethis package"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#using-a-menu",
    "href": "core/week-2-old/study_before_workshop.html#using-a-menu",
    "title": "Independent Study to prepare for workshop",
    "section": "Using a menu",
    "text": "Using a menu\nThere are two menus:\n\nTop left, File menu\nTop Right, drop-down indicated by the .RProj icon\n\nThey both do the same thing.\nIn both cases you choose: New Project | New Directory | New Project\n\nMake sure you ‚ÄúBrowse‚Äù to the folder you want to create the project."
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-1",
    "href": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Using the usethis package",
    "text": "Using the usethis package\nI occasionally use the menu but I mostly use the usethis package.\n\nüé¨ Go to RStudio and check your working directory:\n\ngetwd()\n\n\"C:/Users/er13/Desktop\"\n\n\n‚ùî Is your working directory a good place to create a Project folder?"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-2",
    "href": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-2",
    "title": "Independent Study to prepare for workshop",
    "section": "Using the usethis package",
    "text": "Using the usethis package\nIf this is a good place to create a Project directory then‚Ä¶\nüé¨ Create a project with:\n\nusethis::create_project(\"bananas\")"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-3",
    "href": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-3",
    "title": "Independent Study to prepare for workshop",
    "section": "Using the usethis package",
    "text": "Using the usethis package\nOtherwise\nIf you want the project directory elsewhere, you will need to give the relative path, e.g.\n\nusethis::create_project(\"../Documents/bananas\")"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-4",
    "href": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-4",
    "title": "Independent Study to prepare for workshop",
    "section": "Using the usethis package",
    "text": "Using the usethis package\nThe output will look like this and a new RStudio session will start.\n&gt; usethis::create_project(\"bananas\")\n‚àö Creating 'bananas/'\n‚àö Setting active project to 'C:/Users/er13/Desktop/bananas'\n‚àö Creating 'R/'\n‚àö Writing 'bananas.Rproj'\n‚àö Adding '.Rproj.user' to '.gitignore'\n‚àö Opening 'C:/Users/er13/Desktop/bananas/' in new RStudio session\n‚àö Setting active project to '&lt;no active project&gt;'"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-5",
    "href": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-5",
    "title": "Independent Study to prepare for workshop",
    "section": "Using the usethis package",
    "text": "Using the usethis package\nWhen you create a new RStudio Project with usethis:\n\n\nA folder called bananas/ is created\nRStudio starts a new session in bananas/ i.e., your working directory is now bananas/\n\nA folder called R/ is created\nA file called bananas.Rproj is created\nA file called .gitignore is created\nA hidden directory called .Rproj.user is created"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-6",
    "href": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-6",
    "title": "Independent Study to prepare for workshop",
    "section": "Using the usethis package",
    "text": "Using the usethis package\n\n\nthe .Rproj file is what makes the directory an RStudio Project\nthe Rproj.user directory is where project-specific temporary files are stored. You don‚Äôt need to mess with it.\nthe .gitignore is used for version controlled projects. If not using git, you can ignore it."
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#opening-and-closing",
    "href": "core/week-2-old/study_before_workshop.html#opening-and-closing",
    "title": "Independent Study to prepare for workshop",
    "section": "Opening and closing",
    "text": "Opening and closing\nYou can close an RStudio Project with ONE of:\n\nFile | Close Project\nUsing the drop-down option on the far right of the tool bar where you see the Project name\n\n\nYou can open an RStudio Project with ONE of:\n\nFile | Open Project or File | Recent Projects\n\nUsing the drop-down option on the far right of the tool bar where you see the Project name\n\nDouble-clicking an .Rproj file from your file explorer/finder\n\nWhen you open project, a new R session starts."
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-7",
    "href": "core/week-2-old/study_before_workshop.html#using-the-usethis-package-7",
    "title": "Independent Study to prepare for workshop",
    "section": "Using the usethis package",
    "text": "Using the usethis package\nOnce the RStudio project has been created, usethis helps you follow good practice.\n\nüé¨ We can add a README with:\n\nusethis::use_readme_md()\n\n\n\nThis creates a file called README.md, with a little default text, in the Project directory and opens it for editing.\n\n\nmd stands for markdown, it is a extremely widely used text formatting language which is readable as plain text. If you have ever used asterisks to make text bold or italic, you have used markdown."
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#code-formatting-and-style-1",
    "href": "core/week-2-old/study_before_workshop.html#code-formatting-and-style-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Code formatting and style",
    "text": "Code formatting and style\n\n‚ÄúGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.‚Äù\n\nThe tidyverse style guide"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#code-formatting-and-style-2",
    "href": "core/week-2-old/study_before_workshop.html#code-formatting-and-style-2",
    "title": "Independent Study to prepare for workshop",
    "section": "Code formatting and style",
    "text": "Code formatting and style\nWe have all written code which is hard to read!\nWe all improve over time.\n\n\n\nThe only way to write good code is to write tons of shitty code first. Feeling shame about bad code stops you from getting to good code‚Äî Hadley Wickham (@hadleywickham) April 17, 2015"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#code-formatting-and-style-3",
    "href": "core/week-2-old/study_before_workshop.html#code-formatting-and-style-3",
    "title": "Independent Study to prepare for workshop",
    "section": "Code formatting and style",
    "text": "Code formatting and style\nSome keys points:\n\nbe consistent, emulate experienced coders\n\nuse snake_case for variable names (not CamelCase, dot.case)\n\nuse &lt;- not = for assignment\n\nuse spacing around most operators and after commas\n\nuse indentation\n\navoid long lines, break up code blocks with new lines\n\nuse \" for quoting text (not ') unless the text contains double quotes"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#ugly-code",
    "href": "core/week-2-old/study_before_workshop.html#ugly-code",
    "title": "Independent Study to prepare for workshop",
    "section": "üò© Ugly code üò©",
    "text": "üò© Ugly code üò©\n\ndata&lt;-read_csv('../data-raw/Y101_Y102_Y201_Y202_Y101-5.csv',skip=2)\nlibrary(janitor);sol&lt;-clean_names(data)\ndata=data|&gt;filter(str_detect(description,\"OS=Homo sapiens\"))|&gt;filter(x1pep=='x')\ndata=data|&gt;\nmutate(g=str_extract(description,\n\"GN=[^\\\\s]+\")|&gt;str_replace(\"GN=\",''))\ndata&lt;-data|&gt;mutate(id=str_extract(accession,\"1::[^;]+\")|&gt;str_replace(\"1::\",\"\"))"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#ugly-code-1",
    "href": "core/week-2-old/study_before_workshop.html#ugly-code-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üò© Ugly code üò©",
    "text": "üò© Ugly code üò©\n\nno spacing or indentation\ninconsistent splitting of code blocks over lines\ninconsistent use of quote characters\nno comments\nvariable names convey no meaning\nuse of = for assignment and inconsistently\nmultiple commands on a line\nlibrary statement in the middle of the analysis"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#cool-code",
    "href": "core/week-2-old/study_before_workshop.html#cool-code",
    "title": "Independent Study to prepare for workshop",
    "section": "üòé Cool code üòé",
    "text": "üòé Cool code üòé\n\n# Packages ----------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Import ------------------------------------------------------------------\n\n# define file name\nfile &lt;- \"../data-raw/Y101_Y102_Y201_Y202_Y101-5.csv\"\n\n# import: column headers and data are from row 3\nsolu_protein &lt;- read_csv(file, skip = 2) |&gt;\n  janitor::clean_names()\n\n# Tidy data ----------------------------------------------------------------\n\n# filter out the bovine proteins and those proteins \n# identified from fewer than 2 peptides\nsolu_protein &lt;- solu_protein |&gt;\n  filter(str_detect(description, \"OS=Homo sapiens\")) |&gt;\n  filter(x1pep == \"x\")\n\n# Extract the genename from description column to a column\n# of its own\nsolu_protein &lt;- solu_protein |&gt;\n  mutate(genename =  str_extract(description,\"GN=[^\\\\s]+\") |&gt;\n           str_replace(\"GN=\", \"\"))\n\n# Extract the top protein identifier from accession column (first\n# Uniprot ID after \"1::\") to a column of its own\nsolu_protein &lt;- solu_protein |&gt;\n  mutate(protid =  str_extract(accession, \"1::[^;]+\") |&gt;\n           str_replace(\"1::\", \"\"))"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#cool-code-1",
    "href": "core/week-2-old/study_before_workshop.html#cool-code-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üòé Cool code üòé",
    "text": "üòé Cool code üòé\n\nlibrary() calls collected\nUses code sections to make it easier to navigate\nUses white space and proper indentation\nCommented\nUses more informative name for the dataframe"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#code-algorithmically-1",
    "href": "core/week-2-old/study_before_workshop.html#code-algorithmically-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Code ‚Äòalgorithmically‚Äô",
    "text": "Code ‚Äòalgorithmically‚Äô\n\n\nWrite code which expresses the structure of the problem/solution.\nAvoid hard coding numbers if at all possible - declare variables instead\nDeclare frequently used values as variables at the start e.g., colour schemes, figure saving settings"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#hard-coding-numbers.",
    "href": "core/week-2-old/study_before_workshop.html#hard-coding-numbers.",
    "title": "Independent Study to prepare for workshop",
    "section": "üò© Hard coding numbers.",
    "text": "üò© Hard coding numbers.\n\n\nSuppose we want to calculate the sums of squares, \\(SS(x)\\), for the number of eggs in five nests.\nThe formula is given by: \\(\\sum (x_i- \\bar{x})^2\\)\nWe could calculate the mean and copy it, and the individual numbers into the formula"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#hard-coding-numbers.-1",
    "href": "core/week-2-old/study_before_workshop.html#hard-coding-numbers.-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üò© Hard coding numbers.",
    "text": "üò© Hard coding numbers.\n\n# mean number of eggs per nest\nsum(3, 5, 6, 7, 8) / 5\n\n[1] 5.8\n\n# ss(x) of number of eggs\n(3 - 5.8)^2 + (5 - 5.8)^2 + (6 - 5.8)^2 + (7 - 5.8)^2 + (8 - 5.8)^2\n\n[1] 14.8\n\n\nI am coding the calculation of the mean rather using the mean() function only to explain what ‚Äòcoding algorithmically‚Äô means using a simple example."
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#hard-coding-numbers",
    "href": "core/week-2-old/study_before_workshop.html#hard-coding-numbers",
    "title": "Independent Study to prepare for workshop",
    "section": "üò© Hard coding numbers",
    "text": "üò© Hard coding numbers\n\n\nif any of the sample numbers must be altered, all the code needs changing\nit is hard to tell that the output of the first line is a mean\nits hard to recognise that the numbers in the mean calculation correspond to those in the next calculation\nit is hard to tell that 5 is just the number of nests\nno way of know if numbers are the same by coincidence or they refer to the same thing"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#better",
    "href": "core/week-2-old/study_before_workshop.html#better",
    "title": "Independent Study to prepare for workshop",
    "section": "üòé Better",
    "text": "üòé Better\n\n# eggs each nest\neggs &lt;- c(3, 5, 6, 7, 8)\n\n# mean eggs per nest\nmean_eggs &lt;- sum(eggs) / length(eggs)\n\n# ss(x) of number of eggs\nsum((eggs - mean_eggs)^2)\n\n[1] 14.8"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#better-1",
    "href": "core/week-2-old/study_before_workshop.html#better-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üòé Better",
    "text": "üòé Better\n\n\nthe commenting is similar but it is easier to follow\nif any of the sample numbers must be altered, only that number needs changing\nassigning a value you will later use to a variable with a meaningful name allows us to understand the first and second calculations\nmakes use of R‚Äôs elementwise calculation which resembles the formula (i.e., is expressed as the general rule)"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#summary",
    "href": "core/week-2-old/study_before_workshop.html#summary",
    "title": "Independent Study to prepare for workshop",
    "section": "Summary",
    "text": "Summary\n\n\nUse an RStudio project for any R work (you can also incorporate other languages)\nWrite Cool code not Ugly code: space, consistency, indentation, comments, meaningful variable names\nWrite code which expresses the structure of the problem/solution.\nAvoid hard coding numbers if at all possible - declare variables instead"
  },
  {
    "objectID": "core/week-2-old/study_before_workshop.html#references",
    "href": "core/week-2-old/study_before_workshop.html#references",
    "title": "Independent Study to prepare for workshop",
    "section": "References",
    "text": "References\n\n\n\nüîó About Core 2: File types, workflow tips and other tools\n\n\n\n\nBryan, Jennifer. 2018. ‚ÄúExcuse Me, Do You Have a Moment to Talk about Version Control?‚Äù Am. Stat. 72 (1): 20‚Äì27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\nBryan, Jennifer, Jim Hester, Shannon Pileggi, and E. David Aja. n.d. What They Forgot to Teach You about r. https://rstats.wtf/.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig. 2013. ‚ÄúTen Simple Rules for Reproducible Computational Research.‚Äù PLoS Comput. Biol. 9 (10): e1003285. https://doi.org/10.1371/journal.pcbi.1003285.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K Teal. 2017. ‚ÄúGood Enough Practices in Scientific Computing.‚Äù PLoS Comput. Biol. 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "core/week-2-old/study_after_workshop.html",
    "href": "core/week-2-old/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "bbbb"
  },
  {
    "objectID": "core/week-1-old/study_before_workshop.html",
    "href": "core/week-1-old/study_before_workshop.html",
    "title": "Independent Study to prepare for workshop",
    "section": "",
    "text": "üìñ Read Understanding file systems. This is an approximately 15 - 20 minute read revising file types and filesystems. It covers concepts of working directories and paths. We learned these ideas in stage 1 and you may feel completely confident with them but many students will benefit from a refresher. For BIO00070M students, this is part of the work you will also be asked to complete for BIO00052M Data Analysis in R.\nIn previous years you have submitted and RStudio Project as part of your BABS work. In this module you will develop this by submitting a Research Compendium. A Research Compendium is a documented collection of all the digital parts of the research project including data (or access to data), code and outputs. The Compendium might be a single Quarto/RStudio Project, (like you have done previously but with better documentation) or it might be a folder including an Quarto/RStudio Project and other material/scripts including the description of unscripted processing. You might want to remind yourself of the example RStudio Project, Y12345678.zip used in BABS 2."
  },
  {
    "objectID": "core/week-1-old/study_after_workshop.html",
    "href": "core/week-1-old/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "These are suggestions"
  },
  {
    "objectID": "core/week-1-old/study_after_workshop.html#bio00088h-group-research-project-students",
    "href": "core/week-1-old/study_after_workshop.html#bio00088h-group-research-project-students",
    "title": "Independent Study to consolidate this week",
    "section": "BIO00088H Group Research Project students",
    "text": "BIO00088H Group Research Project students\n\nRevise previous Data Analysis materials. You can find the version you took on the VLE site for 17C / 08C. However, my latest versions (in development) are here: Data Analysis in R. The Becoming a Bioscientist (BABS) modules replace the Laboratory and Professional Skills modules. BABS1 and BABS2 are stage one, and I‚Äôve tried to improve them over 17C / 08C. The site is also searchable (icon top right)"
  },
  {
    "objectID": "core/week-1-old/study_after_workshop.html#msc-bioinformatics-students-doing-bio00070m",
    "href": "core/week-1-old/study_after_workshop.html#msc-bioinformatics-students-doing-bio00070m",
    "title": "Independent Study to consolidate this week",
    "section": "MSc Bioinformatics students doing BIO00070M",
    "text": "MSc Bioinformatics students doing BIO00070M\n\nMake sure you carry out the preparatory work for week 2 of 52M"
  },
  {
    "objectID": "core/week-6/overview.html",
    "href": "core/week-6/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week you will revise some essential concepts for scientific computing: file system organisation, file types, working directories and paths. The workshop will cover a rationale for working reproducibly, project oriented workflow, naming things and documenting your work. We will also examine some file types and the concept of tidy data.\n\nLearning objectives\nThe successful student will be able to:\n\nexplain the organisation of files and directories in a file systems including root, home and working directories\nexplain absolute and relative file paths\nexplain why working reproducibly is important\nknow how to use a project-oriented workflow to organise work\nbe able to give files human- and machine-readable names\noutline some common biological data file formats\n\n\n\nInstructions\n\nPrepare\n\nüìñ Read Understanding file systems\n\nWorkshop\nConsolidate",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "About"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html",
    "href": "core/week-6/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop we will discuss why reproducibility matters and how to organise your work to make it reproducible. We will cover:",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#session-overview",
    "href": "core/week-6/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop we will discuss why reproducibility matters and how to organise your work to make it reproducible. We will cover:",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#what-is-reproducibility",
    "href": "core/week-6/workshop.html#what-is-reproducibility",
    "title": "Workshop",
    "section": "What is reproducibility?",
    "text": "What is reproducibility?\n\nReproducible: Same data + same analysis = identical results. ‚Äú‚Ä¶ obtaining consistent results using the same input data; computational steps, methods, and code; and conditions of analysis. This definition is synonymous with‚Äùcomputational reproducibility‚Äù (National Academies of Sciences et al. 2019)\nReplicable: Different data + same analysis = qualitatively similar results. The work is not dependent on the specificities of the data.\nRobust: Same data + different analysis = qualitatively similar or identical results. The work is not dependent on the specificities of the analysis.\nGeneralisable: Different data + different analysis = qualitatively similar results and same conclusions. The findings can be generalised\n\n\n\n\nThe Turing Way's definitions of reproducible research",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#why-does-it-matter",
    "href": "core/week-6/workshop.html#why-does-it-matter",
    "title": "Workshop",
    "section": "Why does it matter?",
    "text": "Why does it matter?\n\n\n\nfutureself, CC-BY-NC, by Julen Colomb\n\n\n\nFive selfish reasons to work reproducibly (Markowetz 2015). Alternatively, see the very entertaining talk\nMany high profile cases of work which did not reproduce e.g.¬†Anil Potti unravelled by Baggerly and Coombes (2009)\nWill become standard in Science and publishing e.g OECD Global Science Forum Building digital workforce capacity and skills for data-intensive science (OECD Global Science Forum 2020)",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#how-to-achieve-reproducibility",
    "href": "core/week-6/workshop.html#how-to-achieve-reproducibility",
    "title": "Workshop",
    "section": "How to achieve reproducibility",
    "text": "How to achieve reproducibility\n\nScripting\nOrganisation: Project-oriented workflows with file and folder structure, naming things\nDocumentation: Readme files, code comments, metadata, version control",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#rationale-for-scripting",
    "href": "core/week-6/workshop.html#rationale-for-scripting",
    "title": "Workshop",
    "section": "Rationale for scripting?",
    "text": "Rationale for scripting?\n\nScience is the generation of ideas, designing work to test them and reporting the results.\nWe ensure laboratory and field work is replicable, robust and generalisable by planning and recording in lab books and using standard protocols. Repeating results is still hard.\nWorkflows for computational projects, and the data analysis and reporting of other work can, and should, be 100% reproducible!\nScripting is the way to achieve this.",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#project-oriented-workflow",
    "href": "core/week-6/workshop.html#project-oriented-workflow",
    "title": "Workshop",
    "section": "Project-oriented workflow",
    "text": "Project-oriented workflow\n\nuse folders to organise your work\nyou are aiming for structured, systematic and repeatable.\ninputs and outputs should be clearly identifiable from structure and/or naming\n\nExamples\n-- liver_transcriptome/\n   |__data\n      |__raw/\n      |__processed/\n   |__images/\n   |__code/\n   |__reports/\n   |__figures/",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#naming-things",
    "href": "core/week-6/workshop.html#naming-things",
    "title": "Workshop",
    "section": "Naming things",
    "text": "Naming things\n\n\n\ndocuments, CC-BY-NC, https://xkcd.com/1459/\n\n\nGuiding principle - Have a convention! Good file names are:\n\nmachine readable\nhuman readable\nplay nicely with sorting\n\nI suggest\n\nno spaces in names\nuse snake_case or kebab-case rather than CamelCase or dot.case\nuse all lower case except very occasionally where convention is otherwise, e.g., README, LICENSE\nordering: use left-padded numbers e.g., 01, 02‚Ä¶.99 or 001, 002‚Ä¶.999\ndates ISO 8601 format: 2020-10-16\nwrite down your conventions\n\n-- liver_transcriptome/\n   |__data\n      |__raw/\n         |__2022-03-21_donor_1.csv\n         |__2022-03-21_donor_2.csv\n         |__2022-03-21_donor_3.csv\n         |__2022-05-14_donor_1.csv\n         |__2022-05-14_donor_2.csv\n         |__2022-05-14_donor_3.csv\n      |__processed/\n   |__images/\n   |__code/\n      |__functions/\n         |__summarise.R\n         |__normalise.R\n         |__theme_volcano.R\n      |__01_data_processing.py\n      |__02_exploratory.R\n      |__03_modelling.R\n      |__04_figures.R\n   |__reports/\n      |__01_report.qmd\n      |__02_supplementary.qmd\n   |__figures/\n      |__01_volcano_donor_1_vs_donor_2.eps\n      |__02_volcano_donor_1_vs_donor_3.eps",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#readme-files",
    "href": "core/week-6/workshop.html#readme-files",
    "title": "Workshop",
    "section": "Readme files",
    "text": "Readme files\nREADMEs are a form of documentation which have been widely used for a long time. They contain all the information about the other files in a directory. They can be extensive but need not be. Concise is good. Bullet points are good\n\nGive a project title and description, brief\nstart date, last updated date and contact information\nOutline the folder structure\nGive software requirements: programs and versions used or required. There are packages that give session information in R Wickham et al. (2021) and Python Ostblom, Joel (2019)\n\nR:\nsessioninfo::session_info()\nPython:\nimport session_info\nsession_info.show()\n\nInstructions run the code, build reports, and reproduce the figures etc\nWhere to find the data, outputs\nAny other information that needed to understand and recreate the work\nIdeally, a summary of changes with the date\n\n-- liver_transcriptome/\n   |__data\n      |__raw/\n         |__2022-03-21_donor_1.csv\n         |__2022-03-21_donor_2.csv\n         |__2022-03-21_donor_3.csv\n         |__2022-05-14_donor_1.csv\n         |__2022-05-14_donor_2.csv\n         |__2022-05-14_donor_3.csv\n      |__processed/\n   |__images/\n   |__code/\n      |__functions/\n         |__summarise.R\n         |__normalise.R\n         |__theme_volcano.R\n      |__01_data_processing.py\n      |__02_exploratory.R\n      |__03_modelling.R\n      |__04_figures.R\n   |__README.md\n   |__reports/\n      |__01_report.qmd\n      |__02_supplementary.qmd\n   |__figures/\n      |__01_volcano_donor_1_vs_donor_2.eps\n      |__02_volcano_donor_1_vs_donor_3.eps",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-6/workshop.html#code-comments",
    "href": "core/week-6/workshop.html#code-comments",
    "title": "Workshop",
    "section": "Code comments",
    "text": "Code comments\n\nComments are notes in the code which are not executed. They are ignored by the computer but are read by humans. They are used to explain what the code is doing and why. They are also used to temporarily remove code from execution.",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Workshop"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for the Group Research Project",
    "section": "",
    "text": "You are either\n\nan integrated masters student doing BIO00088H Group Research Project or\nan MSc Bioinformatics student doing BIO00070M Research, Professional and Team Skills\n\nIntegrated masters students doing 88H will be doing one of these projects:\nThe project types are:\n\n\n\n\n\n\n\n\nTitle\nDirector\nData analysis strand\n\n\n\n\nIdentifying transcriptional targets of FGF signalling in Xenopus embryos.\nBetsy Pownall\nTranscriptomics, Emma Rand\n\n\nInvestigating the differentiation of stem cells in healthy bone marrow\nJillian Barlow\nTranscriptomics, Emma Rand\n\n\nInvestigating¬† pathways involved in the Nickel detoxification in Willow\nLiz Rylott\nTranscriptomics, Emma Rand\n\n\nInvestigating differential RNA expression through the Leishmania lifecycle\nPegine Walrad\nTranscriptomics, Emma Rand\n\n\nIdentifying novel proteins regulating synaptophagy\nRichard Maguire\nImage analysis, Richard Bingham\n\n\nDefining pathological cascades in dopaminergic neurons in a Parkinson‚Äôs model\nSean Sweeney\nImage analysis, Richard Bingham\n\n\nDiscovery proteins for biotech applications: new classes of antibody mimetics\nMichael Plevin\nStructure Analysis, Jon Agirre\n\n\n\nData Analysis compromises five workshops covering computational skills needed in your project. MSc Bioinformatics students do the Core workshops and the transcriptomics workshops as part of BIO00070M. The data analysis workshops are:\n\n\n\n\n\n\n\nWeek\nData Strand\n\n\n\n\n2\nCore 1 Supporting Information - reproducibility, project-oriented workflow, naming things, cool code, handy shortcuts\n\n\n3\nStrand specific 1\n\n\n4\nStrand specific 2\n\n\n5\nStrand specific 3\n\n\n6\nCore 2 Supporting Information - documenting with a README, curating code, non-coded processes\n\n\n\n\n\n\n\n\n\nStudents who successfully complete this module will be able to\n\nuse appropriate computational techniques to reproducibly process, analyse and visualise data and generate scientific reports based on project work.\n\n\n\n\nAll material is on the VLE so why is this site useful? This site collects everything together in a searchable way. The search icon is on the top right.\n\n\n\nRand E (2024). Data Analysis for Group Project. https://3mmarand.github.io/BIO00088H-data/.\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr [Xie (2024); knitr2; knitr3], kableExtra (Zhu 2021)\nReferences"
  },
  {
    "objectID": "index.html#module-learning-outcome-linked-to-this-content",
    "href": "index.html#module-learning-outcome-linked-to-this-content",
    "title": "Data Analysis for the Group Research Project",
    "section": "",
    "text": "Students who successfully complete this module will be able to\n\nuse appropriate computational techniques to reproducibly process, analyse and visualise data and generate scientific reports based on project work."
  },
  {
    "objectID": "index.html#what-is-this-site-for",
    "href": "index.html#what-is-this-site-for",
    "title": "Data Analysis for the Group Research Project",
    "section": "",
    "text": "All material is on the VLE so why is this site useful? This site collects everything together in a searchable way. The search icon is on the top right."
  },
  {
    "objectID": "index.html#please-cite-as",
    "href": "index.html#please-cite-as",
    "title": "Data Analysis for the Group Research Project",
    "section": "",
    "text": "Rand E (2024). Data Analysis for Group Project. https://3mmarand.github.io/BIO00088H-data/.\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr [Xie (2024); knitr2; knitr3], kableExtra (Zhu 2021)\nReferences"
  },
  {
    "objectID": "transcriptomics/week-5/overview.html",
    "href": "transcriptomics/week-5/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week we cover how to visualise the results of your differential expression analysis. The independent study will allow you to check you have what you should have following the Transcriptomics 2: Statistical Analysis workshop and Consolidation study. It will also summarise the the methods and plots we will go through in the workshop. It will also explain how to write the methods for the analyses with have conducted. In the workshop, we will learn how to carry out and plot a Principle Component Analysis (PCA) as well as how to create a nicely formatted Volcano plot.\nThe plots you have by the end of this week will be suitable for including in your report.\nWe suggest you sit together with your group in the workshop.\n\nLearning objectives\nThe successful student will be able to:\n\nverify they have the required RStudio Project set up and the data and code files from the previous Workshop and Consolidation study\nperform a PCA and understand how to interpret them\ncreate a volcano plot and understand how to interpret them\nwrite the methods for the analyses they have conducted\n\n\n\nInstructions\n\nPrepare\n\nüìñ Read what you should have so far\nüìñ Read about concepts in PCA and volcano plots\nüìñ Read about how to write the methods for the analyses you have conducted\n\nWorkshop\n\nüíª Perform and plot a PCA\nüíª Visualise all the results with a volcano plot\nüíª Look after future you!\n\nConsolidate\n\nüíª Use the work you completed in the workshop as a template to apply to a new case.\n\n\n\n\nReferences",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "About"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html",
    "href": "transcriptomics/week-5/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In the workshop, you will learn how to conduct and plot a Principle Component Analysis (PCA) as well as how to create a nicely formatted Volcano plot. You will also save significant genes to file to make it easier to identify genes of interest. and perform Gene Ontology (GO) term enrichment analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#session-overview",
    "href": "transcriptomics/week-5/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In the workshop, you will learn how to conduct and plot a Principle Component Analysis (PCA) as well as how to create a nicely formatted Volcano plot. You will also save significant genes to file to make it easier to identify genes of interest. and perform Gene Ontology (GO) term enrichment analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#frog-development",
    "href": "transcriptomics/week-5/workshop.html#frog-development",
    "title": "Workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\nüé¨ Open the frogs-88H RStudio Project and the cont-fgf-s30.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#arabidopsis",
    "href": "transcriptomics/week-5/workshop.html#arabidopsis",
    "title": "Workshop",
    "section": "üéÑ Arabidopsis\n",
    "text": "üéÑ Arabidopsis\n\nüé¨ Open the arabi-88H RStudio Project and the wildsuf-wilddef-s30.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#leishmania",
    "href": "transcriptomics/week-5/workshop.html#leishmania",
    "title": "Workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\nüé¨ Open the leish-88H RStudio Project and the pro-meta-s30.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#stem-cells",
    "href": "transcriptomics/week-5/workshop.html#stem-cells",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nüé¨ Open the mice-88H RStudio Project and the hspc-prog.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#everyone",
    "href": "transcriptomics/week-5/workshop.html#everyone",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nüé¨ Make a new folder figures in the project directory.\nThis is where we will save our figure files\nüé¨ Load tidyverse (Wickham et al. 2019) and conflicted (Wickham 2023). You most likely have this code at the top of your script already.\n\nlibrary(tidyverse)\nlibrary(conflicted)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.3     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.3     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package to force all conflicts to become errors\nI recommend you set the dplyr versions of filter() and select() to use by default\nüé¨ Use the dplyr version of filter() by default:\n\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(dplyr::select)",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#everyone-1",
    "href": "transcriptomics/week-5/workshop.html#everyone-1",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nüé¨ Import your results data. This should be a file in the results folder called xxxx_results.csv where xxxx indicates the comparison you made.\nüé¨ Remind yourself what is in the rows and columns and the structure of the dataframes (perhaps using glimpse())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we do PCA we will want to label the samples with their treatment for figures. For üê∏ Frog development, üéÑ Arabidopsis and üíâ Leishmania, this labelling information is most easily added using the metadata. You will need to filter for only the samples in the comparison that was made in the results file.\nYou may need to refer back to the Week 4 Statistical Analysis workshop (in section 2. Create DESeqDataSet object) to remind yourself how to import and filter the metadata you need.\nüé¨ Import the metadata that maps the sample names to treatments. Remember to select only the samples for comparison that was made.\nFor üê≠ Stem cells, cell types are encoded in the column names. We will do some regular expression magic to extract the cell type from the column names.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#frog-development-arabidopsis-and-leishmania",
    "href": "transcriptomics/week-5/workshop.html#frog-development-arabidopsis-and-leishmania",
    "title": "Workshop",
    "section": "üê∏ Frog development, üéÑ Arabidopsis and üíâ Leishmania\n",
    "text": "üê∏ Frog development, üéÑ Arabidopsis and üíâ Leishmania\n\nüé¨ Design the code to log2 transform the normalised counts using the template given\nI recommend viewing the dataframe to see the new columns. Check you have the expected number of columns.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#stem-cells-1",
    "href": "transcriptomics/week-5/workshop.html#stem-cells-1",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nYou do not need to apply this transformation because the data is already log2 transformed.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#everyone-2",
    "href": "transcriptomics/week-5/workshop.html#everyone-2",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nWe now all have dataframes with all the information we need: normalised counts, log2 normalised counts, statistical comparisons with fold changes and p-values, and information about the gene.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#everyone-3",
    "href": "transcriptomics/week-5/workshop.html#everyone-3",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nWe will create dataframe of the significant genes and write them to file. This is subset from the results file but will make it a little easier to examine and select genes of interest.\nThe general form of the code you need is:\n\n# DO NOT DO\n# create a dataframe of genes significant at 0.05 level\nxxxx_results_sig0.05 &lt;- xxxx_results |&gt; \n  filter(pvalue &lt;= 0.05)\n\nNote that you determine the significance level using the adjusted p-values rather than the uncorrected p-values. This column is name padj in DESeq2 output and FDR in scran output.\nüé¨ Create a dataframe of the genes significant at the 0.05 level using filter(). You will need to know the name of column with the adjusted p-values.\n‚ùìHow many genes are significant at the 0.05 levels?\n\n\n\n\n\n\n\n\n\n\nIf you have a very large number of genes significant at the 0.05 level, you may want to consider a more stringent cut-off such as 0.01.\nüé¨ Write the dataframe to a csv file. I recommend using the same file name as you used for the dataframe.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#frog-development-1",
    "href": "transcriptomics/week-5/workshop.html#frog-development-1",
    "title": "Workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\nüé¨ Transpose the log2 transformed normalised counts:\n\ns30_log2_trans &lt;- s30_results |&gt; \n  select(starts_with(\"log2_\")) |&gt;\n  t() |&gt; \n  data.frame()\n\nWe have used the select() function to select all the columns that start with log2_. We then use the t() function to transpose the dataframe. We then convert the resulting matrix to a dataframe using data.frame(). If you view that dataframe you‚Äôll see it has default column name which we can fix using colnames() to set the column names to the Xenbase gene ids.\nüé¨ Set the column names to the Xenbase gene ids:\n\ncolnames(s30_log2_trans) &lt;- s30_results$xenbase_gene_id\n\nüé¨ Perform PCA on the log2 transformed normalised counts:\n\npca &lt;- s30_log2_trans |&gt;\n  prcomp(rank. = 4) \n\nThe rank. argument tells prcomp() to only calculate the first 4 principal components. This is useful for visualisation as we can only plot in 2 or 3 dimensions. We can see the results of the PCA by viewing the summary() of the pca object.\n\nsummary(pca)\n\nImportance of first k=4 (out of 6) components:\n                           PC1     PC2     PC3     PC4\nStandard deviation     64.0124 47.3351 38.4706 31.4111\nProportion of Variance  0.4243  0.2320  0.1532  0.1022\nCumulative Proportion   0.4243  0.6562  0.8095  0.9116\n\n\nThe Proportion of Variance tells us how much of the variance is explained by each component. We can see that the first component explains 0.4243 of the variance, the second 0.2320, and the third 0.1532. Together the first three components explain nearly 81% of the total variance in the data. Plotting PC1 against PC2 will capture about 66% of the variance which is likely very much better than we would get plotting any two genes against each other. To plot the PC1 against PC2 we will need to extract the PC1 and PC2 ‚Äúscores‚Äù from the PCA object and add labels for the samples. Those labels will come from the row names of the transformed data which has the sample ids and from the metadata.\nüé¨ Create a vector of the sample ids from the row names. These include the log2 prefix which we can removed for labelling:\n\nsample_id &lt;- row.names(s30_log2_trans) |&gt; str_remove(\"log2_\")\n\nYou might want to check the result.\nNow we will extract the PC1 and PC2 scores from the PCA object and add. Our PCA object is called pca and the scores are in pca$x. We will create a dataframe of the scores and add the sample ids.\nüé¨ Create a dataframe of PC1 and PC2 scores and add the sample ids:\n\npca_labelled &lt;- data.frame(pca$x,\n                           sample_id)\n\nüé¨ Merge with the metadata so we can label points by treatment and sibling pair:\n\npca_labelled &lt;- pca_labelled |&gt; \n  left_join(meta_s30, \n            by = \"sample_id\")\n\nSince the metadata contained the sample ids, it was especially important to remove the log2_ from the row names so that the join would work.\nThe dataframe should look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nsample_id\nstage\ntreatment\nsibling_rep\n\n\n\n-76.38391\n0.814699\n-60.728327\n-5.820669\nS30_C_1\nstage_30\ncontrol\none\n\n\n-67.02571\n25.668563\n51.476835\n28.480254\nS30_C_2\nstage_30\ncontrol\ntwo\n\n\n-14.02772\n-78.474054\n15.282058\n-9.213076\nS30_C_3\nstage_30\ncontrol\nthree\n\n\n47.60726\n49.035510\n-19.288753\n20.928290\nS30_F_1\nstage_30\nFGF\none\n\n\n26.04954\n32.914201\n20.206072\n-55.752818\nS30_F_2\nstage_30\nFGF\ntwo\n\n\n83.78054\n-29.958919\n-6.947884\n21.378020\nS30_F_3\nstage_30\nFGF\nthree\n\n\n\n\n\nThe next task is to plot PC2 against PC1 and colour by sibling pair. This is just a scatterplot so we can use geom_point(). We will use colour to indicate the sibling pair and shape to indicate the treatment.\nüé¨ Customise the PC2 against PC1 plot:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = sibling_rep,\n             shape = treatment)) +\n  geom_point(size = 3) +\n  theme_classic()\n\n\n\n\n\n\n\nThere is a good separation between treatments on PCA1. The sibling pairs do not seem to cluster together. You can also try plotting PC3 or PC4.\nI prefer to customise the colours and shapes. I especially like the\nviridis colour scales which provide colour scales that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness. See Introduction to viridis for more information.\nggplot provides functions to access the viridis scales. Here I use scale_fill_viridis_d(). The d stands for discrete. The function scale_fill_viridis_c() would be used for continuous data. I‚Äôve used the default ‚Äúviridis‚Äù (or ‚ÄúD‚Äù) option (do ?scale_fill_viridis_d for all the options) and used the begin and end arguments to control the range of colour - I have set the range to be from 0.15 to 0.95 the avoid the strongest contrast. I have also set the name argument to provide a label for the legend.\nI have used scale_shape_manual() to set the shapes for the treatments. I have used the values 21 and 19 which are the codes for filled and open circles and filled triangles. I have set the name argument to NULL to remove the label (it‚Äôs obvious what that categories are treatments) and the labels argument to improve the legend.\nüé¨ Plot PC2 against PC1 and colour by sibling pair and shape by treatment:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = sibling_rep,\n             shape = treatment)) +\n  geom_point(size = 3) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = \"Sibling pair\") +\n  scale_shape_manual(values = c(21, 19),\n                     name = NULL,\n                     labels = c(\"Control\", \"FGF-Treated\")) +\n  theme_classic()\n\n\n\n\n\n\n\nNow go to Volcano plots for üê∏ Frog development",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#arabidopsis-1",
    "href": "transcriptomics/week-5/workshop.html#arabidopsis-1",
    "title": "Workshop",
    "section": "üéÑ Arabidopsis\n",
    "text": "üéÑ Arabidopsis\n\nüé¨ Transpose the log2 transformed normalised counts:\n\nwild_log2_trans &lt;- wild_results |&gt; \n  select(starts_with(\"log2_\")) |&gt;\n  t() |&gt; \n  data.frame()\n\nWe have used the select() function to select all the columns that start with log2_. We then use the t() function to transpose the dataframe. We then convert the resulting matrix to a dataframe using data.frame(). If you view that dataframe you‚Äôll see it has default column name which we can fix using colnames() to set the column names to the gene ids.\nüé¨ Set the column names to the gene ids:\n\ncolnames(wild_log2_trans) &lt;- wild_results$gene_id\n\nüé¨ Perform PCA on the log2 transformed normalised counts:\n\npca &lt;- wild_log2_trans |&gt;\n  prcomp(rank. = 4, scale = TRUE)\n\nThe scale argument tells prcomp() to scale the data before doing the PCA. This is important when the variables are on different scales to stop variables with large values dominating the PCA. The rank. argument tells prcomp() to only calculate the first 4 principal components. This is useful for visualisation as we can only plot in 2 or 3 dimensions. We can see the results of the PCA by viewing the summary() of the pca object.\n\nsummary(pca)\n\nImportance of components:\n                            PC1     PC2     PC3       PC4\nStandard deviation     123.0171 85.6822 62.2924 2.185e-13\nProportion of Variance   0.5742  0.2786  0.1472 0.000e+00\nCumulative Proportion    0.5742  0.8528  1.0000 1.000e+00\n\n\nThe Proportion of Variance tells us how much of the variance is explained by each component. We can see that the first component explains 0.5742 of the variance, the second 0.2786, and the third 0.1472. Together the first three components explain nearly 100% of the total variance in the data. Plotting PC1 against PC2 will capture about 92% of the variance which is very likely very much better than we would get plotting any two genes against each other. To plot the PC1 against PC2 we will need to extract the PC1 and PC2 ‚Äúscores‚Äù from the PCA object and add labels for the samples. Those labels will come from the row names of the transformed data which has the sample ids and from the metadata.\nüé¨ Create a vector of the sample ids from the row names. These include the log2 prefix which we can removed for labelling:\n\nsample_id &lt;- row.names(wild_log2_trans) |&gt; str_remove(\"log2_\")\n\nYou might want to check the result.\nNow we will extract the PC1 and PC2 scores from the PCA object and add. Our PCA object is called pca and the scores are in pca$x. We will create a dataframe of the scores and add the sample ids.\nüé¨ Create a dataframe of PC1 and PC2 scores and add the sample ids:\n\npca_labelled &lt;- data.frame(pca$x,\n                           sample_id)\n\nüé¨ Merge with the metadata so we can label points by treatment and sibling pair:\n\npca_labelled &lt;- pca_labelled |&gt; \n  left_join(meta_wild, \n            by = \"sample_id\")\n\nSince the metadata contained the sample ids, it was especially important to remove the log2_ from the row names so that the join would work.\nThe dataframe should look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nsample_id\ngenotype\ncopper\n\n\n\n-91.23038\n105.92675\n25.80774\n0\nSRX028956_wild_suf\nwt\nsufficient\n\n\n-120.00085\n-95.79115\n-13.72618\n0\nSRX028957_wild_def\nwt\ndeficient\n\n\n92.06667\n23.00841\n-79.23090\n0\nSRX028960_wild_suf\nwt\nsufficient\n\n\n119.16456\n-33.14401\n67.14935\n0\nSRX028961_wild_def\nwt\ndeficient\n\n\n\n\n\nThe next task is to plot PC2 against PC1 and colour by copper conditions. This is just a scatterplot so we can use geom_point(). We will use colour to indicate the copper conditions.\nüé¨ Plot PC2 against PC1 and colour by copper conditions:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC3, y = PC2, \n             colour = copper)) +\n  geom_point(size = 3) +\n  theme_classic()\n\n\n\n\n\n\n\nWe do not see particularly good separation between treatments, though perhaps there is some separation between the copper sufficient and copper deficient on PC2. It is also difficult to see if the replicates cluster together (the treatments separate) when there are only two reps. You can also try plotting PC3 or PC4.\nI prefer to customise the colours and shapes. I especially like the\nviridis colour scales which provide colour scales that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness. See Introduction to viridis for more information.\nggplot provides functions to access the viridis scales. Here I use scale_fill_viridis_d(). The d stands for discrete. The function scale_fill_viridis_c() would be used for continuous data. I‚Äôve used the default ‚Äúviridis‚Äù (or ‚ÄúD‚Äù) option (do ?scale_fill_viridis_d for all the options) and used the begin and end arguments to control the range of colour - I have set the range to be from 0.15 to 0.95 the avoid the strongest contrast. I have also set the name argument to provide a label for the legend.\nüé¨ Customise the PC2 against PC1 plot:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = copper)) +\n  geom_point(size = 3) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = \"Copper\") +\n  theme_classic()\n\n\n\n\n\n\n\nNow go to Volcano plots for üéÑ Arabidopsis",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#leishmania-1",
    "href": "transcriptomics/week-5/workshop.html#leishmania-1",
    "title": "Workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\nüé¨ Transpose the log2 transformed normalised counts:\n\npro_meta_log2_trans &lt;- pro_meta_results |&gt; \n  select(starts_with(\"log2_\")) |&gt;\n  t() |&gt; \n  data.frame()\n\nWe have used the select() function to select all the columns that start with log2_. We then use the t() function to transpose the dataframe. We then convert the resulting matrix to a dataframe using data.frame(). If you view that dataframe you‚Äôll see it has default column name which we can fix using colnames() to set the column names to the gene ids.\nüé¨ Set the column names to the gene ids:\n\ncolnames(pro_meta_log2_trans) &lt;- pro_meta_results$gene_id\n\nüé¨ Perform PCA on the log2 transformed normalised counts:\n\npca &lt;- pro_meta_log2_trans |&gt;\n  prcomp(rank. = 4, scale = TRUE) \n\nThe scale argument tells prcomp() to scale the data before doing the PCA. This is important when the variables are on different scales to stop variables with large values dominating the PCA. The rank. argument tells prcomp() to only calculate the first 4 principal components. This is useful for visualisation as we can only plot in 2 or 3 dimensions. We can see the results of the PCA by viewing the summary() of the pca object.\n\nsummary(pca)\n\nImportance of first k=4 (out of 6) components:\n                           PC1     PC2     PC3      PC4\nStandard deviation     66.0744 41.9741 29.4077 27.61377\nProportion of Variance  0.5175  0.2088  0.1025  0.09038\nCumulative Proportion   0.5175  0.7263  0.8288  0.91916\n\n\nThe Proportion of Variance tells us how much of the variance is explained by each component. We can see that the first component explains 0.5175 of the variance, the second 0.2088, and the third 0.1025. Together the first three components explain nearly 92% of the total variance in the data. Plotting PC1 against PC2 will capture about 66% of the variance which is likely very much better than we would get plotting any two genes against each other. To plot the PC1 against PC2 we will need to extract the PC1 and PC2 ‚Äúscores‚Äù from the PCA object and add labels for the samples. Those labels will come from the row names of the transformed data which has the sample ids and from the metadata.\nüé¨ Create a vector of the sample ids from the row names. These include the log2 prefix which we can removed for labelling:\n\nsample_id &lt;- row.names(pro_meta_log2_trans) |&gt; str_remove(\"log2_\")\n\nYou might want to check the result.\nNow we will extract the PC1 and PC2 scores from the PCA object and add. Our PCA object is called pca and the scores are in pca$x. We will create a dataframe of the scores and add the sample ids.\nüé¨ Create a dataframe of PC1 and PC2 scores and add the sample ids:\n\npca_labelled &lt;- data.frame(pca$x,\n                           sample_id)\n\nüé¨ Merge with the metadata so we can label points by life cycle stage:\n\npca_labelled &lt;- pca_labelled |&gt; \n  left_join(meta_pro_meta, \n            by = \"sample_id\")\n\nSince the metadata contained the sample ids, it was especially important to remove the log2_ from the row names so that the join would work.\nThe dataframe should look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nsample_id\nstage\nreplicate\n\n\n\n-59.42107\n-2.922229\n-11.043368\n-13.4641060\nlm_pro_1\nprocyclic\n1\n\n\n-63.25818\n-15.985856\n-36.248191\n0.7025242\nlm_pro_2\nprocyclic\n2\n\n\n-57.92065\n25.679645\n47.373874\n13.7893070\nlm_pro_3\nprocyclic\n3\n\n\n56.20326\n-59.707787\n5.958727\n31.5528028\nlm_meta_1\nmetacyclic\n1\n\n\n57.41205\n-11.738157\n14.157242\n-47.2285609\nlm_meta_2\nmetacyclic\n2\n\n\n66.98459\n64.674383\n-20.198284\n14.6480329\nlm_meta_3\nmetacyclic\n3\n\n\n\n\n\nThe next task is to plot PC2 against PC1 and colour by sibling pair. This is just a scatterplot so we can use geom_point(). We will use colour to indicate the life cycle stage.\nüé¨ Plot PC2 against PC1 and colour by copper conditions:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = stage)) +\n  geom_point(size = 3) +\n  theme_classic()\n\n\n\n\n\n\n\nThere is a good separation between treatments on PCA1. The replicates do seem to cluster together. You can also try plotting PC3 or PC4.\nI prefer to customise the colours. I especially like the\nviridis colour scales which provide colour scales that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness. See Introduction to viridis for more information.\nggplot provides functions to access the viridis scales. Here I use scale_fill_viridis_d(). The d stands for discrete. The function scale_fill_viridis_c() would be used for continuous data. I‚Äôve used the default ‚Äúviridis‚Äù (or ‚ÄúD‚Äù) option (do ?scale_fill_viridis_d for all the options) and used the begin and end arguments to control the range of colour - I have set the range to be from 0.15 to 0.95 the avoid the strongest contrast. I have also set the name argument to provide a label for the legend.\nüé¨ Plot PC2 against PC1 and colour by life stage:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = stage)) +\n  geom_point(size = 3) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = \"Stage\") +\n  theme_classic()\n\n\n\n\n\n\n\nNow go to Volcano plots for üíâ Leishmania",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#stem-cells-2",
    "href": "transcriptomics/week-5/workshop.html#stem-cells-2",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nüé¨ Transpose the log2 transformed normalised counts:\n\nhspc_prog_trans &lt;- hspc_prog_results |&gt; \n  dplyr::select(starts_with(c(\"HSPC_\", \"Prog_\"))) |&gt;\n  t() |&gt; \n  data.frame()\n\nWe have used the select() function to select all the columns that start with Prog_ or HSPC_. We then use the t() function to transpose the dataframe. We then convert the resulting matrix to a dataframe using data.frame(). If you view that dataframe you‚Äôll see it has default column name which we can fix using colnames() to set the column names to the gene ids.\nüé¨ Set the column names to the gene ids:\n\ncolnames(hspc_prog_trans) &lt;- hspc_prog_results$ensembl_gene_id\n\nüé¨ Perform PCA on the log2 transformed normalised counts:\n\npca &lt;- hspc_prog_trans |&gt;\n  prcomp(rank. = 8)\n\nThe rank. argument tells prcomp() to only calculate the first 8 principal components. This is useful for visualisation as we can only plot in 2 or 3 dimensions. We can see the results of the PCA by viewing the summary() of the pca object.\n\nsummary(pca)\n\nImportance of first k=8 (out of 280) components:\n                           PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     12.5612 8.36646 5.98988 5.41386 4.55730 4.06142 3.84444\nProportion of Variance  0.1099 0.04874 0.02498 0.02041 0.01446 0.01149 0.01029\nCumulative Proportion   0.1099 0.15861 0.18359 0.20400 0.21846 0.22995 0.24024\n                           PC8\nStandard deviation     3.70848\nProportion of Variance 0.00958\nCumulative Proportion  0.24982\n\n\nThe Proportion of Variance tells us how much of the variance is explained by each component. We can see that the first component explains 0.1099 of the variance, the second 0.04874, and the third 0.02498. Together the first three components explain 18% of the total variance in the data. This is not that high but it is also quite a lot better than than we would get plotting any two genes randomly chosen against each other.\nTo plot the PC1 against PC2 we will need to extract the PC1 and PC2 ‚Äúscores‚Äù from the PCA object and add labels for the cells. Our PCA object is called pca and the scores are in pca$x. The cells labels will come from the row names of the transformed data.\nüé¨ Create a dataframe of the PC1 and PC2 scores (in pca$x) and add the cell ids:\n\npca_labelled &lt;- data.frame(pca$x,\n                           cell_id = row.names(hspc_prog_trans))\n\nIt will be helpful to add a column for the cell type so we can label points. One way to do this is to extract the information in the cell_id column into two columns: one with the complete cell id and one with just the cell type. This extraction is done with Regular Expression magic.\nüé¨ Extract the cell type and cell number from the cell_id column (keeping the cell_id column):\n\npca_labelled &lt;- pca_labelled |&gt; \n  extract(cell_id, \n          remove = FALSE,\n          c(\"cell_type\", \"cell_number\"),\n          \"([a-zA-Z]{4})_([0-9]{3})\")\n\nWhat this code does is take what is in the cell_id column (something like Prog_001 or HSPC_001) and split it into two columns (‚Äúcell_type‚Äù and ‚Äúcell_number‚Äù). The reason why we want to do that is to colour the points by cell type. We would not want to use cell_id to colour the points because each cell id is unique and that would be 1000+ colours. The last argument in the extract() function is the pattern to match described with a regular expression. Three patterns are being matched, and two of those are in brackets meaning they are kept to fill the two new columns.\nThe first pattern is ([a-zA-Z]{4})\n\nit is brackets because we want to keep it and put it in cell_type\n\n\n[a-zA-Z] means any lower (a-z) or upper case letter (A-Z).\nThe square brackets means any of the characters in the square brackets will be matched\n\n{4} means 4 of them.\n\nSo the first pattern inside the first (‚Ä¶) will match exactly 4 upper or lower case letters (like Prog or HSPC)\nThe second pattern is _ to match the underscore in every cell id that separates the cell type from the number. It is not in brackets because we do not want to keep it.\nThe third pattern is ([0-9]{3})\n\n\n[0-9] means any number\n\n{3} means 3 of them.\n\nSo the second pattern inside the second (‚Ä¶) will match exactly 3 numbers (like 001 or 851).\nImportant: Prog and HPSC have 4 letters. The column names, LT.HSPC_ have 6 characters and includes a dot. You will need to adjust the regex when make comparison between LT-HSC and other cell types. The pattern to match the LT.HSC as well as the Prog and HSPC is ([a-zA-Z.]{4, 6}). Note the dot inside the square brackets and numbers meaning 4 or 6 of. The pattern to match the underscore and the cell number is the same.\nThe top of the dataframe should look like this (but with more decimal places)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\ncell_id\ncell_type\ncell_number\n\n\n\nHSPC_001\n-2.62\n-15.39\n-7.94\n-4.87\n-2.52\n4.29\n-1.69\n4.45\nHSPC_001\nHSPC\n001\n\n\nHSPC_002\n0.70\n-15.22\n0.13\n3.74\n-6.47\n-1.70\n0.83\n-3.35\nHSPC_002\nHSPC\n002\n\n\nHSPC_003\n-15.52\n7.80\n0.53\n-4.04\n0.10\n-1.48\n-0.32\n-7.40\nHSPC_003\nHSPC\n003\n\n\nHSPC_004\n-4.11\n-7.04\n0.45\n10.58\n-3.09\n-8.30\n-6.01\n2.27\nHSPC_004\nHSPC\n004\n\n\nHSPC_006\n-7.89\n5.15\n-2.95\n12.45\n4.77\n4.63\n5.46\n1.97\nHSPC_006\nHSPC\n006\n\n\nHSPC_008\n-10.29\n-8.06\n-8.33\n-2.69\n1.86\n-2.42\n0.03\n-2.01\nHSPC_008\nHSPC\n008\n\n\n\n\n\nThe next task is to plot PC2 against PC1 and colour by cell type. This is just a scatterplot so we can use geom_point(). We will use colour to indicate the cell type.\nüé¨ Plot PC2 against PC1 and colour by cell type:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = cell_type)) +\n  geom_point(alpha = 0.4) +\n  theme_classic()\n\n\n\n\n\n\n\nThere is a good clustering of cell types but plenty of overlap. You can also try plotting PC3 or PC4 (or others).\nI prefer to customise the colours. I especially like the viridis colour scales which provide colour scales that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness. See Introduction to viridis for more information.\nggplot provides functions to access the viridis scales. Here I use scale_fill_viridis_d(). The d stands for discrete used because cell type is a discrete variable. The function scale_fill_viridis_c() would be used for continuous data. I‚Äôve used the default ‚Äúviridis‚Äù (or ‚ÄúD‚Äù) option (do ?scale_fill_viridis_d for all the options) and used the begin and end arguments to control the range of colour - I have set the range to be from 0.15 to 0.95 the avoid the strongest contrast. I have also set the name argument to NULL because that the legend refers to cell types is obvious.\nüé¨ Plot PC2 against PC1 and colour by cell type:\n\npca_labelled |&gt; \n  ggplot(aes(x = PC1, y = PC2, \n             colour = cell_type)) +\n  geom_point(alpha = 0.4) +\n  scale_colour_viridis_d(end = 0.95, begin = 0.15,\n                         name = NULL) +\n  theme_classic()\n\n\n\n\n\n\n\nNow go to Volcano plots for üê≠ Stem cells",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#frog-development-2",
    "href": "transcriptomics/week-5/workshop.html#frog-development-2",
    "title": "Workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\nWe will add a column to the results dataframe that contains the -log10(padj). You could perform this transformation within the plot command without adding a column to the data if you prefer.\nüé¨ Add a column to the results dataframe that contains the -log10(padj):\n\ns30_results &lt;- s30_results |&gt; \n  mutate(log10_padj = -log10(padj)) \n\nüé¨ Create a volcano plot of the results:\n\ns30_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj)) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nOur dashed lines are at -log10(0.05) and log2(2) and log2(-2) to make more clear which genes (points) are significantly different between the control and the FGF-treated samples and have a fold change of at least 2.\nIn most cases, people colour the points to show that the quadrants. I like to add columns to the dataframe to indicate if the gene is significant and if the fold change is large and use those variables in the plot.\nüé¨ Add columns to the results dataframe to indicate if the gene is significant and if the fold change is large:\n\ns30_results &lt;- s30_results |&gt; \n  mutate(sig = padj &lt;= 0.05,\n         bigfc = abs(log2FoldChange) &gt;= 2) \n\nThe use of abs() (absolute) means genes with a fold change of at least 2 in either direction will be considered to have a large fold change.\nNow we can colour the points by these new columns. I use interaction() to create four categories:\n\nnot significant and not large fold change (FF)\nsignificant and not large fold change (TF)\nnot significant and large fold (FT)\nsignificant and large fold change (TT)\n\nAnd I use scale_colour_manual() to set the colours for these categories.\nüé¨ Create a volcano plot of the results with the points coloured by significance and fold change:\n\ns30_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFor exploring the data, I like add labels to all the significant genes with a large fold change so I can very quickly identity them. The ggrepel package has a function geom_text_repel() that is useful for adding labels so that they don‚Äôt overlap.\nüé¨ Load the package:\n\nlibrary(ggrepel)\n\nüé¨ Add labels to the significant genes with a large fold change:\n\ns30_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_text_repel(data = s30_results |&gt; \n                    filter(bigfc == TRUE, sig == TRUE),\n                  aes(label = xenbase_gene_symbol),\n                  size = 3,\n                  max.overlaps = 50) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nNotice that I have used filter() label only the genes that are both significant and have a large fold change. In systems you are familiar with, this labelling is very informative and can help you quickly identify common themes. Key to interpreting the volcano plot is to remember that positive fold changes means the gene is up-regulated in the FGF-treated samples and negative fold changes means the gene is down-regulated (i.e., higher in the control). This was determined by the order of the treatments in the contrast used in the DESeq2 analysis\nIf you do forget which way round you did the comparison, you can always examine the results dataframe to see which of the treatments seem to be higher for the positive fold changes.\nPlease note that Betsy doesn‚Äôt like graphs like this in the report!\nWhen you have a gene of interest, you may wish to label it, and only it, on the plot.\nThis is done in the same way except that you filter the data to only include the gene of interest. I have used and then use geom_label_repel() rather than geom_text_repel() to put the label in a box and nudged it‚Äôs position to get a line connecting the point and the label. I have also increased the size of the point.\nüé¨ Add a label to one gene of interest (hoxb9.S) and :\n\ns30_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_label_repel(data = s30_results |&gt; \n                    filter(xenbase_gene_symbol == \"hoxb9.S\"),\n                  aes(label = xenbase_gene_symbol),\n                  size = 4,\n                  nudge_x = .5,\n                  nudge_y = 1.5) +\n  geom_point(data = s30_results |&gt; \n                    filter(xenbase_gene_symbol == \"hoxb9.S\"),\n                size = 3) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nShould you want to label more than one gene, you will need to use (for example): filter(xenbase_gene_symbol %in% c(\"hoxb9.S\", \"fzd7.S\"))\nNow go to Save your plots",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#arabidopsis-2",
    "href": "transcriptomics/week-5/workshop.html#arabidopsis-2",
    "title": "Workshop",
    "section": "üéÑ Arabidopsis\n",
    "text": "üéÑ Arabidopsis\n\nWe will add a column to the results dataframe that contains the -log10(padj). You could perform this transformation within the plot command without adding a column to the data if you prefer.\nüé¨ Add a column to the results dataframe that contains the -log10(padj):\n\nwild_results &lt;- wild_results |&gt; \n  mutate(log10_padj = -log10(padj)) \n\nüé¨ Create a volcano plot of the results:\n\nwild_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj)) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nOur dashed lines are at -log10(0.05) and log2(2) and log2(-2) to make more clear which genes (points) are significantly different between the copper sufficient and deficient conditions samples and have a fold change of at least 2.\nIn most cases, people colour the points to show that the quadrants. I like to add columns to the dataframe to indicate if the gene is significant and if the fold change is large and use those variables in the plot.\nüé¨ Add columns to the results dataframe to indicate if the gene is significant and if the fold change is large:\n\nwild_results &lt;- wild_results |&gt; \n  mutate(sig = padj &lt;= 0.05,\n         bigfc = abs(log2FoldChange) &gt;= 2) \n\nThe use of abs() (absolute) means genes with a fold change of at least 2 in either direction will be considered to have a large fold change.\nNow we can colour the points by these new columns. I use interaction() to create four categories:\n\nnot significant and not large fold change (FF)\nsignificant and not large fold change (TF)\nnot significant and large fold (FT)\nsignificant and large fold change (TT)\n\nAnd I use scale_colour_manual() to set the colours for these categories.\nNOTE: there are no ‚Äúsignificant and not large fold change (TF)‚Äù in this case.This means we do not need four colours. I have put the ‚Äúpink‚Äù colour in the code but commented it out.\nüé¨ Create a volcano plot of the results with the points coloured by significance and fold change:\n\nwild_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                # \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFor exploring the data, I like add labels to all the significant genes with a large fold change so I can very quickly identity them. The ggrepel package has a function geom_text_repel() that is useful for adding labels so that they don‚Äôt overlap.\nüé¨ Load the package:\n\nlibrary(ggrepel)\n\nüé¨ Add labels to the significant genes with a large fold change:\n\nwild_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                               #  \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_text_repel(data = wild_results |&gt; \n                    filter(bigfc == TRUE, sig == TRUE),\n                  aes(label = external_gene_name),\n                  size = 3,\n                  max.overlaps = 50) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nNotice that I have used filter() label only the genes that are both significant and have a large fold change. In systems you are familiar with, this labelling is very informative and can help you quickly identify common themes. Key to interpreting the volcano plot is to remember that positive fold changes means the gene is up-regulated in the sufficient conditions and negative fold changes means the gene is down-regulated (i.e., higher in the deficient). This was determined by the order of the treatments in the contrast used in the DESeq2 analysis\nIf you do forget which way round you did the comparison, you can always examine the results dataframe to see which of the treatments seem to be higher for the positive fold changes.\nWhen you have a gene of interest, you may wish to label it, and only it, on the plot. This is done in the same way except that you filter the data to only include the gene of interest. I have used and then use geom_label_repel() rather than geom_text_repel() to put the label in a box and nudged it‚Äôs position to get a line connecting the point and the label. I have also increased the size of the point.\nüé¨ Add a label to one gene of interest (hoxb9.S) and :\n\nwild_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                               #  \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_label_repel(data = wild_results |&gt; \n                    filter(external_gene_name == \"FRO4\"),\n                  aes(label = external_gene_name),\n                  size = 4,\n                  nudge_x = .5,\n                  nudge_y = 1.5) +\n  geom_point(data = wild_results |&gt; \n                    filter(external_gene_name == \"FRO4\"),\n                size = 3) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nShould you want to label more than one gene, you will need to use (for example): filter(external_gene_name %in% c(\"FRO4\", \"FRO5\"))\nNow go to Save your plots",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#leishmania-2",
    "href": "transcriptomics/week-5/workshop.html#leishmania-2",
    "title": "Workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\nWe will add a column to the results dataframe that contains the -log10(padj). You could perform this transformation within the plot command without adding a column to the data if you prefer.\nüé¨ Add a column to the results dataframe that contains the -log10(padj):\n\npro_meta_results &lt;- pro_meta_results |&gt; \n  mutate(log10_padj = -log10(padj)) \n\nüé¨ Create a volcano plot of the results:\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj)) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nOur dashed lines are at -log10(0.05) and log2(2) and log2(-2) to make more clear which genes (points) are significantly different between the life stages and have a fold change of at least 2. Whilst we have very many genes that are significant, we have fewer that are significant and have a large fold change.\nIn most cases, people colour the points to show that the quadrants. I like to add columns to the dataframe to indicate if the gene is significant and if the fold change is large and use those variables in the plot.\nüé¨ Add columns to the results dataframe to indicate if the gene is significant and if the fold change is large:\n\npro_meta_results &lt;- pro_meta_results |&gt; \n  mutate(sig = padj &lt;= 0.05,\n         bigfc = abs(log2FoldChange) &gt;= 2) \n\nThe use of abs() (absolute) means genes with a fold change of at least 2 in either direction will be considered to have a large fold change.\nNow we can colour the points by these new columns. I use interaction() to create four categories:\n\nnot significant and not large fold change (FF)\nsignificant and not large fold change (TF)\nnot significant and large fold (FT)\nsignificant and large fold change (TT)\n\nAnd I use scale_colour_manual() to set the colours for these categories.\nüé¨ Create a volcano plot of the results with the points coloured by significance and fold change:\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFor exploring the data, I like add labels to all the significant genes with a large fold change so I can very quickly identity them. The ggrepel package has a function geom_text_repel() that is useful for adding labels so that they don‚Äôt overlap.\nüé¨ Load the package:\n\nlibrary(ggrepel)\n\nüé¨ Add labels to the significant genes with a large fold change:\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_text_repel(data = pro_meta_results |&gt; \n                    filter(bigfc == TRUE, sig == TRUE),\n                  aes(label = description),\n                  size = 3,\n                  max.overlaps = 50) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nNotice that I have used filter() label only the genes that are both significant and have a large fold change. In systems you are familiar with, this labelling is very informative and can help you quickly identify common themes. However, in this case, we do not have good annotation for the genes. We can label only with the gene id or the description. Many of the descriptions are\nüé¨ Add labels to the significant genes with a large fold change only where description doesn‚Äôt contain ‚Äúhypothetical‚Äù or ‚Äúunspecified :\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_text_repel(data = pro_meta_results |&gt; \n                    filter(bigfc == TRUE, sig == TRUE,\n                           !str_detect(description, \"hypothetical|unspecified\")),\n                  aes(label = description),\n                  size = 3,\n                  max.overlaps = 50) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nKey to interpreting the volcano plot is to remember that positive fold changes means the gene is up-regulated in the procyclic stage and negative fold changes means the gene is down-regulated (i.e., higher in the metacyclic). This was determined by the order of the treatments in the contrast used in the DESeq2 analysis\nIf you do forget which way round you did the comparison, you can always examine the results dataframe to see which of the treatments seem to be higher for the positive fold changes.\nWhen you have a gene of interest, you may wish to label it, and only it, on the plot. This is done in the same way except that you filter the data to only include the gene of interest. I have used and then use geom_label_repel() rather than geom_text_repel() to put the label in a box and nudged it‚Äôs position to get a line connecting the point and the label. I have also increased the size of the point.\nüé¨ Add a label to one gene of interest (elongation factor 1-alpha) and :\n\npro_meta_results |&gt; \n  ggplot(aes(x = log2FoldChange, \n             y = log10_padj, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                 \"gray30\",\n                                 \"deeppink\")) +\n  geom_label_repel(data = pro_meta_results |&gt; \n                    filter(description == \"elongation factor 1-alpha\"),\n                  aes(label = description),\n                  size = 4,\n                  nudge_x = .5,\n                  nudge_y = 1.5) +\n  geom_point(data = pro_meta_results |&gt; \n                    filter(description == \"elongation factor 1-alpha\"),\n                size = 3) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nShould you want to label more than one gene, you will need to use (for example): filter(description %in% c(\"elongation factor 1-alpha\", \"ADP/ATP translocase 1 putative\"))\nNow go to Save your plots",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/workshop.html#stem-cells-3",
    "href": "transcriptomics/week-5/workshop.html#stem-cells-3",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nWe will add a column to the results dataframe that contains the -log10(FDR). You could perform this transformation within the plot command without adding a column to the data if you prefer.\nüé¨ Add a column to the results dataframe that contains the -log10(FDR):\n\nhspc_prog_results &lt;- hspc_prog_results |&gt; \n  mutate(log10_FDR = -log10(FDR)) \n\nüé¨ Create a volcano plot of the results:\n\nhspc_prog_results |&gt; \n  ggplot(aes(x = summary.logFC, \n             y = log10_FDR)) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nOur dashed lines are at -log10(0.05) and log2(2) and log2(-2) to make more clear which genes (points) are significantly different between the cell types and have a fold change of at least 2.\nIn most cases, people colour the points to show that the quadrants. I like to add columns to the dataframe to indicate if the gene is significant and if the fold change is large and use those variables in the plot.\nüé¨ Add columns to the results dataframe to indicate if the gene is significant and if the fold change is large:\n\nhspc_prog_results &lt;- hspc_prog_results |&gt; \n  mutate(sig = FDR &lt;= 0.05,\n         bigfc = abs(summary.logFC) &gt;= 2) \n\nThe use of abs() (absolute) means genes with a fold change of at least 2 in either direction will be considered to have a large fold change.\nNow we can colour the points by these new columns. I use interaction() to create four categories:\n\nnot significant and not large fold change (FF)\nsignificant and not large fold change (TF)\nnot significant and large fold (FT)\nsignificant and large fold change (TT)\n\nAnd I use scale_colour_manual() to set the colours for these categories.\nNOTE: there are no ‚Äúnot significant and large fold change (FT)‚Äù in this case. This means we do not need four colours. I have put the ‚Äúgray30‚Äù colour in the code but commented it out.\nüé¨ Create a volcano plot of the results with the points coloured by significance and fold change:\n\nhspc_prog_results |&gt; \n  ggplot(aes(x = summary.logFC, \n             y = log10_FDR, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                # \"gray30\",\n                                 \"deeppink\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFor exploring the data, I like add labels to all the significant genes with a large fold change so I can very quickly identity them. The ggrepel package has a function geom_text_repel() that is useful for adding labels so that they don‚Äôt overlap.\nüé¨ Load the package:\n\nlibrary(ggrepel)\n\nüé¨ Add labels to the significant genes with a large fold change:\n\nhspc_prog_results |&gt; \n  ggplot(aes(x = summary.logFC, \n             y = log10_FDR, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                              #   \"gray30\",\n                                 \"deeppink\")) +\n  geom_text_repel(data = hspc_prog_results |&gt; \n                    filter(bigfc == TRUE, sig == TRUE),\n                  aes(label = external_gene_name),\n                  size = 3,\n                  max.overlaps = 50) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nNotice that I have used filter() label only the genes that are both significant and have a large fold change. In systems you are familiar with, this labelling is very informative and can help you quickly identify common themes. Key to interpreting the volcano plot is to remember that positive fold changes means the gene is up-regulated in the Prog and negative fold changes means the gene is down-regulated (i.e., higher HSPC). This was determined by us choosing the results_hspc_prog$prog dataframe from the list object\nIf you do forget which way round you did the comparison, you can always examine the gene summary dataframe to see which of the treatments seem to be higher for the positive fold changes.\nWhen you have a gene of interest, you may wish to label it on the plot. This is done in the same way except that you filter the data to only include the gene of interest. I have used and then use geom_label_repel() rather than geom_text_repel() to put the label in a box and nudged it‚Äôs position to get a line connecting the point and the label. I have also increased the size of the point.\nüé¨ Add a label to one gene of interest (Procr) and :\n\nhspc_prog_results |&gt; \n  ggplot(aes(x = summary.logFC, \n             y = log10_FDR, \n             colour = interaction(sig, bigfc))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05), \n             linetype = \"dashed\") +\n  geom_vline(xintercept = 2, \n             linetype = \"dashed\") +\n  geom_vline(xintercept = -2, \n             linetype = \"dashed\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_colour_manual(values = c(\"gray\", \n                                 \"pink\",\n                                # \"gray30\",\n                                 \"deeppink\")) +\n  geom_label_repel(data = hspc_prog_results |&gt; \n                    filter(external_gene_name == \"Procr\"),\n                  aes(label = external_gene_name),\n                  size = 4,\n                  nudge_x = .5,\n                  nudge_y = 1.5) +\n  geom_point(data = hspc_prog_results |&gt; \n                    filter(external_gene_name == \"Procr\"),\n                size = 3) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nShould you want to label more than one gene, you will need to use (for example): filter(external_gene_name %in% c(\"Procr\", \"Emb\"))\nNow go to Save your plots",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/overview.html",
    "href": "transcriptomics/week-4/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week we cover differential expression analysis on raw counts or log normalised values. The independent study will allow you to check you have what you should have following the Transcriptomics 1: Hello Data workshop and Consolidation study. It will also summarise the concepts and methods we will use in the workshop. In the workshop, you will learn how to perform differential expression analysis on raw counts using DESeq2 (Love, Huber, and Anders 2014) or on logged normalised expression values using scran (Lun, McCarthy, and Marioni 2016) or both. You will also add information about genes programmatically.\nWe suggest you sit together with your group in the workshop.\n\nLearning objectives\nThe successful student will be able to:\n\nverify they have the required RStudio Project set up and the data and code files from the previous Workshop and Consolidation study\nexplain the goal of differential expression analysis and the importance of normalisation\nexplain why and how the nature of the input values determines the analysis package used\ndescribe the metadata needed to carry out differential expression analysis and the statistical models used by DESeq2 and scran\nfind genes that are unexpressed or expressed in a just one group\nperform differential expression analysis on raw counts using DESeq2 or on logged normalised expression values using scran or both.\nexplain the output of differential expression: log fold change, p-value, adjusted p-value\nadd information about genes programmatically to their results\nprepare for a discussion with their project supervisor about genes of interest\n\n\n\nInstructions\n\nPrepare\n\nüìñ Check what you should have after week 3\nüìñ Read about concepts in differential expression analysis.\nüìñ Find out what packages we will use.\n\nWorkshop\n\nüíª Find unexpressed genes and those expressed in a single cell type or treatment group.\nüíª Set up the metadata for differential expression analysis.\nüíª Perform differential expression analysis on raw counts using DESeq2 or on logged normalised expression values using scran.\nLook after future you!\n\nConsolidate\n\nüíª Use the work you completed in the workshop as a template to apply to a new case.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLove, Michael I., Wolfgang Huber, and Simon Anders. 2014. ‚ÄúModerated Estimation of Fold Change and Dispersion for RNA-Seq Data with DESeq2.‚Äù Genome Biology 15: 550. https://doi.org/10.1186/s13059-014-0550-8.\n\n\nLun, Aaron T. L., Davis J. McCarthy, and John C. Marioni. 2016. ‚ÄúA Step-by-Step Workflow for Low-Level Analysis of Single-Cell RNA-Seq Data with Bioconductor.‚Äù F1000Res. 5: 2122. https://doi.org/10.12688/f1000research.9501.2.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "About"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html",
    "href": "transcriptomics/week-4/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In the workshop, you will learn how to perform differential expression analysis on raw counts using DESeq2 (Love, Huber, and Anders 2014) or on logged normalised expression values using scran (Lun, McCarthy, and Marioni 2016) or both.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#session-overview",
    "href": "transcriptomics/week-4/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In the workshop, you will learn how to perform differential expression analysis on raw counts using DESeq2 (Love, Huber, and Anders 2014) or on logged normalised expression values using scran (Lun, McCarthy, and Marioni 2016) or both.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#frog-development",
    "href": "transcriptomics/week-4/workshop.html#frog-development",
    "title": "Workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\nüé¨ Open the frogs-88H RStudio Project and the cont-fgf-s30.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#arabidopisis",
    "href": "transcriptomics/week-4/workshop.html#arabidopisis",
    "title": "Workshop",
    "section": "üéÑ Arabidopisis\n",
    "text": "üéÑ Arabidopisis\n\nüé¨ Open the arab-88H RStudio Project and the suff-def-wild.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#leishmania",
    "href": "transcriptomics/week-4/workshop.html#leishmania",
    "title": "Workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\nüé¨ Open the leish-88H RStudio Project and the pro-meta.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#stem-cells",
    "href": "transcriptomics/week-4/workshop.html#stem-cells",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nüé¨ Open the mice-88H RStudio Project and the hspc-prog.R script.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#everyone",
    "href": "transcriptomics/week-4/workshop.html#everyone",
    "title": "Workshop",
    "section": "Everyone",
    "text": "Everyone\nüé¨ Make a new folder results in the project directory.\nThis is where we will save our results.\nüé¨ Load tidyverse (Wickham et al. 2019) You most likely have this code at the top of `your script already.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.3     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.3     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package to force all conflicts to become errors\nHave you ever stopped to think about this message? It is telling us that there are functions in the dplyr package that have the same name as functions in the stats package and that R will use the dplyr version. As this is what you want, this has always been fine. It still is fine in this case. However, as you start to load more packages, you will want to know if you are using a function from a package that has the same name as a function in another loaded package. This is where the conflicted (Wickham 2023) package comes in. Conflicted will warn you when you are using a function that has the same name as a function in another package. You can then choose which function to use.\nüé¨ Load the conflicted package:\n\nlibrary(conflicted)\n\nInstead of getting a warning every time you are using a function that has a function with the same name in another package, we can declare a preference for one function over another. This is useful for the functions you use a lot or ones where you are certain you always want to use a particular function.\nFor example, to always use the dplyr version of filter() by default you can add this to the top of your script:\n\nconflicts_prefer(dplyr::filter)\n\nWe will also want to ensure that we are using the setdiff() function from the GenomicRanges package which is used by DESeq2.\n\nconflicts_prefer(GenomicRanges::setdiff)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#frog-development-1",
    "href": "transcriptomics/week-4/workshop.html#frog-development-1",
    "title": "Workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\nWe need to import the S30 data that were filtered to remove genes with 4, 5 or 6 zeros and those where the total counts was less than 20.\nüé¨ Import the data from the data-processed folder.\nNow go to Differential Expression Analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#arabidopisis-1",
    "href": "transcriptomics/week-4/workshop.html#arabidopisis-1",
    "title": "Workshop",
    "section": "üéÑ Arabidopisis\n",
    "text": "üéÑ Arabidopisis\n\nWe need to import the wildtype data that were filtered to remove genes with 3 or 4 zeros and those where the total counts was less than 20.\nüé¨ Import the data from the data-processed folder.\nNow go to Differential Expression Analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#leishmania-1",
    "href": "transcriptomics/week-4/workshop.html#leishmania-1",
    "title": "Workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\nWe need to import the procyclic- and metacyclic-promastigote data that were filtered to remove genes with 4, 5 or 6 zeros and those where the total counts was less than 20.\nüé¨ Import the data from the data-processed folder.\nNow go to Differential Expression Analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#stem-cells-1",
    "href": "transcriptomics/week-4/workshop.html#stem-cells-1",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nWe need to import the progenitor and HSPC cell data that were combined.\nüé¨ Import the data from the data-processed folder..\nNow go to Differential Expression Analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#frog-development-2",
    "href": "transcriptomics/week-4/workshop.html#frog-development-2",
    "title": "Workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\nThese are the steps we will take\n\nFind the genes that are expressed in only one treatment group.\nCreate a DESeqDataSet object. This is a special object that is used by the DESeq2 package\nPrepare the normalised counts from the DESeqDataSet object.\nDo differential expression analysis on the genes. This needs to be done on the raw counts.\n\nAll but the first step are done with the DESeq2 package\n1. Genes expressed in one treatment\nThe genes expressed in only one treatment group are those with zeros in all three replicates in one group and non-zero values in all three replicates in the other group. For example, those shown here:\n\n\n\n\n\n\n\n\n\n\n\n\n\nxenbase_gene_id\nS30_C_1\nS30_C_2\nS30_C_3\nS30_F_1\nS30_F_2\nS30_F_3\n\n\n\nXB-GENE-1018260\n0\n0\n0\n10\n2\n16\n\n\nXB-GENE-17330117\n0\n0\n0\n13\n4\n17\n\n\nXB-GENE-17332184\n0\n0\n0\n6\n19\n6\n\n\n\n\n\nWe will use filter() to find these genes.\nüé¨ Find the genes that are expressed only in the FGF-treated group:\n\ns30_fgf_only &lt;- s30_filtered |&gt; \n  filter(S30_C_1 == 0, \n         S30_C_2 == 0, \n         S30_C_3 == 0, \n         S30_F_1 &gt; 0, \n         S30_F_2 &gt; 0, \n         S30_F_3 &gt; 0)\n\n‚ùì How many genes are expressed only in the FGF-treated group?\n\n\nüé¨ Now you find any genes that are expressed only in the control group.\n‚ùì How many genes are expressed only in the control group?\n\n\n‚ùì Do the results make sense to you in light of what you know about the biology?\n\n\n\n\n\n\n\nüé¨ Write all the genes that are expressed one group only to file (saved in results)\n2. Create DESeqDataSet object\nüé¨ Load the DESeq2 package:\nA DEseqDataSet object is a custom data type that is used by DESeq2. Custom data types are common in the Bioconductor1 packages. They are used to store data in a way that is useful for the analysis. These data types typically have data, transformed data, metadata and experimental designs within them.\nTo create a DESeqDataSet object, we need to provide three things:\n\nThe raw counts - these are in s30_filtered\n\nThe meta data which gives information about the samples and which treatment groups they belong to\nA design matrix which captures the design of the statistical model.\n\nThe counts must in a matrix rather than a dataframe. Unlike a dataframe, a matrix has columns of all the same type. That is, it will contain only the counts. The gene ids are given as row names rather than a column. The matrix() function will create a matrix from a dataframe of columns of the same type and the select() function can be used to remove the gene ids column.\nüé¨ Create a matrix of the counts:\n\ns30_count_mat &lt;- s30_filtered |&gt;\n  select(-xenbase_gene_id) |&gt;\n  as.matrix()\n\nüé¨ Add the gene ids as row names to the matrix:\n\n# add the row names to the matrix\nrownames(s30_count_mat) &lt;- s30_filtered$xenbase_gene_id\n\nYou might want to view the matrix (click on it in your environment pane).\nThe metadata are in a file, frog_meta_data.txt. This is a tab-delimited file. The first column is the sample name and the other columns give the ‚Äútreatments‚Äù. In this case, the treatments stage (with three levels) and treatment (with two levels).\nüé¨ Make a folder called meta and save the file to it.\nüé¨ Read the metadata into a dataframe:\n\nmeta &lt;- read_table(\"meta/frog_meta_data.txt\")\n\nüé¨ Examine the resulting dataframe.\nWe need to add the sample names as row names to the metadata dataframe. This is because the DESeqDataSet object will use the row names to match the samples in the metadata to the samples in the counts matrix.\nüé¨ Add the sample names as row names to the metadata dataframe:\n\nmeta &lt;- meta |&gt;\n  column_to_rownames(\"sample_id\")\n\nWe are dealing only with the S30 data so we need to remove the samples that are not in the S30 data.\nüé¨ Filter the metadata to keep only the S30 information:\n\nmeta_s30 &lt;- meta |&gt;\n  filter(stage == \"stage_30\")\n\nWe can now create the DESeqDataSet object. The design formula describes the statistical model. You should recognise the form from previous work. The ~ can be read as ‚Äúexplain by‚Äù and on its right hand side are the explanatory variables. That is, the model is counts explained by treatment and sibling_rep. We are interested in the difference between the treatments but we include sibling_rep to account for the fact that the data are paired. The condition of interest should go at the end of the design formula.\nNote that:\n\nThe names of the columns in the count matrix have to exactly match the names of the rows in the metadata dataframe. They also need to be in the same order.\nThe names of the explanatory variables in the design formula have to match the names of columns in the metadata.\n\nüé¨ Create the DESeqDataSet object:\n\ndds &lt;- DESeqDataSetFromMatrix(countData = s30_count_mat,\n                              colData = meta_s30,\n                              design = ~ sibling_rep + treatment)\n\nThe warning ‚ÄúWarning: some variables in design formula are characters, converting to factors‚Äù just means that the variable type of treatment and sibling_rep in the metadata dataframe are ‚Äúchar‚Äù and they have been converted into the factors.\nTo help you understand what the DESeqDataSet object we have called dds contains, we can look its contents\nThe counts are in dds@assays@data@listData[[\"counts\"]] and the metadata are in dds@colData but the easiest way to see them is to use the counts() and colData() functions from the DESeq2 package.\nüé¨ View the counts:\n\ncounts(dds) |&gt; View()\n\nYou should be able to see that this is the same as in s30_count_mat.\nüé¨ View the column information:\n\ncolData(dds)\n\nDataFrame with 6 rows and 3 columns\n              stage treatment sibling_rep\n        &lt;character&gt;  &lt;factor&gt;    &lt;factor&gt;\nS30_C_1    stage_30   control       one  \nS30_C_2    stage_30   control       two  \nS30_C_3    stage_30   control       three\nS30_F_1    stage_30   FGF           one  \nS30_F_2    stage_30   FGF           two  \nS30_F_3    stage_30   FGF           three\n\n\nYou should be able to see this is the same as in meta_s30.\n3. Prepare the normalised counts\nThe normalised counts are the counts that have been transformed to account for the library size (i.e., the total number of reads in a sample) and the gene length. We have to first estimate the normalisation factors and store them in the DESeqDataSet object and then we can get the normalised counts.\nüé¨ Estimate the factors for normalisation and store them in the DESeqDataSet object:\n\ndds &lt;- estimateSizeFactors(dds)\n\nüé¨ Look at the factors (just for information):\n\nsizeFactors(dds)\n\n  S30_C_1   S30_C_2   S30_C_3   S30_F_1   S30_F_2   S30_F_3 \n0.8812200 0.9454600 1.2989886 1.0881870 1.0518961 0.8322894 \n\n\nThe normalised counts will be useful to use later. To get the normalised counts we again used the counts() function but this time we use the normalized=TRUE argument.\nüé¨ Save the normalised to a matrix:\n\nnormalised_counts &lt;- counts(dds, normalized = TRUE)\n\nüé¨ Make a dataframe of the normalised counts, adding a column for the gene ids at the same time:\n\ns30_normalised_counts &lt;- data.frame(normalised_counts,\n                                    xenbase_gene_id = row.names(normalised_counts))\n\n4. Differential expression analysis\nWe use the DESeq() function to do the differential expression analysis. This function fits the statistical model to the data and then uses the model to calculate the significance of the difference between the treatments. It again stores the results in the DESseqDataSet object. Note that the differential expression needs the raw (unnormalised counts) as it does its own normalisation as part of the process.\nüé¨ Run the differential expression analysis and store the results in the same object:\n\ndds &lt;- DESeq(dds)\n\nThe function will take only a few moments to run on this data but can take longer for bigger datasets.\nWe need to define the contrasts we want to test. We want to test the difference between the treatments so we will define the contrast as FGF and control.\nüé¨ Define the contrast:\n\ncontrast_fgf &lt;- c(\"treatment\", \"FGF\", \"control\")\n\nNote that treatment is the name of the column in the metadata dataframe and FGF and control are the names of the levels in the treatment column. By putting them in the order FGF , control we are saying the fold change will be FGF / control. This means:\n\npositive log fold changes indicate FGF &gt; control and\nnegative log fold changes indicates control &gt; FGF.\n\nIf we had put them in the order control, FGF we would have the reverse.\nüé¨ Extract the results from the DESseqDataSet object:\n\nresults_fgf &lt;- results(dds,\n                       contrast = contrast_fgf)\n\nThis will give us the log2 fold change, the p-value and the adjusted p-value for the comparison between the control and the FGF-treatment for each gene.\nüé¨ Put the results in a dataframe and add the gene ids as a column:\n\ns30_results &lt;- data.frame(results_fgf,\n                          xenbase_gene_id = row.names(results_fgf))\n\nIt is useful to have the normalised counts and the statistical results in one dataframe.\nüé¨ Merge the two dataframes:\n\n# merge the results with the normalised counts\ns30_results &lt;- s30_normalised_counts |&gt;\n  left_join(s30_results, by = \"xenbase_gene_id\")\n\nNow go to Add gene information.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#arabidopisis-2",
    "href": "transcriptomics/week-4/workshop.html#arabidopisis-2",
    "title": "Workshop",
    "section": "üéÑ Arabidopisis\n",
    "text": "üéÑ Arabidopisis\n\nThese are the steps we will take\n\nFind the genes that are expressed in only one treatment group.\nCreate a DESeqDataSet object. This is a special object that is used by the DESeq2 package\nPrepare the normalised counts from the DESeqDataSet object.\nDo differential expression analysis on the genes. This needs to be done on the raw counts.\n\nAll but the first step are done with the DESeq2 package\n1. Genes expressed in one treatment\nThe genes expressed in only one treatment group are those with zeros in both replicates in one group and non-zero values in both replicates in the other group. For example, those shown here:\n\n\n\n\n\n\n\n\n\n\n\ngene_id\nSRX028956_wild_suf\nSRX028957_wild_def\nSRX028960_wild_suf\nSRX028961_wild_def\n\n\n\nAT1G04513\n11\n0\n25\n0\n\n\nAT1G22610\n36\n0\n52\n0\n\n\nAT1G26290\n12\n0\n23\n0\n\n\nAT1G59810\n5\n0\n16\n0\n\n\nAT2G44130\n28\n0\n18\n0\n\n\n\n\n\nWe will use filter() to find these genes.\nüé¨ Find the genes that are expressed only in the sufficient copper group:\n\nwild_suf_only &lt;- wild_filtered |&gt;\n  filter(SRX028961_wild_def == 0,\n         SRX028957_wild_def == 0,\n         SRX028960_wild_suf &gt; 0,\n         SRX028956_wild_suf &gt; 0)\n\n‚ùì How many genes are expressed only in the sufficient copper group?\n\n\nüé¨ Now you find any genes that are expressed only in the deficient copper group.\n‚ùì How many genes are expressed only in the deficient copper group?\n\n\n‚ùì Do the results make sense to you in light of what you know about the biology?\n\n\n\n\n\nüé¨ Write all the genes that are expressed one group only to file (saved in results)\n2. Create DESeqDataSet object\nüé¨ Load the DESeq2 package:\nA DEseqDataSet object is a custom data type that is used by DESeq2. Custom data types are common in the Bioconductor2 packages. They are used to store data in a way that is useful for the analysis. These data types typically have data, transformed data, metadata and experimental designs within them.\nTo create a DESeqDataSet object, we need to provide three things:\n\nThe raw counts - these are in wild_filtered\n\nThe meta data which gives information about the samples and which treatment groups they belong to\nA design matrix which captures the design of the statistical model.\n\nThe counts must in a matrix rather than a dataframe. Unlike a dataframe, a matrix has columns of all the same type. That is, it will contain only the counts. The gene ids are given as row names rather than a column. The matrix() function will create a matrix from a dataframe of columns of the same type and the select() function can be used to remove the gene ids column.\nüé¨ Create a matrix of the counts:\n\nwild_count_mat &lt;- wild_filtered |&gt;\n  select(-gene_id) |&gt;\n  as.matrix()\n\nüé¨ Add the gene ids as row names to the matrix:\n\n# add the row names to the matrix\nrownames(wild_count_mat) &lt;- wild_filtered$gene_id\n\nYou might want to view the matrix (click on it in your environment pane).\nThe metadata are in a file, arab_meta_data.txt. This is a tab-delimited file. The first column is the sample name and the other columns give the ‚Äútreatments‚Äù. In this case, the treatments genotype (with two levels) and copper (with two levels).\nüé¨ Make a folder called meta and save the file to it.\nüé¨ Read the metadata into a dataframe:\n\nmeta &lt;- read_table(\"meta/arab_meta_data.txt\")\n\nüé¨ Examine the resulting dataframe.\nWe need to add the sample names as row names to the metadata dataframe. This is because the DESeqDataSet object will use the row names to match the samples in the metadata to the samples in the counts matrix.\nüé¨ Add the sample names as row names to the metadata dataframe:\n\nmeta &lt;- meta |&gt;\n  column_to_rownames(\"sample_id\")\n\nWe are dealing only with the wild data so we need to remove the samples that are not in the wild data.\nüé¨ Filter the metadata to keep only the wild information:\n\nmeta_wild &lt;- meta |&gt;\n  filter(genotype == \"wt\")\n\nWe can now create the DESeqDataSet object. The design formula describes the statistical model. You should recognise the form from previous work. The ~ can be read as ‚Äúexplain by‚Äù and on its right hand side are the explanatory variables. That is, the model is counts explained by copper status.\nNote that:\n\nThe names of the columns in the count matrix have to exactly match the names of the rows in the metadata dataframe. They also need to be in the same order.\nThe names of the explanatory variables in the design formula have to match the names of columns in the metadata.\n\nüé¨ Create the DESeqDataSet object:\n\ndds &lt;- DESeqDataSetFromMatrix(wild_count_mat,\n                              colData = meta_wild,\n                              design = ~ copper)\n\nThe warning ‚ÄúWarning: some variables in design formula are characters, converting to factors‚Äù just means that the variable type of copper in the metadata dataframe is ‚Äúchar‚Äù and it has been converted into a factor type.\nTo help you understand what the DESeqDataSet object we have called dds contains, we can look its contents\nThe counts are in dds@assays@data@listData[[\"counts\"]] and the metadata are in dds@colData but the easiest way to see them is to use the counts() and colData() functions from the DESeq2 package.\nüé¨ View the counts:\n\ncounts(dds) |&gt; View()\n\nYou should be able to see that this is the same as in wild_count_mat.\nüé¨ View the column information:\n\ncolData(dds)\n\nDataFrame with 4 rows and 2 columns\n                      genotype     copper\n                   &lt;character&gt;   &lt;factor&gt;\nSRX028956_wild_suf          wt sufficient\nSRX028957_wild_def          wt deficient \nSRX028960_wild_suf          wt sufficient\nSRX028961_wild_def          wt deficient \n\n\nYou should be able to see this is the same as in meta_wild.\n3. Prepare the normalised counts\nThe normalised counts are the counts that have been transformed to account for the library size (i.e., the total number of reads in a sample) and the gene length. We have to first estimate the normalisation factors and store them in the DESeqDataSet object and then we can get the normalised counts.\nüé¨ Estimate the factors for normalisation and store them in the DESeqDataSet object:\n\ndds &lt;- estimateSizeFactors(dds)\n\nüé¨ Look at the factors (just for information):\n\nsizeFactors(dds)\n\nSRX028956_wild_suf SRX028957_wild_def SRX028960_wild_suf SRX028961_wild_def \n         0.8200020          0.4653024          2.3002428          1.1965924 \n\n\nThe normalised counts will be useful to use later. To get the normalised counts we again used the counts() function but this time we use the normalized=TRUE argument.\nüé¨ Save the normalised to a matrix:\n\nnormalised_counts &lt;- counts(dds, normalized = TRUE)\n\nüé¨ Make a dataframe of the normalised counts, adding a column for the gene ids at the same time:\n\nwild_normalised_counts &lt;- data.frame(normalised_counts,\n                                    gene_id = row.names(normalised_counts))\n\n4. Differential expression analysis\nWe use the DESeq() function to do the differential expression analysis. This function fits the statistical model to the data and then uses the model to calculate the significance of the difference between the treatments. It again stores the results in the DESseqDataSet object. Note that the differential expression needs the raw (unnormalised counts) as it does its own normalisation as part of the process.\nüé¨ Run the differential expression analysis and store the results in the same object:\n\ndds &lt;- DESeq(dds)\n\nThe function will take only a few moments to run on this data but can take longer for bigger datasets.\nWe need to define the contrasts we want to test. We want to test the difference between the treatments so we will define the contrast as sufficient and deficient.\nüé¨ Define the contrast:\n\ncontrast_suf &lt;- c(\"copper\", \"sufficient\", \"deficient\")\n\nNote that copper is the name of the column in the metadata dataframe and sufficient and deficient are the names of the levels in the copper column. By putting them in the order sufficient , deficient we are saying the fold change will be sufficient / deficient. This means:\n\npositive log fold changes indicate sufficient &gt; deficient and\nnegative log fold changes indicates deficient &gt; sufficient.\n\nIf we had put them in the order deficient, sufficient we would have the reverse.\nüé¨ Extract the results from the DESseqDataSet object:\n\nresults_suf &lt;- results(dds,\n                       contrast = contrast_suf)\n\nThis will give us the log2 fold change, the p-value and the adjusted p-value for the comparison between the sufficient- and\ndeficient-copper for each gene.\nüé¨ Put the results in a dataframe and add the gene ids as a column:\n\nwild_results &lt;- data.frame(results_suf,\n                          gene_id = row.names(results_suf))\n\nIt is useful to have the normalised counts and the statistical results in one dataframe.\nüé¨ Merge the two dataframes:\n\n# merge the results with the normalised counts\nwild_results &lt;- wild_normalised_counts |&gt;\n  left_join(wild_results, by = \"gene_id\")\n\nNow go to Add gene information.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#leishmania-2",
    "href": "transcriptomics/week-4/workshop.html#leishmania-2",
    "title": "Workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\nThese are the steps we will take\n\nFind the genes that are expressed in only one treatment group.\nCreate a DESeqDataSet object. This is a special object that is used by the DESeq2 package\nPrepare the normalised counts from the DESeqDataSet object.\nDo differential expression analysis on the genes. This needs to be done on the raw counts.\n\nAll but the first step are done with the DESeq2 package\n1. Genes expressed in one treatment\nThe genes expressed in only one treatment group are those with zeros in all replicates in one group and non-zero values in all replicates in the other group.\nWe will use filter() to find these genes.\nüé¨ Find the genes that are expressed only at the procyclic-promastigote stage:\n\npro_meta_pro_only &lt;- pro_meta_filtered  |&gt;\n  filter(lm_pro_1 &gt; 0,\n         lm_pro_2 &gt; 0,\n         lm_pro_3 &gt; 0,\n         lm_meta_1 == 0,\n         lm_meta_2 == 0,\n         lm_meta_2 == 0)\n\n‚ùì How many genes are expressed only in the procyclic-promastigote stage group?\n\n\nüé¨ Now you find any genes that are expressed only at the metacyclic stage\n‚ùì How many genes are expressed only at the metacyclic stage?\n\n\n‚ùì Do the results make sense to you in light of what you know about the biology?\n\n\n\n\nüé¨ Write all the genes that are expressed one group only to file (saved in results)\n2. Create DESeqDataSet object\nüé¨ Load the DESeq2 package:\nA DEseqDataSet object is a custom data type that is used by DESeq2. Custom data types are common in the Bioconductor3 packages. They are used to store data in a way that is useful for the analysis. These data types typically have data, transformed data, metadata and experimental designs within them.\nTo create a DESeqDataSet object, we need to provide three things:\n\nThe raw counts - these are in pro_meta_filtered\n\nThe meta data which gives information about the samples and which treatment groups they belong to\nA design matrix which captures the design of the statistical model.\n\nThe counts must in a matrix rather than a dataframe. Unlike a dataframe, a matrix has columns of all the same type. That is, it will contain only the counts. The gene ids are given as row names rather than a column. The matrix() function will create a matrix from a dataframe of columns of the same type and the select() function can be used to remove the gene ids column.\nüé¨ Create a matrix of the counts:\n\npro_meta_count_mat &lt;- pro_meta_filtered  |&gt;\n  select(-gene_id) |&gt;\n  as.matrix()\n\nüé¨ Add the gene ids as row names to the matrix:\n\n# add the row names to the matrix\nrownames(pro_meta_count_mat) &lt;- pro_meta_filtered$gene_id\n\nYou might want to view the matrix (click on it in your environment pane).\nThe metadata are in a file, leish_meta_data.txt. This is a tab-delimited file. The first column is the sample name and the other columns give the ‚Äútreatments‚Äù. In this case, the treatment is stage (with three levels).\nüé¨ Make a folder called meta and save the file to it.\nüé¨ Read the metadata into a dataframe:\n\nmeta &lt;- read_table(\"meta/leish_meta_data.txt\")\n\nüé¨ Examine the resulting dataframe.\nWe need to add the sample names as row names to the metadata dataframe. This is because the DESeqDataSet object will use the row names to match the samples in the metadata to the samples in the counts matrix.\nüé¨ Add the sample names as row names to the metadata dataframe:\n\nmeta &lt;- meta |&gt;\n  column_to_rownames(\"sample_id\")\n\nWe are dealing only with the wild data so we need to remove the samples that are not in the wild data.\nüé¨ Filter the metadata to keep only the procyclic and metacyclic information:\n\nmeta_pro_meta &lt;- meta |&gt;\n  filter(stage != \"amastigotes\")\n\nWe can now create the DESeqDataSet object. The design formula describes the statistical model. You should recognise the form from previous work. The ~ can be read as ‚Äúexplain by‚Äù and on its right hand side are the explanatory variables. That is, the model is counts explained by stage status.\nNote that:\n\nThe names of the columns in the count matrix have to exactly match the names of the rows in the metadata dataframe. They also need to be in the same order.\nThe names of the explanatory variables in the design formula have to match the names of columns in the metadata.\n\nüé¨ Create the DESeqDataSet object:\n\ndds &lt;- DESeqDataSetFromMatrix(pro_meta_count_mat,\n                              colData = meta_pro_meta,\n                              design = ~ stage)\n\nThe warning ‚ÄúWarning: some variables in design formula are characters, converting to factors‚Äù just means that the variable type of stage in the metadata dataframe is ‚Äúchar‚Äù and it has been converted into a factor type.\nTo help you understand what the DESeqDataSet object we have called dds contains, we can look its contents\nThe counts are in dds@assays@data@listData[[\"counts\"]] and the metadata are in dds@colData but the easiest way to see them is to use the counts() and colData() functions from the DESeq2 package.\nüé¨ View the counts:\n\ncounts(dds) |&gt; View()\n\nYou should be able to see that this is the same as in pro_meta_count_mat.\nüé¨ View the column information:\n\ncolData(dds)\n\nDataFrame with 6 rows and 2 columns\n               stage replicate\n            &lt;factor&gt; &lt;numeric&gt;\nlm_pro_1  procyclic          1\nlm_pro_2  procyclic          2\nlm_pro_3  procyclic          3\nlm_meta_1 metacyclic         1\nlm_meta_2 metacyclic         2\nlm_meta_3 metacyclic         3\n\n\nYou should be able to see this is the same as in meta_pro_meta.\n3. Prepare the normalised counts\nThe normalised counts are the counts that have been transformed to account for the library size (i.e., the total number of reads in a sample) and the gene length. We have to first estimate the normalisation factors and store them in the DESeqDataSet object and then we can get the normalised counts.\nüé¨ Estimate the factors for normalisation and store them in the DESeqDataSet object:\n\ndds &lt;- estimateSizeFactors(dds)\n\nüé¨ Look at the factors (just for information):\n\nsizeFactors(dds)\n\n lm_pro_1  lm_pro_2  lm_pro_3 lm_meta_1 lm_meta_2 lm_meta_3 \n1.3029351 0.9158157 0.9943186 0.7849299 0.8443586 1.3250409 \n\n\nThe normalised counts will be useful to use later. To get the normalised counts we again used the counts() function but this time we use the normalized=TRUE argument.\nüé¨ Save the normalised to a matrix:\n\nnormalised_counts &lt;- counts(dds, normalized = TRUE)\n\nüé¨ Make a dataframe of the normalised counts, adding a column for the gene ids at the same time:\n\npro_meta_normalised_counts &lt;- data.frame(normalised_counts,\n                                    gene_id = row.names(normalised_counts))\n\n4. Differential expression analysis\nWe use the DESeq() function to do the differential expression analysis. This function fits the statistical model to the data and then uses the model to calculate the significance of the difference between the treatments. It again stores the results in the DESseqDataSet object. Note that the differential expression needs the raw (unnormalised counts) as it does its own normalisation as part of the process.\nüé¨ Run the differential expression analysis and store the results in the same object:\n\ndds &lt;- DESeq(dds)\n\nThe function will take only a few moments to run on this data but can take longer for bigger datasets.\nWe need to define the contrasts we want to test. We want to test the difference between the treatments so we will define the contrast as procyclic and metacyclic.\nüé¨ Define the contrast:\n\ncontrast_pro_meta &lt;- c(\"stage\", \"procyclic\", \"metacyclic\")\n\nNote that stage is the name of the column in the metadata dataframe and procyclic and metacyclic are the names of the levels in the stage column. By putting them in the order procyclic , metacyclic we are saying the fold change will be procyclic / metacyclic. This means:\n\npositive log fold changes indicate procyclic &gt; metacyclic and\nnegative log fold changes indicates metacyclic &gt; procyclic.\n\nIf we had put them in the order metacyclic, procyclic we would have the reverse.\nüé¨ Extract the results from the DESseqDataSet object:\n\nresults_pro_meta &lt;- results(dds,\n                       contrast = contrast_pro_meta)\n\nThis will give us the log2 fold change, the p-value and the adjusted p-value for the comparison between procyclic and metacyclic stage for each gene\nüé¨ Put the results in a dataframe and add the gene ids as a column:\n\npro_meta_results &lt;- data.frame(results_pro_meta,\n                          gene_id = row.names(results_pro_meta))\n\nIt is useful to have the normalised counts and the statistical results in one dataframe.\nüé¨ Merge the two dataframes:\n\n# merge the results with the normalised counts\npro_meta_results &lt;- pro_meta_normalised_counts |&gt;\n  left_join(pro_meta_results, by = \"gene_id\")\n\nNow go to Add gene information.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#stem-cells-2",
    "href": "transcriptomics/week-4/workshop.html#stem-cells-2",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nThese are the steps we will take\n\nFind the genes that are expressed in only one cell type (the prog or the hspc).\nPrepare the data for differential expression analysis with the scran package.\nDo differential expression analysis on the genes using the scran package. This needs to be done on the logged normalised counts.\n\n1. Genes expressed in one cell type\nThe genes expressed in only cell type are those with zeros in all the cells of the other type. We can find these by summing the expression values for each gene across the cells of one type and filtering for those that are zero.\nTo do row wise aggregates such as the sum across rows we can use the rowwise() function. c_across() allows us to use the colon notation to select columns. This is very useful when you have a lot of columns because it would be annoying to have to list all of them. HSPC_001:HSPC_852 means all the columns from HSPC_001 to HSPC_852.\nüé¨ Find the genes expressed only in progenitor cells (i.e., those that are 0 in every HSPC cell):\n\nhspc_prog |&gt; \n  rowwise() |&gt; \n  filter(sum(c_across(HSPC_001:HSPC_852)) == 0)\n\n# A tibble: 0 √ó 1,500\n# Rowwise: \n# ‚Ñπ 1,500 variables: ensembl_gene_id &lt;chr&gt;, HSPC_001 &lt;dbl&gt;, HSPC_002 &lt;dbl&gt;,\n#   HSPC_003 &lt;dbl&gt;, HSPC_004 &lt;dbl&gt;, HSPC_006 &lt;dbl&gt;, HSPC_008 &lt;dbl&gt;,\n#   HSPC_009 &lt;dbl&gt;, HSPC_011 &lt;dbl&gt;, HSPC_012 &lt;dbl&gt;, HSPC_014 &lt;dbl&gt;,\n#   HSPC_015 &lt;dbl&gt;, HSPC_016 &lt;dbl&gt;, HSPC_017 &lt;dbl&gt;, HSPC_018 &lt;dbl&gt;,\n#   HSPC_020 &lt;dbl&gt;, HSPC_021 &lt;dbl&gt;, HSPC_022 &lt;dbl&gt;, HSPC_023 &lt;dbl&gt;,\n#   HSPC_024 &lt;dbl&gt;, HSPC_025 &lt;dbl&gt;, HSPC_026 &lt;dbl&gt;, HSPC_027 &lt;dbl&gt;,\n#   HSPC_028 &lt;dbl&gt;, HSPC_030 &lt;dbl&gt;, HSPC_031 &lt;dbl&gt;, HSPC_033 &lt;dbl&gt;, ‚Ä¶\n\n\nWe already know from last week‚Äôs work that there are no genes that are zero across all the cells (both types). If we did not know that, we would need to add |&gt; filter(sum(c_across(Prog_001:Prog_852)) != 0)\nmeaning zero in all the HSPC but not zero in all the Prog\n‚ùì How many genes are expressed only in the progenitor cells only\n\n\nüé¨ Now you find any genes that are expressed only in the HSPC cells.\n‚ùì How many genes are expressed only in the HSPC cells?\n\n\n‚ùì Do the results make sense to you in light of what you know about the biology?\n\n\n\n\n\n\n\nüé¨ Write all the genes that are expressed one cell type only to file (saved in results)\n2. Prepare the data for analysis with scran\n\nscran can use a matrix or a dataframe of counts but these must be log normalised counts. If using a dataframe, the columns must only contain the expression values (not the gene ids). The rows can be named to retain the gene ids.\nhspc_prog is a dataframe so we will use the ensembl gene ids to name the rows and remove the gene ids from the dataframe.\nüé¨ Add the gene ids as the row names:\n\nhspc_prog &lt;- hspc_prog |&gt;\n  column_to_rownames(\"ensembl_gene_id\")\n\nLike DESeq2, scran needs metadata to define which columns were in which group. Instead of having this is a file, we will create a vector that indicates which column belongs to which cell type.\nüé¨ Create a vector that indicates which column belongs to which cell type:\n\nn_hspc &lt;- 701\nn_prog &lt;- 798\n\ncell_type &lt;- rep(c(\"hspc\",\"prog\"), \n                 times = c(n_hspc, n_prog))\n\nThe number of times each cell type is repeated is the number of cells of that type. Do check that the length of the cell_type vector is the same as the number of columns in the hspc_prog dataframe.\n3. Differential expression analysis\nüé¨ Load the scran package:\nDifferential expression is carried out with the findMarkers() function. It takes two arguments. The first argument is the dataframe containing the data and the second argument is the vector indicating which columns are in which cell type. Make sure that the order of the cell types in the vector is appropriate for the order of the columns in the dataframe.\nüé¨ Run the differential expression analysis:\n\nresults_hspc_prog &lt;- findMarkers(hspc_prog, \n                             cell_type)\n\nThe output is a list object which, rather unnecessarily, includes two dataframes. This is not really necessary as the results are the same except for the fold change having a different sign.\n\n\nThe dataframe results_hspc_prog$prog is log prog - log hspc (i.e.,Prog/HSPC). This means:\n\nPositive fold change: prog is higher than hspc\nNegative fold change: hspc is higher than prog\n\n\n\n\n\n\nThe results_hspc_prog$prog dataframe\n\n\n\n\n\n\n\n\n\n\n\nTop\np.value\nFDR\nsummary.logFC\nlogFC.hspc\nensembl_gene_id\n\n\n\nENSMUSG00000028639\n1\n0\n0\n1.596910\n1.596910\nENSMUSG00000028639\n\n\nENSMUSG00000024053\n2\n0\n0\n3.035165\n3.035165\nENSMUSG00000024053\n\n\nENSMUSG00000041329\n3\n0\n0\n3.261056\n3.261056\nENSMUSG00000041329\n\n\nENSMUSG00000030336\n4\n0\n0\n-2.146491\n-2.146491\nENSMUSG00000030336\n\n\nENSMUSG00000016494\n5\n0\n0\n-3.056730\n-3.056730\nENSMUSG00000016494\n\n\nENSMUSG00000002808\n6\n0\n0\n3.000810\n3.000810\nENSMUSG00000002808\n\n\n\n\n\n\n\nThe dataframe results_hspc_prog$hspc is log hspc - log prog (i.e., HSPC/Prog). This means:\n\nPositive fold change: hspc is higher than prog\nNegative fold change: prog is higher than hspc\n\n\n\n\n\n\nThe results_hspc_prog$hspc dataframe. Notice the sign of the fold change is the other way\n\n\n\n\n\n\n\n\n\n\n\nTop\np.value\nFDR\nsummary.logFC\nlogFC.prog\nensembl_gene_id\n\n\n\nENSMUSG00000028639\n1\n0\n0\n-1.596910\n-1.596910\nENSMUSG00000028639\n\n\nENSMUSG00000024053\n2\n0\n0\n-3.035165\n-3.035165\nENSMUSG00000024053\n\n\nENSMUSG00000041329\n3\n0\n0\n-3.261056\n-3.261056\nENSMUSG00000041329\n\n\nENSMUSG00000030336\n4\n0\n0\n2.146491\n2.146491\nENSMUSG00000030336\n\n\nENSMUSG00000016494\n5\n0\n0\n3.056730\n3.056730\nENSMUSG00000016494\n\n\nENSMUSG00000002808\n6\n0\n0\n-3.000810\n-3.000810\nENSMUSG00000002808\n\n\n\n\n\nWe only need one of these results dataframes and we will use the prog one. It does not matter which one you use but you do need keep the direction of the foldchange in mind when interpreting the results.\nNotice that the dataframes are ordered by significance rather than the original gene order.\nIt is useful to have the normalised counts and the statistical results in one dataframe to which we will add the gene information from Ensembl. Having all the information together will make it easier to interpret the results and select genes of interest. We will need to extract the results from the list object, add the gene ids and then join with the normalised counts.\nüé¨ Extract the results dataframe from the list object and add the gene ids as a column:\n\nhspc_prog_results &lt;- data.frame(results_hspc_prog$prog, \n                                ensembl_gene_id = row.names(results_hspc_prog$prog)) \n\nüé¨ Return the ensembl gene ids as a column to the normalised counts:\n\nhspc_prog &lt;- hspc_prog |&gt;\n  rownames_to_column(var = \"ensembl_gene_id\")\n\nüé¨ Merge the results dataframe with the normalised counts:\n\n# merge the results with the normalised counts\nhspc_prog_results &lt;- hspc_prog_results |&gt;\n  left_join(hspc_prog, by = \"ensembl_gene_id\")\n\nNow go to Add gene information.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#frog-development-3",
    "href": "transcriptomics/week-4/workshop.html#frog-development-3",
    "title": "Workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\n\nI got the information from the Xenbase information pages under Data Reports | Gene Information\nThis is listed: Xenbase Gene Product Information [readme] gzipped gpi (tab separated)\nClick on the readme link to see the file format and columns\nI downloaded xenbase.gpi.gz, unzipped it, removed header lines and the Xenopus tropicalis (taxon:8364) entries and saved it as xenbase_info.xlsx\n\nIf you want to emulate what I did you can use the following commands in the terminal after downloading the file:\ngunzip xenbase.gpi.gz\nless xenbase.gpi\nq\ngunzip unzips the file and less allows you to view the file. q quits the viewer. You will see the header lines and that the file contains both Xenopus tropicalis and Xenopus laevis. I read the file in with read_tsv (skipping the first header lines) then filtered out the Xenopus tropicalis entries, dropped some columns and saved the file as an excel file.\nHowever, I have already done this for you and saved the file as xenbase_info.xlsx in the meta folder. We will import this file and join it to the results dataframe.\nüé¨ Load the readxl (Wickham and Bryan 2023) package:\n\nlibrary(readxl)\n\nüé¨ Import the Xenbase gene information file:\n\ngene_info &lt;- read_excel(\"meta/xenbase_info.xlsx\") \n\nYou should view the resulting dataframe to see what information is available. You can use glimpse() or View().\nüé¨ Merge the gene information with the results:\n\n# join the gene info with the results\ns30_results &lt;- s30_results |&gt;\n  left_join(gene_info, by = \"xenbase_gene_id\")\n\nüé¨ Save the results to a file:\n\nwrite_csv(s30_results, file = \"results/s30_results.csv\")",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#arabidopisis-3",
    "href": "transcriptomics/week-4/workshop.html#arabidopisis-3",
    "title": "Workshop",
    "section": "üéÑ Arabidopisis\n",
    "text": "üéÑ Arabidopisis\n\nEnsembl (Martin et al. 2023; Birney et al. 2004)is a bioinformatics project to organise all the biological information around the sequences of large genomes. The are a large number of databases and BioMart (Smedley et al. 2009) provides a consistent interface to the material. There are web-based tools to use these but the R package biomaRt (Durinck et al. 2009, 2005) gives you programmatic access making it easier to integrate information into R dataframes.\nüé¨ Load the biomaRt (Durinck et al. 2009, 2005) package:\n\nlibrary(biomaRt)\n\nThe biomaRt package includes a function to list all the available datasets\nüé¨ List the Ensembl ‚Äúmarts‚Äù available:\n\nlistEnsemblGenomes()\n\n              biomart                        version\n1       protists_mart      Ensembl Protists Genes 60\n2 protists_variations Ensembl Protists Variations 60\n3          fungi_mart         Ensembl Fungi Genes 60\n4    fungi_variations    Ensembl Fungi Variations 60\n5        metazoa_mart       Ensembl Metazoa Genes 60\n6  metazoa_variations  Ensembl Metazoa Variations 60\n7         plants_mart        Ensembl Plants Genes 60\n8   plants_variations   Ensembl Plants Variations 60\n\n\nplants_mart looks like the one we want. We can see what genomes are available with names like ‚ÄúArabidopsis‚Äù in this mart using the searchDatasets() function.\nüé¨\n\nsearchDatasets(useEnsemblGenomes(biomart = \"plants_mart\"), \n               pattern = \"Arabidopsis\")\n\n             dataset                         description version\n4   ahalleri_eg_gene Arabidopsis halleri genes (Ahal2.2) Ahal2.2\n6    alyrata_eg_gene    Arabidopsis lyrata genes (v.1.0)   v.1.0\n11 athaliana_eg_gene Arabidopsis thaliana genes (TAIR10)  TAIR10\n\n\nathaliana_eg_gene is the Arabidopsis thaliana genes (TAIR10) dataset we want.\nüé¨ Connect to the athaliana_eg_gene database in plants_mart:\n\nensembl &lt;- useEnsemblGenomes(biomart = \"plants_mart\",\n                             dataset = \"athaliana_eg_gene\")\n\nüé¨ See the the types of information we can retrieve:\n\nlistAttributes(mart = ensembl) |&gt; View()\n\nThere are many (1,714!) possible bits of information (attributes) that can be obtained.\nWe use the getBM() function to retrieve information from the database. The filters argument is used to specified what kind of identifier we are supplying in values to retrieve information. The attributes argument is used to select the information we want to retrieve. The values argument is used to specify the identifiers. The mart argument is used to specify the connection we created.\nüé¨ Get the the gene name and a description. We also retreive the gene id so we can later join the information with the results:\n\ngene_info &lt;- getBM(filters = \"ensembl_gene_id\",\n                   attributes = c(\"ensembl_gene_id\",\n                                  \"external_gene_name\",\n                                  \"description\"),\n                   values = wild_results$gene_id,\n                   mart = ensembl)\n\nYou should view the resulting dataframe to see what information is available. You can use glimpse() or View().\nüé¨ Merge the gene information with the results:\n\n# join the gene info with the results\nwild_results &lt;- wild_results |&gt;\n  left_join(gene_info,\n            by = join_by(gene_id == ensembl_gene_id))\n\nüé¨ Save the results to a file:\n\nwrite_csv(wild_results, file = \"results/wild_results.csv\")",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#leishmania-3",
    "href": "transcriptomics/week-4/workshop.html#leishmania-3",
    "title": "Workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\n\nI got the information from TriTrypDB\nwhich is a functional genomic resource for the Trypanosomatidae and Plasmodidae\nhttps://tritrypdb.org/tritrypdb/app/downloads section\nI downloaded the L. mexicana MHOM/GT/2001/U1103 Full GFF and extracted the gene information and saved it as leishmania_mex.xlsx\n\nWe will import this file and join it to the results dataframe.\nüé¨ Load the readxl (Wickham and Bryan 2023) package:\n\nlibrary(readxl)\n\nüé¨ Import the Xenbase gene information file:\n\ngene_info &lt;- read_excel(\"meta/leishmania_mex.xlsx\") \n\nYou should view the resulting dataframe to see what information is available. You can use glimpse() or View().\nüé¨ Merge the gene information with the results:\n\n# join the gene info with the results\npro_meta_results &lt;- pro_meta_results |&gt;\n  left_join(gene_info, by = \"gene_id\")\n\nüé¨ Save the results to a file:\n\nwrite_csv(pro_meta_results, file = \"results/pro_meta_results.csv\")",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#stem-cells-3",
    "href": "transcriptomics/week-4/workshop.html#stem-cells-3",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nEnsembl (Martin et al. 2023; Birney et al. 2004)is a bioinformatics project to organise all the biological information around the sequences of large genomes. The are a large number of databases but BioMart (Smedley et al. 2009) provides a consistent interface to the material. There are web-based tools to use these but the R package biomaRt (Durinck et al. 2009, 2005) gives you programmatic access making it easier to integrate information into R dataframes\nüé¨ Load the biomaRt (Durinck et al. 2009, 2005) package:\n\nlibrary(biomaRt)\n\nüé¨ Connect to the mouse database\n\n# Connect to the mouse database\n\nensembl &lt;- useMart(biomart = \"ensembl\", \n                   dataset = \"mmusculus_gene_ensembl\")\n\nüé¨ See information we can retrieve:\n\n# See what information we can retrieve\nlistAttributes(mart = ensembl) |&gt; View()\n\nThere are many (2,988!) possible bits of information (attributes) that can be obtained.\nWe use the getBM() function to retrieve information from the database. The filters argument is used to specified what kind of identifier we are supplying in values to retrieve information. The attributes argument is used to select the information we want to retrieve. The values argument is used to specify the identifiers. The mart argument is used to specify the connection we created.\nüé¨ Get the the gene name and a description. We also retrieve the gene id so we can later join the information with the results:\n\ngene_info &lt;- getBM(filters = \"ensembl_gene_id\",\n                   attributes = c(\"ensembl_gene_id\",\n                                  \"external_gene_name\",\n                                  \"description\"),\n                   values = hspc_prog_results$ensembl_gene_id,\n                   mart = ensembl)\n\nYou should view the resulting dataframe to see what information is available. You can use glimpse() or View(). Notice the dataframe returned only has 279 rows - one of the ids does not have information.\nüé¨ Merge the gene information with the results:\n\n# join the gene info with the results\nhspc_prog_results &lt;- hspc_prog_results |&gt; \n  left_join(gene_info, by = \"ensembl_gene_id\")\n\nüé¨ Save the results to a file:\n\nwrite_csv(hspc_prog_results, file = \"results/hspc_prog_results.csv\")",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/workshop.html#footnotes",
    "href": "transcriptomics/week-4/workshop.html#footnotes",
    "title": "Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\nBioconductor is a project that develops and supports R packages for bioinformatics.‚Ü©Ô∏é\nBioconductor is a project that develops and supports R packages for bioinformatics.‚Ü©Ô∏é\nBioconductor is a project that develops and supports R packages for bioinformatics.‚Ü©Ô∏é",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/overview.html",
    "href": "transcriptomics/week-3/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week you will meet your data. The independent study will summarise how these data were generated and how they have been processed before being given to you. There will also be an overview of the analysis we will carry out over three workshops. In the workshop, you will learn what steps to take to get a good understanding of transcriptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It gives you the deep understanding of the data structures and values that you will need to code and trouble-shoot code, allows you to spot failed or problematic samples and informs your decisions on quality control.\nWe suggest you sit together with your group in the workshop.\n\nLearning objectives\nThe successful student will be able to:\n\nexplore transcriptomics data to find the number of rows and columns and know how these correspond to samples and variables\nexplore the distribution of expression measures across whole data sets, across variables and across samples by summarising and plotting\nexplain what distributions are expected and interpret the distributions they have\nexplain on what basis we might filter out variables or samples\nimport, explore and filter transcriptomics data reproducibly so they can understand and reuse their code in the future\n\n\n\nInstructions\n\nPrepare\n\nüìñ Read how the data were generated and how they have been processed so far and a summary of the analysis we will carry out over three workshops.\n\nWorkshop\n\nüíª Set up a Project\nüíª Import data\nüíª Explore the distribution of values across rows and columns\nüíª Look after future you!\n\nConsolidate\n\nüíª Use the work you completed in the workshop as a template to apply to a new case.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "About"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html",
    "href": "transcriptomics/week-3/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop you will learn what steps to take to get a good understanding of your transcriptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It gives you the deep understanding of the data structures and values that you will need to code and trouble-shoot code, allows you to spot failed or problematic samples and informs your decisions on quality control.\nIn this session, you should examine all four data sets because the comparisons will give you a much stronger understanding of your own project data. Compare and contrast is a very useful way to build understanding.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#session-overview",
    "href": "transcriptomics/week-3/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop you will learn what steps to take to get a good understanding of your transcriptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It gives you the deep understanding of the data structures and values that you will need to code and trouble-shoot code, allows you to spot failed or problematic samples and informs your decisions on quality control.\nIn this session, you should examine all four data sets because the comparisons will give you a much stronger understanding of your own project data. Compare and contrast is a very useful way to build understanding.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#frog-development",
    "href": "transcriptomics/week-3/workshop.html#frog-development",
    "title": "Workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\nImport\nImport the data for stage 30.\nüé¨ Import xlaevis_counts_S30.csv\n\n# üê∏ import the s30 data\ns30 &lt;- read_csv(\"data-raw/xlaevis_counts_S30.csv\")\n\nüé¨ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in ‚Äòtidy‚Äô format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nüé¨ Pivot the counts (stack the columns) so all the counts are in a single column (count) labelled in sample by the column it came from and pipe into ggplot() to create a histogram:\n\ns30 |&gt;\n  pivot_longer(cols = -xenbase_gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = count)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis data is very skewed - there are very many low counts and a very few higher numbers. It is hard to see the very low bars for the higher values. Logging the counts is a way to make the distribution more visible. You cannot take the log of 0 so we add 1 to the count before logging. The log of 1 is zero so we will be able to see how many zeros we had.\nüé¨ Repeat the plot of log of the counts.\n\ns30 |&gt;\n  pivot_longer(cols = -xenbase_gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = log10(count + 1))) +\n  geom_histogram()\n\n\n\n\n\n\n\nI‚Äôve used base 10 only because it easy to convert to the original scale (1 is 10, 2 is 100, 3 is 1000 etc). Notice we have a peak at zero indicating there are many zeros. We would expect the distribution of counts to be roughly log normal because this is expression of all the genes in the genome1. The number of low counts is inflated (small peak near the low end). This suggests that these lower counts might be false positives. The removal of low counts is a common processing step in ‚Äôomic data. We will revisit this after we have considered the distribution of counts across samples and genes.\nDistribution of values across the samples\nSummary statistics including the the number of NAs can be seen using the summary(). It is most helpful which you have up to about 25 columns. There is nothing special about the number 25, it is just that summaries of a larger number of columns are difficult to grasp.\nüé¨ Get a quick overview of the 7 columns:\n\n# examine all the columns quickly\n# works well with smaller numbers of column\nsummary(s30)\n\n xenbase_gene_id       S30_C_1            S30_C_2            S30_C_3        \n Length:11893       Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n Class :character   1st Qu.:    14.0   1st Qu.:    14.0   1st Qu.:    23.0  \n Mode  :character   Median :    70.0   Median :    75.0   Median :   107.0  \n                    Mean   :   317.1   Mean   :   335.8   Mean   :   426.3  \n                    3rd Qu.:   205.0   3rd Qu.:   220.0   3rd Qu.:   301.0  \n                    Max.   :101746.0   Max.   :118708.0   Max.   :117945.0  \n    S30_F_1            S30_F_2            S30_F_3       \n Min.   :     0.0   Min.   :     0.0   Min.   :    0.0  \n 1st Qu.:    19.0   1st Qu.:    17.0   1st Qu.:   16.0  \n Median :    88.0   Median :    84.0   Median :   69.0  \n Mean   :   376.2   Mean   :   376.5   Mean   :  260.4  \n 3rd Qu.:   251.0   3rd Qu.:   246.0   3rd Qu.:  187.0  \n Max.   :117573.0   Max.   :130672.0   Max.   :61531.0  \n\n\nNotice that:\n\nthe minimum count is 0 and the maximums are very high in all the columns\nthe medians are quite a lot lower than the means so the data are skewed (hump to the left, tail to the right) and there must be quite a lot of zeros\n\nS30_F_3 does have a somewhat lower maximum count\n\nWe want to know how many zeros there are in each a column. To achieve this, we can make use of the fact that TRUE evaluates to 1 and FALSE evaluates to 0. Consequently, summing a column of TRUE/FALSE values will give you the number of TRUE values. For example, sum(S30_C_1 &gt; 0) gives the number of values above zero in the S30_C_1 column. If you wanted the number of zeros, you could use sum(S30_C_1 == 0).\nüé¨ Find the number values above zero in all six columns:\n\ns30 |&gt;\n  summarise(sum(S30_C_1 &gt; 0),\n            sum(S30_C_2 &gt; 0),\n            sum(S30_C_3 &gt; 0),\n            sum(S30_F_1 &gt; 0),\n            sum(S30_F_2 &gt; 0),\n            sum(S30_F_3 &gt; 0))\n\n# A tibble: 1 √ó 6\n  `sum(S30_C_1 &gt; 0)` `sum(S30_C_2 &gt; 0)` `sum(S30_C_3 &gt; 0)` `sum(S30_F_1 &gt; 0)`\n               &lt;int&gt;              &lt;int&gt;              &lt;int&gt;              &lt;int&gt;\n1              10553              10532              10895              10683\n# ‚Ñπ 2 more variables: `sum(S30_F_2 &gt; 0)` &lt;int&gt;, `sum(S30_F_3 &gt; 0)` &lt;int&gt;\n\n\nThere is a better way of doing this that saves you having to repeat so much code - very useful if you have a lot more than 6 columns! We can use pivot_longer() to put the data in tidy format and then use the group_by() and summarise() approach we have used extensively before.\nüé¨ Find the number of zeros in all columns:\n\ns30 |&gt;\n  pivot_longer(cols = -xenbase_gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(n_above_zero = sum(count &gt; 0))\n\n# A tibble: 6 √ó 2\n  sample  n_above_zero\n  &lt;chr&gt;          &lt;int&gt;\n1 S30_C_1        10553\n2 S30_C_2        10532\n3 S30_C_3        10895\n4 S30_F_1        10683\n5 S30_F_2        10694\n6 S30_F_3        10930\n\n\nYou could expand this code to get get other useful summary information\nüé¨ Summarise all the samples:\n\ns30 |&gt;\n  pivot_longer(cols = -xenbase_gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(min = min(count),\n            lowerq = quantile(count, 0.25),\n            mean = mean(count),\n            median = median(count),\n            upperq = quantile(count, 0.75),\n            max = max(count),\n            n_above_zero = sum(count &gt; 0))\n\n# A tibble: 6 √ó 8\n  sample    min lowerq  mean median upperq    max n_above_zero\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;int&gt;\n1 S30_C_1     0     14  317.     70    205 101746        10553\n2 S30_C_2     0     14  336.     75    220 118708        10532\n3 S30_C_3     0     23  426.    107    301 117945        10895\n4 S30_F_1     0     19  376.     88    251 117573        10683\n5 S30_F_2     0     17  376.     84    246 130672        10694\n6 S30_F_3     0     16  260.     69    187  61531        10930\n\n\nThe mean count ranges from 260 to 426. S30_F_3 does stand out a little but not by too much. If we had more replicates we might consider conducting our analysis both with and without this replicate to determine whether its oddness was influencing our conclusions. Since we have just 3 replicates, we will leave it in. The potential effect of an odd replicate is reduced statistical power. Major differences in gene expression will still be uncovered. Differences between genes with lower average expression and or more variable expression might be missed. Whether this matters depends on the biological question you are asking. In this case, it does not matter because the major differences in gene expression will be enough.\nüé¨ Save the summary as a dataframe, s30_summary_samp (using assignment).\nWe can also plot the distribution of counts across samples. We have many\nvalues (11893) so we are not limited to using geom_histogram(). geom_density() gives us a smooth distribution.\nüé¨ Plot the log10 of the counts + 1 again but this time facet by the sample:\n\ns30 |&gt;\n  pivot_longer(cols = -xenbase_gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(log10(count + 1))) +\n  geom_density() +\n  facet_wrap(. ~ sample, nrow = 3)\n\n\n\n\n\n\n\nThe key information to take from these plots is:\n\nthe distributions are roughly similar though S30_F_3 does stand out a little\nthe peak at zero suggests quite a few counts of 1.\nwe would expect the distribution of counts in each sample to be roughly log normal so that the small rise near the low end, even before the peak at zero, suggests that these lower counts might be anomalies.\n\nWe have found the distribution across samples to be similar to that over all. This is good because it means that the samples are fairly consistent with each other. We can now move on to the next step.\nDistribution of values across the genes\nThere are lots of genes in this dataset therefore we will take a slightly different approach. We would not want to use plot a distribution for each gene in the same way. Will pivot the data to tidy and then summarise the counts for each gene.\nüé¨ Summarise the counts for each gene and save the result as s30_summary_gene. Include the same columns as we had in the by sample summary (s30_summary_samp) and an additional column, total for the total number of counts for each gene.\nüé¨ View the s30_summary_gene dataframe.\nNotice that we have:\n\na lot of genes with counts of zero in every sample\na lot of genes with zero counts in several of the samples\nsome very very low counts.\n\nGenes with very low counts should be filtered out because they are unreliable - or, at the least, uninformative. The goal of our downstream analysis will be to see if there is a significant difference in gene expression between the control and FGF-treated sibling. Since we have only three replicates in each group, having one or two unreliable, missing or zero values, makes such a determination impossible for a particular gene. We will use the total counts (total) and the number of samples with non-zero values (n_above_zero) in this dataframe to filter our genes later.\nAs we have a lot of genes, it is helpful to plot the mean counts with geom_pointrange() to get an overview of the distributions. We will again plot the log of the mean counts. We will also order the genes from lowest to highest mean count.\nüé¨ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\ns30_summary_gene |&gt; \n  ggplot(aes(x = reorder(xenbase_gene_id, mean), y = log10(mean))) +\n  geom_pointrange(aes(ymin = log10(mean - sd), \n                      ymax = log10(mean + sd )),\n                  size = 0.1)\n\n\n\n\n\n\n\n(Note the warning is expected since we have zero means).\nYou can see we also have quite a few genes with means less than 1 (log below zero). Note that the variability between genes (average counts between 0 and 102586) is far greater than between samples (average counts from 260 to 426) which is exactly what we would expect to see.\nNow go to Filtering for QC.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#arabidopsis",
    "href": "transcriptomics/week-3/workshop.html#arabidopsis",
    "title": "Workshop",
    "section": "üéÑ Arabidopsis\n",
    "text": "üéÑ Arabidopsis\n\nImport\nImport the data for wildtype plants.\nüé¨ Import arabidopsis-wild.csv\n\n# üéÑ import the wild data\nwild &lt;- read_csv(\"data-raw/arabidopsis-wild.csv\")\n\nüé¨ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in ‚Äòtidy‚Äô format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nüé¨ Pivot the counts (stack the columns) so all the counts are in a single column (count) labelled in sample by the column it came from and pipe into ggplot() to create a histogram:\n\nwild |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = count)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis data is very skewed - there are very many low counts and a very few higher numbers. It is hard to see the very low bars for the higher values. Logging the counts is a way to make the distribution more visible. You cannot take the log of 0 so we add 1 to the count before logging. The log of 1 is zero so we will be able to see how many zeros we had.\nüé¨ Repeat the plot of log of the counts.\n\nwild |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = log10(count + 1))) +\n  geom_histogram()\n\n\n\n\n\n\n\nI‚Äôve used base 10 only because it easy to convert to the original scale (1 is 10, 2 is 100, 3 is 1000 etc). Notice we have a peak at zero indicating there are many zeros. We would expect the distribution of counts to be roughly log normal because this is expression of all the genes in the genome2. The number of low counts is inflated (small peak near the low end). This suggests that these lower counts might be false positives. The removal of low counts is a common processing step in ‚Äôomic data. We will revisit this after we have considered the distribution of counts across samples and genes.\nDistribution of values across the samples\nSummary statistics including the the number of NAs can be seen using the summary(). It is most helpful which you have up to about 25 columns. There is nothing special about the number 25, it is just that summaries of a larger number of columns are difficult to grasp.\nüé¨ Get a quick overview of the 5 columns:\n\n# examine all the columns quickly\n# works well with smaller numbers of column\nsummary(wild)\n\n   gene_id          SRX028956_wild_suf SRX028957_wild_def SRX028960_wild_suf\n Length:32833       Min.   :    0.0    Min.   :    0.00   Min.   :    0.0   \n Class :character   1st Qu.:    6.0    1st Qu.:    2.00   1st Qu.:   15.0   \n Mode  :character   Median :   29.0    Median :   15.00   Median :   76.0   \n                    Mean   :  112.3    Mean   :   70.27   Mean   :  295.5   \n                    3rd Qu.:   99.0    3rd Qu.:   63.00   3rd Qu.:  263.0   \n                    Max.   :38287.0    Max.   :24439.00   Max.   :80527.0   \n SRX028961_wild_def\n Min.   :    0.0   \n 1st Qu.:    6.0   \n Median :   37.0   \n Mean   :  173.4   \n 3rd Qu.:  151.0   \n Max.   :58548.0   \n\n\nNotice that:\n\nthe minimum count is 0 and the maximums are very high in all the columns\nthe medians are quite a lot lower than the means so the data are skewed (hump to the left, tail to the right) and there must be quite a lot of zeros\n\nWe want to know how many zeros there are in each a column. To achieve this, we can make use of the fact that TRUE evaluates to 1 and FALSE evaluates to 0. Consequently, summing a column of TRUE/FALSE values will give you the number of TRUE values. For example, sum(SRX028961_wild_def &gt; 0) gives the number of values above zero in the SRX028961_wild_def column. If you wanted the number of zeros, you could use sum(SRX028961_wild_def == 0).\nüé¨ Find the number values above zero in all six columns:\n\nwild |&gt;\n  summarise(sum(SRX028961_wild_def &gt; 0),\n            sum(SRX028957_wild_def &gt; 0),\n            sum(SRX028960_wild_suf &gt; 0),\n            sum(SRX028956_wild_suf &gt; 0))\n\n# A tibble: 1 √ó 4\n  `sum(SRX028961_wild_def &gt; 0)` sum(SRX028957_wild_def ‚Ä¶¬π sum(SRX028960_wild_s‚Ä¶¬≤\n                          &lt;int&gt;                     &lt;int&gt;                  &lt;int&gt;\n1                         29712                     28015                  30946\n# ‚Ñπ abbreviated names: ¬π‚Äã`sum(SRX028957_wild_def &gt; 0)`,\n#   ¬≤‚Äã`sum(SRX028960_wild_suf &gt; 0)`\n# ‚Ñπ 1 more variable: `sum(SRX028956_wild_suf &gt; 0)` &lt;int&gt;\n\n\nThere is a better way of doing this that saves you having to repeat so much code - very useful if you have a lot more than 6 columns! We can use pivot_longer() to put the data in tidy format and then use the group_by() and summarise() approach we have used extensively before.\nüé¨ Find the number of zeros in all columns:\n\nwild |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(n_above_zero = sum(count &gt; 0))\n\n# A tibble: 4 √ó 2\n  sample             n_above_zero\n  &lt;chr&gt;                     &lt;int&gt;\n1 SRX028956_wild_suf        29997\n2 SRX028957_wild_def        28015\n3 SRX028960_wild_suf        30946\n4 SRX028961_wild_def        29712\n\n\nYou could expand this code to get get other useful summary information\nüé¨ Summarise all the samples:\n\nwild |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(min = min(count),\n            lowerq = quantile(count, 0.25),\n            mean = mean(count),\n            median = median(count),\n            upperq = quantile(count, 0.75),\n            max = max(count),\n            n_above_zero = sum(count &gt; 0))\n\n# A tibble: 4 √ó 8\n  sample               min lowerq  mean median upperq   max n_above_zero\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;        &lt;int&gt;\n1 SRX028956_wild_suf     0      6 112.      29     99 38287        29997\n2 SRX028957_wild_def     0      2  70.3     15     63 24439        28015\n3 SRX028960_wild_suf     0     15 296.      76    263 80527        30946\n4 SRX028961_wild_def     0      6 173.      37    151 58548        29712\n\n\nThe mean count ranges from 70 to 296. It is difficult to determine whether any replicates are ‚Äúunusual‚Äù when there are only two replicates. The potential effect of only two replicates, or of an an odd replicate when you have more replicates, is reduced statistical power. Major differences in gene expression will still be uncovered. Differences between genes with lower average expression and or more variable expression might be missed. Whether this matters depends on the biological question you are asking. In this case, it does not matter because the major differences in gene expression will be enough.\nüé¨ Save the summary as a dataframe, wild_summary_samp (using assignment).\nWe can also plot the distribution of counts across samples. We have many values (32833) so we are not limited to using geom_histogram(). geom_density() gives us a smooth distribution.\nüé¨ Plot the log10 of the counts + 1 again but this time facet by the sample:\n\nwild |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(log10(count + 1))) +\n  geom_density() +\n  facet_wrap(. ~ sample, nrow = 3)\n\n\n\n\n\n\n\nThe key information to take from these plots is:\n\ndifficult to say was is usual/unusual with 2 replicates\nthe peak at zero suggests quite a few counts of 1.\nwe would expect the distribution of counts in each sample to be roughly log normal so that the rise near the low end, even before the peak at zero, suggests that these lower counts might be anomalies.\n\nWe have found the distribution across samples to be similar to that over all. This is good because it means that the samples are fairly consistent with each other. We can now move on to the next step.\nDistribution of values across the genes\nThere are lots of genes in this dataset therefore we will take a slightly different approach. We would not want to use plot a distribution for each gene in the same way. Will pivot the data to tidy and then summarise the counts for each gene.\nüé¨ Summarise the counts for each gene and save the result as wild_summary_gene. Include the same columns as we had in the by sample summary (wild_summary_samp) and an additional column, total for the total number of counts for each gene.\nüé¨ View the wild_summary_gene dataframe.\nNotice that we have:\n\na lot of genes with counts of zero in every sample\na lot of genes with zero counts in several of the samples\nsome very very low counts.\n\nGenes with very low counts should be filtered out because they are unreliable - or, at the least, uninformative. The goal of our downstream analysis will be to see if there is a significant difference in gene expression between the control and FGF-treated sibling. Since we have only three replicates in each group, having one or two unreliable, missing or zero values, makes such a determination impossible for a particular gene. We will use the total counts (total) and the number of samples with non-zero values (n_above_zero) in this dataframe to filter our genes later.\nAs we have a lot of genes, it is helpful to plot the mean counts with geom_pointrange() to get an overview of the distributions. We will again plot the log of the mean counts. We will also order the genes from lowest to highest mean count.\nüé¨ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\nwild_summary_gene |&gt; \n  ggplot(aes(x = reorder(gene_id, mean), y = log10(mean))) +\n  geom_pointrange(aes(ymin = log10(mean - sd), \n                      ymax = log10(mean + sd )),\n                  size = 0.1)\n\n\n\n\n\n\n\n(Note the warning is expected since we have zero means).\nYou can see we also have quite a few genes with means less than 1 (log below zero). Note that the variability between genes (average counts between 0 and 43348) is far greater than between samples (average counts from 70 to 296) which is exactly what we would expect to see.\nNow go to Filtering for QC.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#leishmania",
    "href": "transcriptomics/week-3/workshop.html#leishmania",
    "title": "Workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\nImport\nImport the data for L.mexicana procyclic promastigote (pro) and the metacyclic promastigotes (meta)\nüé¨ Import leishmania-mex-pro.csv and leishmania-mex-meta.csv\n\n# üíâ import the pro and meta leish data\npro &lt;- read_csv(\"data-raw/leishmania-mex-pro.csv\")\nmeta &lt;- read_csv(\"data-raw/leishmania-mex-meta.csv\")\n\nWe will need to combine the two sets of columns (datasets) so we can compare the two stages. We will join them using gene_id to match the rows. The column names differ so we don‚Äôt need to worry about renaming any of them.\nüé¨ Combine the two datasets by gene_id and save the result as pro_meta.\n\n#  combine the two datasets\npro_meta &lt;- pro |&gt;\n  left_join(meta, \n            by = \"gene_id\")\n\nüé¨ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in ‚Äòtidy‚Äô format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nüé¨ Pivot the counts (stack the columns) so all the counts are in a single column (count) labelled in sample by the column it came from and pipe into ggplot() to create a histogram:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = count)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis data is very skewed - there are very many low counts and a very few higher numbers. It is hard to see the very low bars for the higher values. Logging the counts is a way to make the distribution more visible. You cannot take the log of 0 so we add 1 to the count before logging. The log of 1 is zero so we will be able to see how many zeros we had.\nüé¨ Repeat the plot of log of the counts.\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(x = log10(count + 1))) +\n  geom_histogram()\n\n\n\n\n\n\n\nI‚Äôve used base 10 only because it easy to convert to the original scale (1 is 10, 2 is 100, 3 is 1000 etc). Notice we have a peak at zero indicating there are many zeros. We would expect the distribution of counts to be roughly log normal because this is expression of all the genes in the genome3. The number of low counts is inflated (small peak near the low end). This suggests that these lower counts might be false positives. The removal of low counts is a common processing step in ‚Äôomic data. We will revisit this after we have considered the distribution of counts across samples and genes.\nDistribution of values across the samples\nSummary statistics including the the number of NAs can be seen using the summary(). It is most helpful which you have up to about 25 columns. There is nothing special about the number 25, it is just that summaries of a larger number of columns are difficult to grasp.\nüé¨ Get a quick overview of the 7 columns:\n\n# examine all the columns quickly\n# works well with smaller numbers of column\nsummary(pro_meta)\n\n   gene_id             lm_pro_1           lm_pro_2           lm_pro_3       \n Length:8677        Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n Class :character   1st Qu.:    77.0   1st Qu.:    53.0   1st Qu.:    59.0  \n Mode  :character   Median :   191.0   Median :   135.0   Median :   145.0  \n                    Mean   :   364.5   Mean   :   255.7   Mean   :   281.4  \n                    3rd Qu.:   332.0   3rd Qu.:   238.0   3rd Qu.:   256.0  \n                    Max.   :442477.0   Max.   :295423.0   Max.   :411663.0  \n   lm_meta_1          lm_meta_2          lm_meta_3       \n Min.   :     0.0   Min.   :     0.0   Min.   :     0.0  \n 1st Qu.:    48.0   1st Qu.:    51.0   1st Qu.:    78.0  \n Median :   110.0   Median :   120.0   Median :   187.0  \n Mean   :   220.3   Mean   :   221.9   Mean   :   355.9  \n 3rd Qu.:   197.0   3rd Qu.:   215.0   3rd Qu.:   341.0  \n Max.   :244569.0   Max.   :205203.0   Max.   :498303.0  \n\n\nNotice that:\n\nthe minimum count is 0 and the maximums are very high in all the columns\nthe medians are quite a lot lower than the means so the data are skewed (hump to the left, tail to the right) and there must be quite a lot of zeros\n\nWe want to know how many zeros there are in each a column. To achieve this, we can make use of the fact that TRUE evaluates to 1 and FALSE evaluates to 0. Consequently, summing a column of TRUE/FALSE values will give you the number of TRUE values. For example, sum(lm_pro_1 &gt; 0) gives the number of values above zero in the lm_pro_1 column. If you wanted the number of zeros, you could use sum(lm_pro_1 == 0).\nüé¨ Find the number values above zero in all six columns:\n\npro_meta |&gt;\n  summarise(sum(lm_pro_1 &gt; 0),\n            sum(lm_pro_2 &gt; 0),\n            sum(lm_pro_3 &gt; 0),\n            sum(lm_meta_1 &gt; 0),\n            sum(lm_meta_2 &gt; 0),\n            sum(lm_meta_3 &gt; 0))\n\n# A tibble: 1 √ó 6\n  `sum(lm_pro_1 &gt; 0)` `sum(lm_pro_2 &gt; 0)` `sum(lm_pro_3 &gt; 0)`\n                &lt;int&gt;               &lt;int&gt;               &lt;int&gt;\n1                8549                8522                8509\n# ‚Ñπ 3 more variables: `sum(lm_meta_1 &gt; 0)` &lt;int&gt;, `sum(lm_meta_2 &gt; 0)` &lt;int&gt;,\n#   `sum(lm_meta_3 &gt; 0)` &lt;int&gt;\n\n\nThere is a better way of doing this that saves you having to repeat so much code - very useful if you have a lot more than 6 columns! We can use pivot_longer() to put the data in tidy format and then use the group_by() and summarise() approach we have used extensively before.\nüé¨ Find the number of zeros in all columns:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(n_above_zero = sum(count &gt; 0))\n\n# A tibble: 6 √ó 2\n  sample    n_above_zero\n  &lt;chr&gt;            &lt;int&gt;\n1 lm_meta_1         8535\n2 lm_meta_2         8535\n3 lm_meta_3         8530\n4 lm_pro_1          8549\n5 lm_pro_2          8522\n6 lm_pro_3          8509\n\n\nYou could expand this code to get get other useful summary information\nüé¨ Summarise all the samples:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  group_by(sample) |&gt;\n  summarise(min = min(count),\n            lowerq = quantile(count, 0.25),\n            mean = mean(count),\n            median = median(count),\n            upperq = quantile(count, 0.75),\n            max = max(count),\n            n_above_zero = sum(count &gt; 0))\n\n# A tibble: 6 √ó 8\n  sample      min lowerq  mean median upperq    max n_above_zero\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;int&gt;\n1 lm_meta_1     0     48  220.    110    197 244569         8535\n2 lm_meta_2     0     51  222.    120    215 205203         8535\n3 lm_meta_3     0     78  356.    187    341 498303         8530\n4 lm_pro_1      0     77  364.    191    332 442477         8549\n5 lm_pro_2      0     53  256.    135    238 295423         8522\n6 lm_pro_3      0     59  281.    145    256 411663         8509\n\n\nThe mean count ranges from 220 to 364. We do not appear to have any outlying (odd) replicates. The potential effect of an odd replicate is reduced statistical power. Major differences in gene expression will still be uncovered. Differences between genes with lower average expression and or more variable expression might be missed. Whether this matters depends on the biological question you are asking.\nüé¨ Save the summary as a dataframe, pro_meta_summary_samp (using assignment).\nWe can also plot the distribution of counts across samples. We have many values (8677) so we are not limited to using geom_histogram(). geom_density() gives us a smooth distribution.\nüé¨ Plot the log10 of the counts + 1 again but this time facet by the sample:\n\npro_meta |&gt;\n  pivot_longer(cols = -gene_id,\n               names_to = \"sample\",\n               values_to = \"count\") |&gt;\n  ggplot(aes(log10(count + 1))) +\n  geom_density() +\n  facet_wrap(. ~ sample, nrow = 3)\n\n\n\n\n\n\n\nThe key information to take from these plots is:\n\nthe distributions are roughly similar\nthe peak at zero suggests quite a few counts of 1.\nwe would expect the distribution of counts in each sample to be roughly log normal so that the small rise near the low end, even before the peak at zero, suggests that these lower counts might be anomalies.\n\nWe have found the distribution across samples to be similar to that over all. This is good because it means that the samples are fairly consistent with each other. We can now move on to the next step.\nDistribution of values across the genes\nThere are lots of genes in this dataset therefore we will take a slightly different approach. We would not want to use plot a distribution for each gene in the same way. Will pivot the data to tidy and then summarise the counts for each gene.\nüé¨ Summarise the counts for each gene and save the result as pro_meta_summary_gene. Include the same columns as we had in the by sample summary (pro_meta_summary_samp) and an additional column, total for the total number of counts for each gene.\nüé¨ View the pro_meta_summary_gene dataframe.\nNotice that we have:\n\na lot of genes with counts of zero in every sample\na lot of genes with zero counts in several of the samples\nsome very very low counts.\n\nGenes with very low counts should be filtered out because they are unreliable - or, at the least, uninformative. The goal of our downstream analysis will be to see if there is a significant difference in gene expression between the stages. Since we have only three replicates in each group, having one or two unreliable, missing or zero values, makes such a determination impossible for a particular gene. We will use the total counts (total) and the number of samples with non-zero values (n_above_zero) in this dataframe to filter our genes later.\nAs we have a lot of genes, it is helpful to plot the mean counts with geom_pointrange() to get an overview of the distributions. We will again plot the log of the mean counts. We will also order the genes from lowest to highest mean count.\nüé¨ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\npro_meta_summary_gene |&gt; \n  ggplot(aes(x = reorder(gene_id, mean), y = log10(mean))) +\n  geom_pointrange(aes(ymin = log10(mean - sd), \n                      ymax = log10(mean + sd )),\n                  size = 0.1)\n\n\n\n\n\n\n\n(Note the warning is expected since we have zero means).\nYou can see we also have quite a few genes with means less than 1 (log below zero). Note that the variability between genes (average counts between 0 and 349606) is far greater than between samples (average counts from 220 to 364) which is exactly what we would expect to see.\nNow go to Filtering for QC.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#stem-cells",
    "href": "transcriptomics/week-3/workshop.html#stem-cells",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nImport\nImport the data for the HSPC and the Progenitor cells.\nüé¨ Import surfaceome_hspc.csv and surfaceome_hspc.csv\n\n# üê≠ import the hspc and prog data\nhspc &lt;- read_csv(\"data-raw/surfaceome_hspc.csv\")\nprog &lt;- read_csv(\"data-raw/surfaceome_prog.csv\")\n\nWe will need to combine the two sets of columns (datasets) so we can compare the two stages. We will join them using ensembl_gene_id to match the rows. The column names differ so we don‚Äôt need to worry about renaming any of them.\nüé¨ Combine the two datasets by ensembl_gene_id and save the result as hspc_prog.\n\n#  combine the two datasets\nhspc_prog &lt;- hspc |&gt;\n  left_join(prog, \n            by = \"ensembl_gene_id\")\n\nüé¨ Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.\nDistribution of values across all the data in the file\nThe values are spread over multiple columns so in order to plot the distribution as a whole, we will need to first use pivot_longer() to put the data in ‚Äòtidy‚Äô format (Wickham 2014) by stacking the columns. We could save a copy of the stacked data and then plot it, but here, I have just piped the stacked data straight into ggplot(). This helps me avoid cluttering my R environment with temporary objects.\nüé¨ Pivot the counts (stack the columns) so all the counts are in a single column (expr) labelled in cell by the column it came from and pipe into ggplot() to create a histogram:\n\nhspc_prog |&gt;\n  pivot_longer(cols = -ensembl_gene_id,\n               names_to = \"cell\",\n               values_to = \"expr\") |&gt; \n  ggplot(aes(x = expr)) +\n  geom_histogram()\n\n\n\n\n\n\n\nThis is a very striking distribution. Is it what we are expecting? Notice we have a peak at zero indicating there are low values zeros. This inflation of low values suggests some are anomalous - they will have been derived from low counts which are likely false positives. As inaccurate measures, we will want to exclude expression values below (about) 1. We will revisit this after we have considered the distribution of expression across cells and genes.\nWhat about the bimodal appearance of the the ‚Äòreal‚Äô values? If we had the whole transcriptome we would not expect to see such a pattern - we‚Äôd expect to see a roughly normal distribution4. However, this is a subset of the genome and the nature of the subsetting has had an influence here. These are a subset of cell surface proteins that show a significant difference between at least two of twelve cell subtypes. That is, all of these genes are either ‚Äúhigh‚Äù or ‚Äúlow‚Äù leading to a bimodal distribution.\nUnlike the other three datasets, which count raw counts, these data are normalised and log2 transformed. We do not need to plot the log of the values to see the distribution - they are already logged.\nDistribution of values across the samples\nFor the other three datasets, we used the summary() function to get an overview of the columns. This works well when you have upto about 25 columns but it is not helpful here because we have a lot of cells! Feel free to try it!\nIn this data set, there is even more of an advantage of using the pivot_longer(), group_by() and summarise() approach. We will be able to open the dataframe in the Viewer and make plots to examine whether the distributions are similar across cells. The mean and the standard deviation are useful to see the distributions across cells in a plot but we will also examine the interquartile values, maximums and the number of non-zero values.\nüé¨ Summarise all the cells:\n\nhspc_prog_summary_cell &lt;- hspc_prog |&gt;\n  pivot_longer(cols = -ensembl_gene_id,\n               names_to = \"cell\",\n               values_to = \"expr\") |&gt;\n  group_by(cell) |&gt;\n  summarise(min = min(expr),\n            lowerq = quantile(expr, 0.25),\n            sd = sd(expr),\n            mean = mean(expr),\n            median = median(expr),\n            upperq = quantile(expr, 0.75),\n            max = max(expr),\n            total = sum(expr),\n            n_above_zero = sum(expr &gt; 0))\n\nüé¨ View the hspc_prog_summary_cell dataframe (click on it in the environment).\nNotice that: - a minimum value of 0 appears in all 1499 cells - the lower quartiles are all zero and so are many of the medians - there are no cells with above 0 expression in all 280 of the gene subset - the highest number of genes expressed is 208, the lowest is 94\nIn short, there are quite a lot of zeros.\nTo get a better understanding of the distribution of expressions in cells we can create a ggplot using the pointrange geom. Pointrange puts a dot at the mean and a line between a minimum and a maximum such as +/- one standard deviation. Not unlike a boxplot, but when you need the boxes too be very narrow!\nüé¨ Create a pointrange plot.\n\nhspc_prog_summary_cell |&gt; \n  ggplot(aes(x = cell, y = mean)) +\n  geom_pointrange(aes(ymin = mean - sd, \n                      ymax = mean + sd ),\n                  size = 0.1)\n\n\n\n\n\n\n\nYou will need to use the Zoom button to pop the plot window out so you can make it as wide as possible\nThe things to notice are:\n\nthe average expression in cells is similar for all cells. This is good to know - if some cells had much lower expression perhaps there is something wrong with them, or their sequencing, and they should be excluded.\nthe distributions are roughly similar in width too\n\nThe default order of cell is alphabetical. It can be easier to judge if there are unusual cells if we order the lines by the size of the mean.\nüé¨ Order a pointrange plot with reorder(variable_to_order, order_by).\n\nhspc_prog_summary_cell |&gt; \n  ggplot(aes(x = reorder(cell, mean), y = mean)) +\n  geom_pointrange(aes(ymin = mean - sd, \n                      ymax = mean + sd ),\n                  size = 0.1)\n\n\n\n\n\n\n\nreorder() arranges cell in increasing size of mean\nAs we thought, the distributions are similar across cells - there are not any cells that are obviously different from the others (only incrementally).\nDistribution of values across the genes\nWe will use the same approach to summarise the genes.\nüé¨ Summarise the expression for each gene and save the result as hspc_prog_summary_gene. Include the same columns as we had in the by cell summary (hspc_prog_summary_cell) and an additional column, total for the total expression for each gene.\nüé¨ View the hspc_prog_summary_gene dataframe. Remember these are normalised and logged (base 2) so we should not see very large values.\nNotice that we have:\n\nsome genes (7) expressed in every cell, and many expressed in most cells\nquite a few genes with zero in many cells but this matters less when we have many cells (samples) than when we have few samples.\nno genes with zeros in every cell - the lowest number of cells is 15.\n\nIt is again helpful to plot the ordered mean expression with pointrange to get an overview.\nüé¨ Plot the logged mean counts for each gene in order of size using geom_pointrange():\n\nhspc_prog_summary_gene |&gt; \n  ggplot(aes(x = reorder(ensembl_gene_id, mean), y = mean)) +\n  geom_pointrange(aes(ymin = mean - sd, \n                      ymax = mean + sd),\n                  size = 0.1)\n\n\n\n\n\n\n\nNote that the variability between genes (average expression between 0.020 and and 9.567) is far greater than between cells (average expression from 1.319 to 9.567) which is just what we would expect.\nNow go to Filtering for QC.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#frog-development-1",
    "href": "transcriptomics/week-3/workshop.html#frog-development-1",
    "title": "Workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\nOur samples look to be similarly well sequenced. There are no samples we should remove. However, some genes are not expressed or the expression values are so low in for a gene that they are uninformative. We will filter the s30_summary_gene dataframe to obtain a list of xenbase_gene_id we can use to filter s30.\nMy suggestion is to include only the genes with counts in at least 3 samples and those with total counts above 20. I chose 3 because that would keep genes expressed only in one treatment: [0, 0, 0] [#,#,#]. This is a difference we cannot test statistically, but which matters biologically.\nüé¨ Filter the summary by gene dataframe:\n\ns30_summary_gene_filtered &lt;- s30_summary_gene |&gt; \n  filter(total &gt; 20) |&gt; \n  filter(n_above_zero &gt;= 3)\n\n‚ùì How many genes do you have left\n\n\n\nüé¨ Use the list of xenbase_gene_id in the filtered summary to filter the original dataset:\n\ns30_filtered &lt;- s30 |&gt; \n  filter(xenbase_gene_id %in%  s30_summary_gene_filtered$xenbase_gene_id)\n\nüé¨ Write the filtered data to file:\n\nwrite_csv(s30_filtered, \n          file = \"data-processed/s30_filtered.csv\")\n\nNow go to Look after future you",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#arabidopsis-1",
    "href": "transcriptomics/week-3/workshop.html#arabidopsis-1",
    "title": "Workshop",
    "section": "üéÑ Arabidopsis\n",
    "text": "üéÑ Arabidopsis\n\nOur samples look to be similarly well sequenced although this is difficult to determine with only two replicates. However, some genes are not expressed or the expression values are so low in for a gene that they are uninformative. We will filter the wild_summary_gene dataframe to obtain a list of gene_id we can use to filter wild.\nMy suggestion is to include only the genes with counts in at least 2 samples, and those with total counts above 20. I chose 2 because that would keep genes expressed only in one treatment: [0, 0] [#,#]. This is a difference we cannot test statistically, but which matters biologically.\nüé¨ Filter the summary by gene dataframe:\n\nwild_summary_gene_filtered &lt;- wild_summary_gene |&gt; \n  filter(total &gt; 20) |&gt; \n  filter(n_above_zero &gt;= 2)\n\n‚ùì How many genes do you have left\n\n\n\nüé¨ Use the list of gene_id in the filtered summary to filter the original dataset:\n\nwild_filtered &lt;- wild |&gt; \n  filter(gene_id %in%  wild_summary_gene_filtered$gene_id)\n\nüé¨ Write the filtered data to file:\n\nwrite_csv(wild_filtered, \n          file = \"data-processed/wild_filtered.csv\")\n\nNow go to Look after future you",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#leishmania-1",
    "href": "transcriptomics/week-3/workshop.html#leishmania-1",
    "title": "Workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\nOur samples look to be similarly well sequenced. There are no samples we should remove. However, some genes are not expressed or the expression values are so low in for a gene that they are uninformative. We will filter the pro_meta_summary_gene dataframe to obtain a list of gene_id we can use to filter pro_meta.\nMy suggestion is to include only the genes with counts in at least 3 samples and those with total counts above 20. I chose 3 because that would keep genes expressed only in one treatment: [0, 0, 0] [#,#,#]. This is a difference we cannot test statistically, but which matters biologically.\nüé¨ Filter the summary by gene dataframe:\n\npro_meta_summary_gene_filtered &lt;- pro_meta_summary_gene |&gt; \n  filter(total &gt; 20) |&gt; \n  filter(n_above_zero &gt;= 3)\n\n‚ùì How many genes do you have left\n\n\n\nüé¨ Use the list of gene_id in the filtered summary to filter the original dataset:\n\npro_meta_filtered &lt;- pro_meta |&gt; \n  filter(gene_id %in%  pro_meta_summary_gene_filtered$gene_id)\n\nüé¨ Write the filtered data to file:\n\nwrite_csv(pro_meta_filtered, \n          file = \"data-processed/pro_meta_filtered.csv\")\n\nNow go to Look after future you",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#stem-cells-1",
    "href": "transcriptomics/week-3/workshop.html#stem-cells-1",
    "title": "Workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nIn this dataset, we will not see and genes that are not expressed in any of the cells because we are using a specific subset of the transcriptome that was deliberately selected. In other words unexpressed genes have already been filtered out. However, it is good practices to verify there are no unexpressed genes before we embark on our analysis.\nWhere the sum of all the values in the rows is zero, all the entries must be zero. We can use this to find any the genes that are not expressed in any of the cells. To do row wise aggregates such as the sum across rows we can use the rowwise() function. c_across() allows us to use the colon notation to select columns. This is very useful when you have a lot of columns because it would be annoying to have to list all of them HSPC_001:Prog_852 means all the columns from HSPC_001 to Prog_852.\nüé¨ Find the genes that are 0 in every column of the hspc_prog dataframe:\n\nhspc_prog |&gt; \n  rowwise() |&gt; \n  filter(sum(c_across(HSPC_001:Prog_852)) == 0)\n\n# A tibble: 0 √ó 1,500\n# Rowwise: \n# ‚Ñπ 1,500 variables: ensembl_gene_id &lt;chr&gt;, HSPC_001 &lt;dbl&gt;, HSPC_002 &lt;dbl&gt;,\n#   HSPC_003 &lt;dbl&gt;, HSPC_004 &lt;dbl&gt;, HSPC_006 &lt;dbl&gt;, HSPC_008 &lt;dbl&gt;,\n#   HSPC_009 &lt;dbl&gt;, HSPC_011 &lt;dbl&gt;, HSPC_012 &lt;dbl&gt;, HSPC_014 &lt;dbl&gt;,\n#   HSPC_015 &lt;dbl&gt;, HSPC_016 &lt;dbl&gt;, HSPC_017 &lt;dbl&gt;, HSPC_018 &lt;dbl&gt;,\n#   HSPC_020 &lt;dbl&gt;, HSPC_021 &lt;dbl&gt;, HSPC_022 &lt;dbl&gt;, HSPC_023 &lt;dbl&gt;,\n#   HSPC_024 &lt;dbl&gt;, HSPC_025 &lt;dbl&gt;, HSPC_026 &lt;dbl&gt;, HSPC_027 &lt;dbl&gt;,\n#   HSPC_028 &lt;dbl&gt;, HSPC_030 &lt;dbl&gt;, HSPC_031 &lt;dbl&gt;, HSPC_033 &lt;dbl&gt;, ‚Ä¶\n\n\nNotice that we have summed across all the columns.\n‚ùì What do you conclude?\n\n\nüé¨ Write combined data to file:\n\nwrite_csv(hspc_prog, \n          file = \"data-processed/hspc_prog.csv\")\n\nNow go to Look after future you",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#frogs-and-future-you",
    "href": "transcriptomics/week-3/workshop.html#frogs-and-future-you",
    "title": "Workshop",
    "section": "üê∏ Frogs and future you",
    "text": "üê∏ Frogs and future you\nüé¨ Create a new Project, frogs-88H, populated with folders and your data. Make a script file called cont-fgf-s30.R. This will a be commented analysis of the comparison between the control and FGF-treated embroys at S30 comparison. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#arabidopsis-and-future-you",
    "href": "transcriptomics/week-3/workshop.html#arabidopsis-and-future-you",
    "title": "Workshop",
    "section": "üéÑ Arabidopsis and future you",
    "text": "üéÑ Arabidopsis and future you\nüé¨ Create a new Project, arab-88H, populated with folders and your data. Make a script file called suff-def-wild.R. This will a be commented analysis of comparison between copper sufficient and copper deficient wildtype plants. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#leishmania-and-future-you",
    "href": "transcriptomics/week-3/workshop.html#leishmania-and-future-you",
    "title": "Workshop",
    "section": "üíâ Leishmania and future you",
    "text": "üíâ Leishmania and future you\nüé¨ Create a new Project, leish-88H, populated with folders and your data. Make a script file called pro_meta.R. This will a be commented analysis of comparison procyclic promastigote and metacyclic promastigotes. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#stem-cells-and-future-you",
    "href": "transcriptomics/week-3/workshop.html#stem-cells-and-future-you",
    "title": "Workshop",
    "section": "üê≠ Stem cells and future you",
    "text": "üê≠ Stem cells and future you\nüé¨ Create a new Project, mice-88H, populated with folders and your data. Make a script file called hspc-prog.R. This will a be commented analysis of the hspc cells vs the prog cells. You will build on this each workshop and be able to use it as a template to examine other comparisons. Copy in the appropriate code and comments from workshop-1.R. Edit to improve your comments where your understanding has developed since you made them. Make sure you can close down RStudio, reopen it and run your whole script again.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/workshop.html#footnotes",
    "href": "transcriptomics/week-3/workshop.html#footnotes",
    "title": "Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.‚Ü©Ô∏é\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.‚Ü©Ô∏é\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.‚Ü©Ô∏é\nThis a result of the Central limit theorem,one consequence of which is that adding together lots of distributions - whatever distributions they are - will tend to a normal distribution.‚Ü©Ô∏é",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Workshop"
    ]
  },
  {
    "objectID": "images/images.html",
    "href": "images/images.html",
    "title": "Image Data Analysis for Group Project",
    "section": "",
    "text": "The following ImageJ workflow uses the processing steps you used in workshop 3 with one change. That change is to save the results to file rather than having the results window pop up and saving from there. Or maybe two changes: it also tells you to use meaning systematic file names that will be easy to process when importing data. The RStudio workflow shows you how to import multiple files into one dataframe with columns indicating the treatment.\n\nSave files with systematic names: ev_0.avi 343_0.avi ev_1.avi 343_1.avi ev_2.5.avi 343_2.5.avi\nOpen ImageJ\nOpen video file eg ev_2.5.avi\n\nConvert to 8-bit: Image | Type | 8-bit\nCrop to petri dish: Select then Image | Crop\nCalculate average pixel intensity: Image | Stacks | Z Project\n\nProjection type: Average Intensity to create AVG_ev_2.5.avi\n\n\n\nSubtract average from image: Process | Image Calculator\n\nImage 1: ev_2.5.avi\n\nOperation: Subtract\nImage 2: AVG_ev_2.5.avi\n\nCreate new window: checked\nOK, Yes to Process all\n\n\nInvert: Edit | Invert\nAdjust threshold: Image | Adjust | Threshold\n\nMethod: Default\nThresholding: Default, B&W\nDark background: checked\nAuto or adjust a little but make sure the larvae do not disappear at later points in the video (use the slider)\nApply\n\n\nInvert: Edit | Invert\nTrack: Plugins | wrMTrck\n\nSet minSize: 10\nSet maxSize: 400\nSet maxVelocity: 10\nSet maxAreaChange: 200\nSet bendThreshold: 1\n\nImportant: check Save Results File This is different to what you did in the workshop. It will help because the results will be saved automatically rather than to saving from the Results window that other pops up. Consequently, you will be able to save the results files with systematic names relating to their treatments and then read them into R simultaneously. That will also allow you to add information from the name of the file (which has the treatment information) to the resulting dataframes\n\n\nwrMTrck window with the settings listed above shown\n\n\nClick OK. Save to a folder for all the tracking data files. I recommend deleting the ‚ÄúResults of..‚Äù part of the name\n\n\nCheck that the Summary window indicates 3 tracks and that the 3 larvae are what is tracked by using the slider on the Result image\nRepeat for all videos\n\nThis is the code you need to import multiple csv files into a single dataframe and add a column with the treatment information from the file name. This is why systematic file names are good.\nIt assumes\n\nyour files are called type_concentration.txt for example: ev_0.txt 343_0.txt ev_1.txt 343_1.txt ev_2.5.txt 343_2.5.txt.\nthe .txt datafile are in a folder called track inside your working directory\nyou have installed the following packages: tidyverse, janitor\n\n\nüé¨ Load the tidyverse\n\nlibrary(tidyverse)\n\nüé¨ Put the file names into a vector we will iterate through\n\n# get a vector of the file names\nfiles &lt;- list.files(path = \"track\", full.names = TRUE )\n\nWe can use map_df() from the purrr package which is one of the tidyverse gems loaded with tidyvserse. map_df() will iterate through files and read them into a dataframe with a specified import function. We are using read_table(). map_df() keeps track of the file by adding an index column called file to the resulting dataframe. Instead of this being a number (1 - 6 here) we can use set_names() to use the file names instead. The clean_names() function from the janitor package will clean up the column names (make them lower case, replace spaces with _ remove special characters etc)\nüé¨ Import multiple csv files into one dataframe called tracking\n\n# import multiple data files into one dataframe called tracking\n# using map_df() from purrr package\n# clean the column names up using janitor::clean_names()\ntracking &lt;- files |&gt; \n  set_names() |&gt;\n  map_dfr(read_table, .id = \"file\") |&gt;\n  janitor::clean_names()\n\nYou will get a warning Duplicated column names deduplicated: 'avgX' =&gt; 'avgX_1' [15] for each of the files because the csv files each have two columns called avgX. If you click on the tracking dataframe you see is contains the data from all the files.\nNow we can add columns for the type and the concentration by processing the values in the file. The values are like track/343_0.txt so we need to remove .txt and track/ and separate the remaining words into two columns.\nüé¨ Process the file column to add columns for the type and the concentration\n\n# extract type and concentration from file name\n# and put them into additopnal separate columns\ntracking &lt;- tracking |&gt; \n  mutate(file = str_remove(file, \".txt\")) |&gt;\n  mutate(file = str_remove(file, \"track/\")) |&gt;\n  extract(file, remove = \n            FALSE,\n          into = c(\"type\", \"conc\"), \n          regex = \"([^_]{2,3})_(.+)\") \n\n[^_]{2,3} matches two or three characters that are not _ at the start of the string (^)\n.+ matches one or more characters. The extract() function puts the first match into the first column, type, and the second match into the second column, conc. The remove = FALSE argument means the original column is kept.\nYou now have a dataframe with all the tracking data which is relatively easy to summarise and plot using tools you know.\nThere is an example RStudio project containing this code here: tips. You can also download the project as a zip file from there but there is some code that will do that automatically for you. Since this is an RStudio Project, do not run the code from inside a project. You may want to navigate to a particular directory or edit the destdir:\n\nusethis::use_course(url = \"3mmaRand/tips\", destdir = \".\")\n\nYou can agree to deleting the zip. You should find RStudio restarts and you have a new project called tips-xxxxxx. The xxxxxx is a commit reference - you do not need to worry about that, it is just a way to tell you which version of the repo you downloaded. You can now run the code in the project.",
    "crumbs": [
      "Image Analysis",
      "Image Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "images/images.html#worm-tracking",
    "href": "images/images.html#worm-tracking",
    "title": "Image Data Analysis for Group Project",
    "section": "",
    "text": "The following ImageJ workflow uses the processing steps you used in workshop 3 with one change. That change is to save the results to file rather than having the results window pop up and saving from there. Or maybe two changes: it also tells you to use meaning systematic file names that will be easy to process when importing data. The RStudio workflow shows you how to import multiple files into one dataframe with columns indicating the treatment.\n\nSave files with systematic names: ev_0.avi 343_0.avi ev_1.avi 343_1.avi ev_2.5.avi 343_2.5.avi\nOpen ImageJ\nOpen video file eg ev_2.5.avi\n\nConvert to 8-bit: Image | Type | 8-bit\nCrop to petri dish: Select then Image | Crop\nCalculate average pixel intensity: Image | Stacks | Z Project\n\nProjection type: Average Intensity to create AVG_ev_2.5.avi\n\n\n\nSubtract average from image: Process | Image Calculator\n\nImage 1: ev_2.5.avi\n\nOperation: Subtract\nImage 2: AVG_ev_2.5.avi\n\nCreate new window: checked\nOK, Yes to Process all\n\n\nInvert: Edit | Invert\nAdjust threshold: Image | Adjust | Threshold\n\nMethod: Default\nThresholding: Default, B&W\nDark background: checked\nAuto or adjust a little but make sure the larvae do not disappear at later points in the video (use the slider)\nApply\n\n\nInvert: Edit | Invert\nTrack: Plugins | wrMTrck\n\nSet minSize: 10\nSet maxSize: 400\nSet maxVelocity: 10\nSet maxAreaChange: 200\nSet bendThreshold: 1\n\nImportant: check Save Results File This is different to what you did in the workshop. It will help because the results will be saved automatically rather than to saving from the Results window that other pops up. Consequently, you will be able to save the results files with systematic names relating to their treatments and then read them into R simultaneously. That will also allow you to add information from the name of the file (which has the treatment information) to the resulting dataframes\n\n\nwrMTrck window with the settings listed above shown\n\n\nClick OK. Save to a folder for all the tracking data files. I recommend deleting the ‚ÄúResults of..‚Äù part of the name\n\n\nCheck that the Summary window indicates 3 tracks and that the 3 larvae are what is tracked by using the slider on the Result image\nRepeat for all videos\n\nThis is the code you need to import multiple csv files into a single dataframe and add a column with the treatment information from the file name. This is why systematic file names are good.\nIt assumes\n\nyour files are called type_concentration.txt for example: ev_0.txt 343_0.txt ev_1.txt 343_1.txt ev_2.5.txt 343_2.5.txt.\nthe .txt datafile are in a folder called track inside your working directory\nyou have installed the following packages: tidyverse, janitor\n\n\nüé¨ Load the tidyverse\n\nlibrary(tidyverse)\n\nüé¨ Put the file names into a vector we will iterate through\n\n# get a vector of the file names\nfiles &lt;- list.files(path = \"track\", full.names = TRUE )\n\nWe can use map_df() from the purrr package which is one of the tidyverse gems loaded with tidyvserse. map_df() will iterate through files and read them into a dataframe with a specified import function. We are using read_table(). map_df() keeps track of the file by adding an index column called file to the resulting dataframe. Instead of this being a number (1 - 6 here) we can use set_names() to use the file names instead. The clean_names() function from the janitor package will clean up the column names (make them lower case, replace spaces with _ remove special characters etc)\nüé¨ Import multiple csv files into one dataframe called tracking\n\n# import multiple data files into one dataframe called tracking\n# using map_df() from purrr package\n# clean the column names up using janitor::clean_names()\ntracking &lt;- files |&gt; \n  set_names() |&gt;\n  map_dfr(read_table, .id = \"file\") |&gt;\n  janitor::clean_names()\n\nYou will get a warning Duplicated column names deduplicated: 'avgX' =&gt; 'avgX_1' [15] for each of the files because the csv files each have two columns called avgX. If you click on the tracking dataframe you see is contains the data from all the files.\nNow we can add columns for the type and the concentration by processing the values in the file. The values are like track/343_0.txt so we need to remove .txt and track/ and separate the remaining words into two columns.\nüé¨ Process the file column to add columns for the type and the concentration\n\n# extract type and concentration from file name\n# and put them into additopnal separate columns\ntracking &lt;- tracking |&gt; \n  mutate(file = str_remove(file, \".txt\")) |&gt;\n  mutate(file = str_remove(file, \"track/\")) |&gt;\n  extract(file, remove = \n            FALSE,\n          into = c(\"type\", \"conc\"), \n          regex = \"([^_]{2,3})_(.+)\") \n\n[^_]{2,3} matches two or three characters that are not _ at the start of the string (^)\n.+ matches one or more characters. The extract() function puts the first match into the first column, type, and the second match into the second column, conc. The remove = FALSE argument means the original column is kept.\nYou now have a dataframe with all the tracking data which is relatively easy to summarise and plot using tools you know.\nThere is an example RStudio project containing this code here: tips. You can also download the project as a zip file from there but there is some code that will do that automatically for you. Since this is an RStudio Project, do not run the code from inside a project. You may want to navigate to a particular directory or edit the destdir:\n\nusethis::use_course(url = \"3mmaRand/tips\", destdir = \".\")\n\nYou can agree to deleting the zip. You should find RStudio restarts and you have a new project called tips-xxxxxx. The xxxxxx is a commit reference - you do not need to worry about that, it is just a way to tell you which version of the repo you downloaded. You can now run the code in the project.",
    "crumbs": [
      "Image Analysis",
      "Image Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_after_workshop.html",
    "href": "transcriptomics/week-3/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "You need only do the section for your own project data",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_after_workshop.html#frog-development",
    "href": "transcriptomics/week-3/study_after_workshop.html#frog-development",
    "title": "Independent Study to consolidate this week",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\nüé¨ Open your frogs-88H Project. Make a new script, cont-fgf-s20.R, and, using cont-fgf-s30.R as a template, repeat the analysis stage 20.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_after_workshop.html#arabidopisis",
    "href": "transcriptomics/week-3/study_after_workshop.html#arabidopisis",
    "title": "Independent Study to consolidate this week",
    "section": "üéÑ Arabidopisis",
    "text": "üéÑ Arabidopisis\nüé¨ Open your arab-88H Project. Make a new script, suff-def-spl7.R, and, using suff-def-wild.R as a template, repeat the analysis on the spl7 mutants.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_after_workshop.html#leishmania",
    "href": "transcriptomics/week-3/study_after_workshop.html#leishmania",
    "title": "Independent Study to consolidate this week",
    "section": "üíâ Leishmania",
    "text": "üíâ Leishmania\nüé¨ Open your leish-88H Project. Make a new script, pro_ama.R, and, using pro_meta.R as a template, repeat the analysis on the procyclic promastigotes (pro) and amastigotes (ama).",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_after_workshop.html#stem-cells",
    "href": "transcriptomics/week-3/study_after_workshop.html#stem-cells",
    "title": "Independent Study to consolidate this week",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\nüé¨ Open your mice-88H Project. Make a new script, hspc-lthsc.R and, using hspc-prog.R as a template, repeat the analysis on the HSPC and LT-HSC cells.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#overview",
    "href": "transcriptomics/week-3/study_before_workshop.html#overview",
    "title": "Independent Study to prepare for workshop",
    "section": "Overview",
    "text": "Overview\n\n\nConcise summary of the experimental design and aims\nWhat the raw data consist of\nWhat has been done to the data so far\nWhat steps we will take in the workshop",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#the-data",
    "href": "transcriptomics/week-3/study_before_workshop.html#the-data",
    "title": "Independent Study to prepare for workshop",
    "section": "The Data",
    "text": "The Data\nThere are 4 transcriptomic datasets\n\nüê∏ bulk RNA-seq from Xenopus laevis embryos.\nüéÑ bulk RNA-seq from Arabidopsis thaliana\nüíâ bulk RNA-seq from Leishmania mexicana\nüê≠ single cell RNA-seq from mouse stemcells",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Experimental design",
    "text": "üê∏ Experimental design\n\nSchematic of frog development experiment",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-2",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-2",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Experimental design",
    "text": "üê∏ Experimental design\n\nSchematic of frog development experiment\n\n3 fertilisations\n2 siblings from each fertilisation one control, one FGF treated\nsequenced at 3 time points\n3 x 2 x 3 = 18 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-3",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-3",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Experimental design",
    "text": "üê∏ Experimental design\n\nSchematic of frog development experiment\n\n3 fertilisations. These are the replicates, 1, 2, 3\n2 siblings from each fertilisation one control, one FGF treated. The treatments are paired\nsequenced at 3 time points. S14, S20, S30\n3 x 2 x 3 = 18 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#aim",
    "href": "transcriptomics/week-3/study_before_workshop.html#aim",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Aim",
    "text": "üê∏ Aim\n\n\nFind genes that are ‚Äúdifferentially expressed‚Äù between control-treated and FGF-treated siblings\nDifferentially expressed means the expression in one group is significantly higher than in the other",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#guided-analysis",
    "href": "transcriptomics/week-3/study_before_workshop.html#guided-analysis",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Guided analysis",
    "text": "üê∏ Guided analysis\n\n\nThe workshops will take you through comparing the control and FGF treated sibling at S30\nYou will make other comparisons independently\nYou will be guided to carefully document your work so you can apply the same methods to other comparisons\nDo the independent study before and after the workshop!",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-4",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-4",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ Experimental design",
    "text": "üéÑ Experimental design\n\nSchematic of arabidopsis experiment",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-5",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-5",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ Experimental design",
    "text": "üéÑ Experimental design\n\nSchematic of arabidopsis experiment\n\n2 plant genotypes\n2 copper conditions\n2 plants\n2 x 2 x 3 = 8 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-6",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-6",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ Experimental design",
    "text": "üéÑ Experimental design\n\nSchematic of arabidopsis experiment\n\n2 plant genotypes: wildtype and spl7 mutant. This is the genotype treatment\n2 copper conditions: sufficient and deficient. This is the Cu treatment\n2 plants. These are the replicates\n2 x 2 x 3 = 8 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#aim-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#aim-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ Aim",
    "text": "üéÑ Aim\n\n\nFind genes that are ‚Äúdifferentially expressed‚Äù between plant types and copper conditions e.g.¬†wildtype plants grown under copper sufficient and copper deficient conditions\nDifferentially expressed means the expression in one group is significantly higher than in the other",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ Guided analysis",
    "text": "üéÑ Guided analysis\n\n\nThe workshops will take you through comparing the wildtype plants grown under copper sufficient and copper deficient conditions\nYou will make other comparisons independently\nYou will be guided to carefully document your work so you can apply the same methods to other comparisons\nDo the independent study before and after the workshop!",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-7",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-7",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ Experimental design",
    "text": "üíâ Experimental design\n\nSchematic of leishmania experiment",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-8",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-8",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ Experimental design",
    "text": "üíâ Experimental design\n\nSchematic of leishmania experiment\n\n3 stages\n3 samples\n3 x 3 = 9 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-9",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-9",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ Experimental design",
    "text": "üíâ Experimental design\n\nSchematic of leishmania experiment\n\nthree stages: procyclic promastigotes, metacyclic promastigotes and amastigotes. This is the stage treatment\nthree samples. These are the replicates\n3 x 3 = 9 samples",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#aim-2",
    "href": "transcriptomics/week-3/study_before_workshop.html#aim-2",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ Aim",
    "text": "üíâ Aim\n\n\nFind genes that are ‚Äúdifferentially expressed‚Äù between stages e.g., procyclic promastigotes and the metacyclic promastigotes\nDifferentially expressed means the expression in one group is significantly higher than in the other",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-2",
    "href": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-2",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ Guided analysis",
    "text": "üíâ Guided analysis\n\n\nThe workshops will take you through comparing the procyclic promastigotes and the metacyclic promastigotes\nYou will make other comparisons independently\nYou will be guided to carefully document your work so you can apply the same methods to other comparisons\nDo the independent study before and after the workshop!",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-10",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-10",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Experimental design",
    "text": "üê≠ Experimental design\n\nSchematic of stem cell experiment",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-11",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-11",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Experimental design",
    "text": "üê≠ Experimental design\n\nSchematic of stem cell experiment\n\nCells were sorted using flow cytometry on the basis of cell surface markers\nThere are 3 cell types\nMany cells of each cell type were sequenced",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#experimental-design-12",
    "href": "transcriptomics/week-3/study_before_workshop.html#experimental-design-12",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Experimental design",
    "text": "üê≠ Experimental design\n\nSchematic of stem cell experiment\n\nThere are three cell types: LT-HSCs, HSPCs, Progs This is the cell ‚Äútreatment‚Äù\nMany cells of each type were sequenced: These are the replicates\n155 LT-HSCs, 701 HSPCs, 798 Progs",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#aim-3",
    "href": "transcriptomics/week-3/study_before_workshop.html#aim-3",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Aim",
    "text": "üê≠ Aim\n\n\nfind genes that are ‚Äúdifferentially expressed‚Äù between at least two cell types\nDifferentially expressed means the expression in one group is significantly higher than in the other",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-3",
    "href": "transcriptomics/week-3/study_before_workshop.html#guided-analysis-3",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Guided analysis",
    "text": "üê≠ Guided analysis\n\n\nThe workshops will take you through comparing the HSPC and Prog cells\nYou will make other comparisons independently\nYou will be guided to carefully document your work so you can apply the same methods to other comparisons\nDo the independent study before and after the workshop!",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#raw-sequence-data",
    "href": "transcriptomics/week-3/study_before_workshop.html#raw-sequence-data",
    "title": "Independent Study to prepare for workshop",
    "section": "Raw Sequence data",
    "text": "Raw Sequence data\n\n\nThe raw data are ‚Äúreads‚Äù from a sequencing machine in FASTQ files\nA read is sequence of RNA which is shorter than the whole transcriptome\nThe length of the reads depends on the type of sequencing machine\n\nShort-read technologies (e.g.¬†Illumina) have higher base accuracy but are harder to align\nLong-read technologies (e.g.¬†Nanopore) have lower base accuracy but are easier to align",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#raw-sequence-data-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#raw-sequence-data-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Raw Sequence data",
    "text": "Raw Sequence data\nOptional\nYou can read more about Sequencing technologies in Statistically useful experimental design(Rand and Forrester 2022)",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#general-steps",
    "href": "transcriptomics/week-3/study_before_workshop.html#general-steps",
    "title": "Independent Study to prepare for workshop",
    "section": "General steps",
    "text": "General steps\n\n\nReads are filtered and trimmed on the basis of a quality score\nThey are then aligned/pseudo-aligned to a reference genome/transcriptome (or assembled de novo)\nAnd then counted to quantify the expression\nCounts need to be normalised to account for differences in sequencing depth and transcript length before, or as part of, statistical analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#data",
    "href": "transcriptomics/week-3/study_before_workshop.html#data",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Data",
    "text": "üê∏ Data\n\nUnpublished (so far!)\nExpression for the whole transcriptome X. laevis v10.1 genome assembly(Fisher et al. 2023)\nValues are raw counts\nThe statistical analysis method we will use is DESeq2 (Love, Huber, and Anders 2014). It requires raw counts and performs the normalisation itself.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#data-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#data-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ Data",
    "text": "üéÑ Data\n\nBased on PRJNA132271(Bernal et al. 2012)\nExpression for the whole transcriptome ENSEMBL Arabidopsis TAIR10(Yates et al. 2022)\nValues are raw counts\nThe statistical analysis method we will use is DESeq2 (Love, Huber, and Anders 2014). It requires raw counts and performs the normalisation itself.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#data-2",
    "href": "transcriptomics/week-3/study_before_workshop.html#data-2",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ Data",
    "text": "üíâ Data\n\nBrand spanking new!\nExpression for the whole transcriptome L. mexicana MHOM/GT/2001/U1103(Rogers et al. 2011)\nValues are raw counts\nThe statistical analysis method we will use is DESeq2 (Love, Huber, and Anders 2014). It requires raw counts and performs the normalisation itself.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#data-3",
    "href": "transcriptomics/week-3/study_before_workshop.html#data-3",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Data",
    "text": "üê≠ Data\n\nPublished in Nestorowa et al. (2016)\nExpression for a subset of genes, the surfaceome\nValues are log2 normalised values\nThe statistical analysis method we will use is scran (Lun, McCarthy, and Marioni 2016) and it requires normalised values",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#workshops-1",
    "href": "transcriptomics/week-3/study_before_workshop.html#workshops-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Workshops",
    "text": "Workshops\n\nTranscriptomics 1: Hello data Getting to know the data. Checking the distributions of values overall, across rows and columns to check things are as we expect and detect rows/columns that need to be removed\nTranscriptomics 2: Statistical Analysis. Identifying which genes are differentially expressed between treatments. This is the main analysis step. We will use different methods for bulk and single cell data.\nTranscriptomics 3: Visualising. Principal Component Analysis (PCA) and volcano plots to visualise the results of the analysis.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-3/study_before_workshop.html#references",
    "href": "transcriptomics/week-3/study_before_workshop.html#references",
    "title": "Independent Study to prepare for workshop",
    "section": "References",
    "text": "References\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr (Xie 2024, 2015, 2014), kableExtra (Zhu 2021)\n\n\n\nüîó About Transcriptomics 1: Hello data!\n\n\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2024. ‚ÄúQuarto.‚Äù https://doi.org/10.5281/zenodo.5960048.\n\n\nBernal, Mar√≠a, David Casero, Vasantika Singh, Grandon T. Wilson, Arne Grande, Huijun Yang, Sheel C. Dodani, et al. 2012. ‚ÄúTranscriptome Sequencing Identifies SPL7-Regulated Copper Acquisition Genes FRO4/FRO5 and the Copper Dependence of Iron Homeostasis in Arabidopsis.‚Äù The Plant Cell 24 (2): 738‚Äì61. https://doi.org/10.1105/tpc.111.090431.\n\n\nFisher, Malcolm, Christina James-Zorn, Virgilio Ponferrada, Andrew J Bell, Nivitha Sundararaj, Erik Segerdell, Praneet Chaturvedi, et al. 2023. ‚ÄúXenbase: Key Features and Resources of the Xenopus Model Organism Knowledgebase.‚Äù Genetics 224 (1): iyad018. https://doi.org/10.1093/genetics/iyad018.\n\n\nLove, Michael I., Wolfgang Huber, and Simon Anders. 2014. ‚ÄúModerated Estimation of Fold Change and Dispersion for RNA-Seq Data with DESeq2.‚Äù Genome Biology 15: 550. https://doi.org/10.1186/s13059-014-0550-8.\n\n\nLun, Aaron T. L., Davis J. McCarthy, and John C. Marioni. 2016. ‚ÄúA Step-by-Step Workflow for Low-Level Analysis of Single-Cell RNA-Seq Data with Bioconductor.‚Äù F1000Res. 5: 2122. https://doi.org/10.12688/f1000research.9501.2.\n\n\nNestorowa, Sonia, Fiona K. Hamey, Blanca Pijuan Sala, Evangelia Diamanti, Mairi Shepherd, Elisa Laurenti, Nicola K. Wilson, David G. Kent, and Berthold G√∂ttgens. 2016. ‚ÄúA Single-Cell Resolution Map of Mouse Hematopoietic Stem and Progenitor Cell Differentiation.‚Äù Blood 128 (8): e20‚Äì31. https://doi.org/10.1182/blood-2016-05-716480.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRand, Emma, and Sarah Forrester. 2022. ‚ÄúStatistically Useful Experimental Design.‚Äù https://cloud-span.github.io/experimental_design00-overview/.\n\n\nRogers, Matthew B., James D. Hilley, Nicholas J. Dickens, Jon Wilkes, Paul A. Bates, Daniel P. Depledge, David Harris, et al. 2011. ‚ÄúChromosome and Gene Copy Number Variation Allow Major Structural Change Between Species and Strains of Leishmania.‚Äù Genome Research 21 (12): 2129‚Äì42. https://doi.org/10.1101/gr.122945.111.\n\n\nXie, Yihui. 2014. ‚ÄúKnitr: A Comprehensive Tool for Reproducible Research in R.‚Äù In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\n‚Äî‚Äî‚Äî. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\n‚Äî‚Äî‚Äî. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nYates, Andrew D, James Allen, Ridwan M Amode, Andrey G Azov, Matthieu Barba, Andr√©s Becerra, Jyothish Bhai, et al. 2022. ‚ÄúEnsembl Genomes 2022: An Expanding Genome Resource for Non-Vertebrates.‚Äù Nucleic Acids Research 50 (D1): D996‚Äì1003. https://doi.org/10.1093/nar/gkab1007.\n\n\nZhu, Hao. 2021. ‚ÄúkableExtra: Construct Complex Table with ‚ÄôKable‚Äô and Pipe Syntax.‚Äù https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Transcriptomics",
      "Week 3: Hello data!",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_after_workshop.html",
    "href": "transcriptomics/week-4/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "You need only do the section for your own project data\nüê∏ Frog development\nüé¨ Open your frogs-88H RStudio Project and the cont-fgf-s20.R script you began in the Consolidation study last week. Use the differential expression analysis you did in the workshop (in cont-fgf-s30.R) as a template to continue your script.\nüéÑ Arabidopsis\nüé¨ Open your arab-88H RStudio Project and the suff-def-spl7.R script you began in the Consolidation study last week. Use the differential expression analysis you did in the workshop (in suff-def-wild.R) as a template to continue your script.\nüíâ Leishmania\nüé¨ Open your leish-88H RStudio Project and the pro_ama.R script you began in the Consolidation study last week. Use the differential expression analysis you did in the workshop (in pro_meta.R) as a template to continue your script.\nüê≠ Stem cells\nüé¨ Open your mice-88H RStudio Project and the hspc-lthsc.R script you began in the Consolidation study last week. Use the differential expression analysis you did in the workshop (in hspc-prog.R) as a template to continue your script.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#overview",
    "href": "transcriptomics/week-4/study_before_workshop.html#overview",
    "title": "Independent Study to prepare for workshop",
    "section": "Overview",
    "text": "Overview\nIn these slides we will:\n\n\nCheck where you are following week 3\n\nlearn some concepts in differential expression\n\nlog2 fold changes\nMultiple correction\nnormalisation\nstatistical model\n\n\nFind out what packages we will use",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#what-we-did-in-transcriptomics-1-hello-data",
    "href": "transcriptomics/week-4/study_before_workshop.html#what-we-did-in-transcriptomics-1-hello-data",
    "title": "Independent Study to prepare for workshop",
    "section": "What we did in Transcriptomics 1: üëã Hello data!",
    "text": "What we did in Transcriptomics 1: üëã Hello data!\n\n\n\nDiscovered how many rows and columns we had in our datasets and what these were.\nExamined the distribution of values\n\nacross the whole dataset\nacross the samples/cells (i.e., averaged over genes) to see variation between samples/cells\nacross the genes (i.e., averaged over samples/cells) to see variation between genes\n\n\nFiltered data for quality control and wrote to file (except üê≠)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#where-should-you-be-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#where-should-you-be-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Where should you be?",
    "text": "Where should you be?\nAfter the Transcriptomics 1: üëã Hello data! Workshop including:\n\nü§ó Look after future you! and\nthe Independent Study to consolidate, you should have:",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#frog-development",
    "href": "transcriptomics/week-4/study_before_workshop.html#frog-development",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\n\n\nAn RStudio Project called frogs-88H which contains:\n\ndata-raw: xlaevis_counts_S14.csv, xlaevis_counts_S20.csv, xlaevis_counts_S30.csv\n\ndata-processed: s30_filtered.csv, s20_filtered.csv\n\nTwo scripts: cont-fgf-s30.R, cont-fgf-s20.R",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#arabidopsis",
    "href": "transcriptomics/week-4/study_before_workshop.html#arabidopsis",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ Arabidopsis\n",
    "text": "üéÑ Arabidopsis\n\n\n\nAn RStudio Project called arab-88H which contains:\n\ndata-raw: arabidopsis-wild.csv, arabidopsis-spl7.csv\n\ndata-processed: wild_filtered.csv, spl7_filtered.csv\n\nTwo scripts: suff-def-wild.R, suff-def-spl7.R",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#leishmania",
    "href": "transcriptomics/week-4/study_before_workshop.html#leishmania",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\n\n\nAn RStudio Project called leish-88H which contains:\n\ndata-raw: leishmania-mex-ama.csv, leishmania-mex-pro.csv, leishmania-mex-meta.csv\n\ndata-processed: pro_meta_filtered.csv, pro_ama_filtered.csv\n\nTwo scripts: pro_meta.R, pro_ama.R",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#stem-cells",
    "href": "transcriptomics/week-4/study_before_workshop.html#stem-cells",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\n\n\nAn RStudio Project called mice-88H which contains\n\ndata-raw: surfaceome_hspc.csv, surfaceome_prog.csv, surfaceome_lthsc.csv\n\ndata-processed: hspc_prog.csv, hspc_lthsc.csv\n\nTwo scripts: hspc-prog.R, hspc-lthsc.R",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#additionally",
    "href": "transcriptomics/week-4/study_before_workshop.html#additionally",
    "title": "Independent Study to prepare for workshop",
    "section": "Additionally‚Ä¶",
    "text": "Additionally‚Ä¶\nFiles should be organised into folders. Code should well commented and easy to read. You should have curated your code to remove unnecessary commands that were useful to troubleshoot or understand objects in your environment but which are not needed for the final analysis.\nIf you are missing files, go through:\n\nTranscriptomics 1: üëã Hello data! Workshop including:\nü§ó Look after future you! and\nthe Independent Study to consolidate",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#differential-expression-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#differential-expression-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Differential expression",
    "text": "Differential expression\n\n\nThe goal of differential expression is to test whether there is a significant difference in gene expression between groups.\nA large number of computational methods have been developed for differential expression analysis\nR is the leading language for differential expression analysis",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#differential-expression-2",
    "href": "transcriptomics/week-4/study_before_workshop.html#differential-expression-2",
    "title": "Independent Study to prepare for workshop",
    "section": "Differential expression",
    "text": "Differential expression\n\n\nthe statistical concepts are very similar to those you have already encountered in stages 1 and 2\nyou are essentially doing paired- or independent-samples tests\nbut you are doing a lot of them! One for every gene\ndata need normalisation before comparison",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#statistical-concepts",
    "href": "transcriptomics/week-4/study_before_workshop.html#statistical-concepts",
    "title": "Independent Study to prepare for workshop",
    "section": "Statistical concepts",
    "text": "Statistical concepts\nLike familiar tests:\n\n\nthe type of test (the function) you use depends on the type of data you have and the type of assumptions you want to make\nthe tests work by comparing the variation between groups to the variation within groups.\nyou will get: the difference between groups, a test statistic, and a p-value\nyou also get an adjusted p-value which is the ‚Äòcorrection‚Äô for multiple testing",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#the-difference-between-groups",
    "href": "transcriptomics/week-4/study_before_workshop.html#the-difference-between-groups",
    "title": "Independent Study to prepare for workshop",
    "section": "The difference between groups",
    "text": "The difference between groups\n\n\nThe difference between groups is given as the log2 fold change in expression between groups\nA fold change is the expression in one group divided by the expression in the other group: \\(\\frac{A}{B}\\)\nwe use fold changes because the absolute expression values may not be accurate and relative changes are what matters\nwe use log2 fold changes because they are symmetrical around 0",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#why-log2-fold-change",
    "href": "transcriptomics/week-4/study_before_workshop.html#why-log2-fold-change",
    "title": "Independent Study to prepare for workshop",
    "section": "Why log2 fold change?",
    "text": "Why log2 fold change?\n\n\nlog2 means log to the base 2\nSuppose the expression in group A is 5 and the expression in group B is 8\n\\(\\frac{A}{B} = \\frac{5}{8}\\) = 0.625 and \\(\\frac{B}{A} = \\frac{8}{5}\\) = 1.6\nIf B &gt; A the range of \\(\\frac{A}{B}\\) is 0 - 1 but the range of \\(\\frac{B}{A}\\) is 1 - \\(\\infty\\)\nHowever, if we take the log2 of \\(\\frac{A}{B}\\) we get -0.678 and the log2 of \\(\\frac{B}{A}\\) is 0.678.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#adjusted-p-value",
    "href": "transcriptomics/week-4/study_before_workshop.html#adjusted-p-value",
    "title": "Independent Study to prepare for workshop",
    "section": "Adjusted p-value",
    "text": "Adjusted p-value\n\n\nThe p-value has to be adjusted because of the number of tested being done\nIn stage 1, we used Tukey‚Äôs HSD to adjust for multiple testing following an ANOVA\nHere the Benjamini-Hochberg procedure (Benjamini and Hochberg 1995) is used to adjust for multiple testing\nBH controls the False Discovery Rate (FDR)\nThe FDR is the proportion of false positives among the genes called significant",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#normalisation",
    "href": "transcriptomics/week-4/study_before_workshop.html#normalisation",
    "title": "Independent Study to prepare for workshop",
    "section": "Normalisation",
    "text": "Normalisation\n\n\nNormalisation adjusts raw counts to account for factors that prevent direct comparisons\nNormalisation usually influences the experimental design as well as the analysis",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#normalisation-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#normalisation-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Normalisation",
    "text": "Normalisation\n\n\nüê≠ mice data are normalised\nüê∏ frog, üéÑ Arabidopisis and üíâ Leishmania data are raw counts (not normalised) because the differential expression method will do this.\nNormalisation is a big topic. See D√ºren, Lederer, and Qin (2022); Bullard et al. (2010); Lytal, Ran, and An (2020); Abrams et al. (2019); Vallejos et al. (2017); Evans, Hardin, and Stoebel (2017)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#type-of-de-tests",
    "href": "transcriptomics/week-4/study_before_workshop.html#type-of-de-tests",
    "title": "Independent Study to prepare for workshop",
    "section": "Type of DE tests",
    "text": "Type of DE tests\n\n\nA large number of computational methods have been developed for differential expression analysis\nMethods vary in the types of normalisation they do, the statistical model they use, and the assumptions they make\nSome of the most well-known methods are provided by: DESeq2 (Love, Huber, and Anders 2014), edgeR (Robinson, McCarthy, and Smyth 2010; McCarthy, Chen, and Smyth 2012; Chen, Lun, and Smyth 2016), limma (Ritchie et al. 2015) and scran (Lun, McCarthy, and Marioni 2016)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#type-of-test-the-function",
    "href": "transcriptomics/week-4/study_before_workshop.html#type-of-test-the-function",
    "title": "Independent Study to prepare for workshop",
    "section": "Type of test (the function)",
    "text": "Type of test (the function)\n\n\n\nDESeq2 and edgeR\n\nboth require raw counts as input\nboth assume that most genes are not DE\nboth use a negative binomial distribution1 to model the data\nuse slightly different normalisation methods: DESeq2 uses the median of ratios method; edgeR uses the trimmed mean of M values (TMM) method\n\n\n\n\nA discrete distribution for counts, similar to the Poisson distribution",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#type-of-test-the-function-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#type-of-test-the-function-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Type of test (the function)",
    "text": "Type of test (the function)\n\n\nscran\n\nworks on normalized log-expression values\nperforms Welch t-tests",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#meta-data",
    "href": "transcriptomics/week-4/study_before_workshop.html#meta-data",
    "title": "Independent Study to prepare for workshop",
    "section": "Meta data",
    "text": "Meta data\n\n\nDE methods require two types of data: the expression data and the meta data\nThe meta data gives the information about the samples\nIt says which samples (which columns of data) are in which treatment group (s)\nMeta data is usually stored in a separate file",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#frog-development-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#frog-development-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\n\nExpression for the whole transcriptome X. laevis v10.1 genome assembly\nValues are raw counts\nThe statistical analysis method we will use DESeq2 (Love, Huber, and Anders 2014) requires raw counts and performs the normalisation itself",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#arabidopisis",
    "href": "transcriptomics/week-4/study_before_workshop.html#arabidopisis",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ Arabidopisis\n",
    "text": "üéÑ Arabidopisis\n\n\nExpression for the whole transcriptome ENSEMBL Arabidopsis TAIR10(Yates et al. 2022)\nValues are raw counts\nThe statistical analysis method we will use DESeq2 (Love, Huber, and Anders 2014) requires raw counts and performs the normalisation itself",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#leishmania-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#leishmania-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\n\nExpression for the whole transcriptome L. mexicana MHOM/GT/2001/U1103(Rogers et al. 2011)\nValues are raw counts\nThe statistical analysis method we will use DESeq2 (Love, Huber, and Anders 2014) requires raw counts and performs the normalisation itself",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#stem-cells-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#stem-cells-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\n\nExpression for a subset of the transcriptome, the surfaceome\nValues are log2 normalised values\nThe statistical analysis method we will use scran (Lun, McCarthy, and Marioni 2016) requires normalised values",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#adding-gene-information-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#adding-gene-information-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Adding gene information",
    "text": "Adding gene information\n\n\nThe gene id is difficult to interpret\nTherefore we need to add information such as the gene name and a description to the results\n\n\n\nüê∏ Frog data information comes from Xenbase (Fisher et al. 2023)\nüéÑ Arabidopisis information comes from TAIR10 (Yates et al. 2022)\nüíâ Leishmania information comes TriTrypDB (Rogers et al. 2011)\nüê≠ Stem cell information comes from Ensembl (Birney et al. 2004)",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#xenbase",
    "href": "transcriptomics/week-4/study_before_workshop.html#xenbase",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Xenbase",
    "text": "üê∏ Xenbase\n\nxenbase logoXenbase is a model organism database that provides genomic, molecular, and developmental biology information about Xenopus laevis and Xenopus tropicalis.\n\nIt took me some time to find the information you need.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#xenbase-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#xenbase-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Xenbase",
    "text": "üê∏ Xenbase\n\n\nI got the information from the Xenbase information pages under Data Reports | Gene Information\nThis is listed: Xenbase Gene Product Information [readme] gzipped gpi (tab separated)\nClick on the readme link to see the file format and columns\nI downloaded xenbase.gpi.gz, unzipped it, removed header lines and the Xenopus tropicalis (taxon:8364) entries and saved it as xenbase_info.xlsx\nIn the workshop you will import this file and merge the information with the results file",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#tair10-through-ensembl",
    "href": "transcriptomics/week-4/study_before_workshop.html#tair10-through-ensembl",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ TAIR10 through Ensembl",
    "text": "üéÑ TAIR10 through Ensembl\n\n\nEnsembl creates, integrates and distributes reference datasets and analysis tools that enable genomics\nBioMart (Smedley et al. 2009) provides uniform access to these large datasets\nbiomaRt (Durinck et al. 2009, 2005) is a Bioconductor package gives you programmatic access to BioMart.\nIn the workshop you use this package to get information you can merge with the results file",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#tritrypdb",
    "href": "transcriptomics/week-4/study_before_workshop.html#tritrypdb",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ TriTrypDB",
    "text": "üíâ TriTrypDB\n\n\n\nI got the information from TriTrypDB\nwhich is a functional genomic resource for the Trypanosomatidae and Plasmodidae\nhttps://tritrypdb.org/tritrypdb/app/downloads section\nI downloaded the L. mexicana MHOM/GT/2001/U1103 Full GFF and extracted the gene information and saved it as leishmania_mex.xlsx\nIn the workshop you will import this file and merge the information with the results file",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#ensembl",
    "href": "transcriptomics/week-4/study_before_workshop.html#ensembl",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Ensembl",
    "text": "üê≠ Ensembl\n\n\nEnsembl creates, integrates and distributes reference datasets and analysis tools that enable genomics\nBioMart (Smedley et al. 2009) provides uniform access to these large datasets\nbiomaRt (Durinck et al. 2009, 2005) is a Bioconductor package gives you programmatic access to BioMart.\nIn the workshop you use this package to get information you can merge with the results file",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#packages",
    "href": "transcriptomics/week-4/study_before_workshop.html#packages",
    "title": "Independent Study to prepare for workshop",
    "section": "Packages",
    "text": "Packages\nThese packages are all on the University computers which you can access on campus or remotely using the VDS\nIf you want to use your own machine you will need to install the packages.\n\nInstall BiocManager from CRAN in the the normal way and set the version of Bioconductor packages to install:\n\ninstall.packages(\"BiocManager\")\nBiocManager::install(version = \"3.19\")\n\nInstall DESeq2 from Bioconductor using BiocManager:\n\nBiocManager::install(\"DESeq2\")\n\nInstall scran from Bioconductor using BiocManager:\n\nBiocManager::install(\"scran\")\n\nInstall biomaRt from Bioconductor using BiocManager:\n\nBiocManager::install(\"biomaRt\")",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#workshops-1",
    "href": "transcriptomics/week-4/study_before_workshop.html#workshops-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Workshops",
    "text": "Workshops\n\nTranscriptomics 1: Hello data. Getting to know the data. Checking the distributions of values overall, across rows and columns to check things are as we expect and detect rows/columns that need to be removed\nTranscriptomics 2: Statistical Analysis. Identifying which genes are differentially expressed between treatments. This is the main analysis step. We will use different methods for bulk and single cell data.\nTranscriptomics 3: Visualising. Principal Component Analysis (PCA) volcano plots to visualise the results of the",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-4/study_before_workshop.html#references",
    "href": "transcriptomics/week-4/study_before_workshop.html#references",
    "title": "Independent Study to prepare for workshop",
    "section": "References",
    "text": "References\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr (Xie 2024, 2015, 2014), kableExtra (Zhu 2021)\n\n\n\nüîó About Transcriptomics 2: Statistical Analysis\n\n\n\n\nAbrams, Zachary B., Travis S. Johnson, Kun Huang, Philip R. O. Payne, and Kevin Coombes. 2019. ‚ÄúA Protocol to Evaluate RNA Sequencing Normalization Methods.‚Äù BMC Bioinformatics 20 (24): 679. https://doi.org/10.1186/s12859-019-3247-x.\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2024. ‚ÄúQuarto.‚Äù https://doi.org/10.5281/zenodo.5960048.\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. ‚ÄúControlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.‚Äù J. R. Stat. Soc. Series B Stat. Methodol. 57 (1): 289‚Äì300. http://www.jstor.org/stable/2346101.\n\n\nBirney, Ewan, T. Daniel Andrews, Paul Bevan, Mario Caccamo, Yuan Chen, Laura Clarke, Guy Coates, et al. 2004. ‚ÄúAn Overview of Ensembl.‚Äù Genome Research 14 (5): 925‚Äì28. https://doi.org/10.1101/gr.1860604.\n\n\nBullard, James H., Elizabeth Purdom, Kasper D. Hansen, and Sandrine Dudoit. 2010. ‚ÄúEvaluation of Statistical Methods for Normalization and Differential Expression in mRNA-Seq Experiments.‚Äù BMC Bioinformatics 11 (1): 94. https://doi.org/10.1186/1471-2105-11-94.\n\n\nChen, Yunshun, Aaron T. L. Lun, and Gordon K. Smyth. 2016. ‚ÄúFrom Reads to Genes to Pathways: Differential Expression Analysis of RNA-Seq Experiments Using Rsubread and the edgeR Quasi-Likelihood Pipeline.‚Äù https://doi.org/10.12688/f1000research.8987.2.\n\n\nD√ºren, Yannick, Johannes Lederer, and Li-Xuan Qin. 2022. ‚ÄúDepth Normalization of Small RNA Sequencing: Using Data and Biology to Select a Suitable Method.‚Äù Nucleic Acids Research 50 (10): e56. https://doi.org/10.1093/nar/gkac064.\n\n\nDurinck, Steffen, Yves Moreau, Arek Kasprzyk, Sean Davis, Bart De Moor, Alvis Brazma, and Wolfgang Huber. 2005. ‚ÄúBioMart and Bioconductor: A Powerful Link Between Biological Databases and Microarray Data Analysis.‚Äù Bioinformatics 21: 3439‚Äì40.\n\n\nDurinck, Steffen, Paul T. Spellman, Ewan Birney, and Wolfgang Huber. 2009. ‚ÄúMapping Identifiers for the Integration of Genomic Datasets with the r/Bioconductor Package biomaRt.‚Äù Nature Protocols 4: 1184‚Äì91.\n\n\nEvans, Ciaran, Johanna Hardin, and Daniel M Stoebel. 2017. ‚ÄúSelecting Between-Sample RNA-Seq Normalization Methods from the Perspective of Their Assumptions.‚Äù Briefings in Bioinformatics 19 (5): 776‚Äì92. https://doi.org/10.1093/bib/bbx008.\n\n\nFisher, Malcolm, Christina James-Zorn, Virgilio Ponferrada, Andrew J Bell, Nivitha Sundararaj, Erik Segerdell, Praneet Chaturvedi, et al. 2023. ‚ÄúXenbase: Key Features and Resources of the Xenopus Model Organism Knowledgebase.‚Äù Genetics 224 (1): iyad018. https://doi.org/10.1093/genetics/iyad018.\n\n\nLove, Michael I., Wolfgang Huber, and Simon Anders. 2014. ‚ÄúModerated Estimation of Fold Change and Dispersion for RNA-Seq Data with DESeq2.‚Äù Genome Biology 15: 550. https://doi.org/10.1186/s13059-014-0550-8.\n\n\nLun, Aaron T. L., Davis J. McCarthy, and John C. Marioni. 2016. ‚ÄúA Step-by-Step Workflow for Low-Level Analysis of Single-Cell RNA-Seq Data with Bioconductor.‚Äù F1000Res. 5: 2122. https://doi.org/10.12688/f1000research.9501.2.\n\n\nLytal, Nicholas, Di Ran, and Lingling An. 2020. ‚ÄúNormalization Methods on Single-Cell RNA-Seq Data: An Empirical Survey.‚Äù Frontiers in Genetics 11. https://www.frontiersin.org/articles/10.3389/fgene.2020.00041.\n\n\nMcCarthy, Davis J., Yunshun Chen, and Gordon K. Smyth. 2012. ‚ÄúDifferential Expression Analysis of Multifactor RNA-Seq Experiments with Respect to Biological Variation.‚Äù Nucleic Acids Research 40 (10): 4288‚Äì97. https://doi.org/10.1093/nar/gks042.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRitchie, Matthew E., Belinda Phipson, Di Wu, Yifang Hu, Charity W. Law, Wei Shi, and Gordon K. Smyth. 2015. ‚ÄúLimma Powers Differential Expression Analyses for RNA-Sequencing and Microarray Studies.‚Äù Nucleic Acids Research 43 (7): e47. https://doi.org/10.1093/nar/gkv007.\n\n\nRobinson, Mark D., Davis J. McCarthy, and Gordon K. Smyth. 2010. ‚ÄúedgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.‚Äù Bioinformatics 26 (1): 139‚Äì40. https://doi.org/10.1093/bioinformatics/btp616.\n\n\nRogers, Matthew B., James D. Hilley, Nicholas J. Dickens, Jon Wilkes, Paul A. Bates, Daniel P. Depledge, David Harris, et al. 2011. ‚ÄúChromosome and Gene Copy Number Variation Allow Major Structural Change Between Species and Strains of Leishmania.‚Äù Genome Research 21 (12): 2129‚Äì42. https://doi.org/10.1101/gr.122945.111.\n\n\nSmedley, Damian, Syed Haider, Benoit Ballester, Richard Holland, Darin London, Gudmundur Thorisson, and Arek Kasprzyk. 2009. ‚ÄúBioMart  Biological Queries Made Easy.‚Äù BMC Genomics 10 (1): 22. https://doi.org/10.1186/1471-2164-10-22.\n\n\nVallejos, Catalina A., Davide Risso, Antonio Scialdone, Sandrine Dudoit, and John C. Marioni. 2017. ‚ÄúNormalizing Single-Cell RNA Sequencing Data: Challenges and Opportunities.‚Äù Nature Methods 14 (6): 565‚Äì71. https://doi.org/10.1038/nmeth.4292.\n\n\nXie, Yihui. 2014. ‚ÄúKnitr: A Comprehensive Tool for Reproducible Research in R.‚Äù In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\n‚Äî‚Äî‚Äî. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\n‚Äî‚Äî‚Äî. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nYates, Andrew D, James Allen, Ridwan M Amode, Andrey G Azov, Matthieu Barba, Andr√©s Becerra, Jyothish Bhai, et al. 2022. ‚ÄúEnsembl Genomes 2022: An Expanding Genome Resource for Non-Vertebrates.‚Äù Nucleic Acids Research 50 (D1): D996‚Äì1003. https://doi.org/10.1093/nar/gkab1007.\n\n\nZhu, Hao. 2021. ‚ÄúkableExtra: Construct Complex Table with ‚ÄôKable‚Äô and Pipe Syntax.‚Äù https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Transcriptomics",
      "Week 4: Statistical Analysis",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_after_workshop.html",
    "href": "transcriptomics/week-5/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "You need only do the section for one of the examples.\nYou need only do the section for your own project data\nüê∏ Frog development\nüé¨ Open your frogs-88H Project and the cont-fgf-s20.R script you began in the Consolidation study of Transcriptomics 1 and continued to work on in Transcriptomics 2. Use the code you used in the workshop (in cont-fgf-s30.R) as a template to visualise the s20 results.\nüéÑ Arabidopisis\nüé¨ Open your arab-88H RStudio Project and the suff-def-spl7.R script you began in the Consolidation study of Transcriptomics 1 and continued to work on in Transcriptomics 2. Use the code you used in the workshop (in suff-def-wild.R) as a template to visualise the results for the comparison between the copper sufficient and deficient conditions in spl7.\nüíâ Leishmania\nüé¨ Open your leish-88H RStudio Project and the pro_ama.R script you began in the Consolidation study of Transcriptomics 1 and continued to work on in Transcriptomics 2. Use the code you used in the workshop (in pro_meta.R) as a template to visualise the results for the comparison between the procyclic promastigote and amastigote stages.\nüê≠ Stem cells\nüé¨ Open your mice-88H RStudio Project and the hspc-lthsc.R script you began in the Consolidation study of Transcriptomics 1 and continued to work on in Transcriptomics 2. Use the code you used in the workshop (in hspc-prog.R) as a template to visualise the results for the comparison between HSPC and LT-HSPC cells",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Consolidate!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#overview",
    "href": "transcriptomics/week-5/study_before_workshop.html#overview",
    "title": "Independent Study to prepare for workshop",
    "section": "Overview",
    "text": "Overview\nIn these slides we will:\n\n\nCheck where you are\n\nlearn some concepts used omics visualisation\n\nPrinciple Component Analysis (PCA)\nVolcano plots\n\n\nFind out what packages to install before the workshop",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#what-we-did-in-transcriptomics-2-statistical-analysis",
    "href": "transcriptomics/week-5/study_before_workshop.html#what-we-did-in-transcriptomics-2-statistical-analysis",
    "title": "Independent Study to prepare for workshop",
    "section": "What we did in Transcriptomics 2: Statistical Analysis",
    "text": "What we did in Transcriptomics 2: Statistical Analysis\n\n\ncarried out differential expression analysis\nfound genes not expressed at all, or expressed in one group only\nSaved results files",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#where-should-you-be-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#where-should-you-be-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Where should you be?",
    "text": "Where should you be?\nAfter the Transcriptomics 2: üëã Statistical Analysis Workshop including:\n\nü§ó Look after future you! and\nthe Independent Study to consolidate, you should have:",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#frog-development",
    "href": "transcriptomics/week-5/study_before_workshop.html#frog-development",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ Frog development",
    "text": "üê∏ Frog development\n\n\nAn RStudio Project called frogs-88H which contains:\n\ndata-raw: xlaevis_counts_S14.csv, xlaevis_counts_S20.csv, xlaevis_counts_S30.csv\n\ndata-processed: s30_filtered.csv, s20_filtered.csv\n\nresults: s30_fgf_only.csv (there were no control only genes in s30), s30_results.csv, and equivalent for S20\n\nTwo scripts: cont-fgf-s30.R, cont-fgf-s20.R",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#arabidopisis",
    "href": "transcriptomics/week-5/study_before_workshop.html#arabidopisis",
    "title": "Independent Study to prepare for workshop",
    "section": "üéÑ Arabidopisis\n",
    "text": "üéÑ Arabidopisis\n\n\n\nAn RStudio Project called arab-88H which contains:\n\ndata-raw: arabidopsis-wild.csv, arabidopsis-spl7.csv\n\ndata-processed: wild_filtered.csv, spl7_filtered.csv\n\nresults: wild_suf_only.csv, wild-_def_only.csv, wild_results.csv, and equivalents for spl7\nTwo scripts: suff-def-wild.R, suff-def-spl7.R",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#leishmania",
    "href": "transcriptomics/week-5/study_before_workshop.html#leishmania",
    "title": "Independent Study to prepare for workshop",
    "section": "üíâ Leishmania\n",
    "text": "üíâ Leishmania\n\n\n\nAn RStudio Project called leish-88H which contains:\n\ndata-raw: leishmania-mex-ama.csv, leishmania-mex-pro.csv, leishmania-mex-meta.csv\n\ndata-processed: pro_meta_filtered.csv, pro_ama_filtered.csv\n\nresults: pro_meta_results.csv, pro_ama_results.csv\n\nTwo scripts: pro_meta.R, pro_ama.R",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#stem-cells",
    "href": "transcriptomics/week-5/study_before_workshop.html#stem-cells",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\n\n\nAn RStudio Project called mice-88H which contains:\n\ndata-raw: surfaceome_hspc.csv, surfaceome_prog.csv, surfaceome_lthsc.csv\n\ndata-processed: hspc_prog.csv, hspc_lthsc.csv\n\nresults: hspc_prog_results.csv, hspc-lthsc_results.csv,\nTwo scripts: hspc-prog.R, hspc-lthsc.R",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#additionally",
    "href": "transcriptomics/week-5/study_before_workshop.html#additionally",
    "title": "Independent Study to prepare for workshop",
    "section": "Additionally‚Ä¶",
    "text": "Additionally‚Ä¶\nFiles should be organised into folders. Code should well commented and easy to read. You should have curated your code to remove unnecessary commands that were useful to troubleshoot or understand objects in your environment but which are not needed for the final analysis.\nIf you are missing files, go through:\nGo through:\n\nTranscriptomics 2: Statistical Analysis including:\nü§ó Look after future you! and\nthe Independent Study to consolidate",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#all-results-files",
    "href": "transcriptomics/week-5/study_before_workshop.html#all-results-files",
    "title": "Independent Study to prepare for workshop",
    "section": "All results files",
    "text": "All results files\nRemind yourself of the key columns in any of the results files:\n\nnormalised counts for each sample/cell\na log2 fold change\nan unadjusted p-value\na p value adjusted for multiple testing (called FDR or padj)\na gene id\nother information about each gene",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#results-files",
    "href": "transcriptomics/week-5/study_before_workshop.html#results-files",
    "title": "Independent Study to prepare for workshop",
    "section": "üê∏ , üéÑ , üíâ results files",
    "text": "üê∏ , üéÑ , üíâ results files\n\n\nbaseMean is the mean of the normalised counts for the gene across all samples\n\nlfcSE standard error of the fold change\n\nstat is the test statistic (the Wald statistic)",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#stem-cells-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#stem-cells-1",
    "title": "Independent Study to prepare for workshop",
    "section": "üê≠ Stem cells",
    "text": "üê≠ Stem cells\n\nTop is the rank of the gene ordered by the p-value (smallest first)\n\nsummary.logFC and logFC.hspc give the same value (in this case since comparing two cell types)",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#what-is-the-purpose-of-a-transcriptomics-plot",
    "href": "transcriptomics/week-5/study_before_workshop.html#what-is-the-purpose-of-a-transcriptomics-plot",
    "title": "Independent Study to prepare for workshop",
    "section": "What is the purpose of a Transcriptomics plot?",
    "text": "What is the purpose of a Transcriptomics plot?\n\n\nIn general, we plot data to help us summarise and understand it\nThis is especially import for transcriptomics data where we have a very large number of variables and often a large number of observations\nWe will look at two plots very commonly used in transcriptomics analysis: Principal Component Analysis (PCA) plot and Volcano Plots",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\n\n\nPrincipal Component Analysis is an unsupervised machine learning technique\nUnsupervised methods1 are unsupervised in that they do not use/optimise to a particular output. The goal is to uncover structure. They do not test hypotheses\nIt is often used to visualise high dimensional data because it is a dimension reduction technique\n\n\nYou may wish to read a previous introduction to unsupervised methods I have written An introduction to Machine Learning: Unsupervised methods (Rand 2021)",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca-1",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\n\n\nTakes a large number of continuous variables (like gene expression) and reduces them to a smaller number of variables (called principal components) that explain most of the variation in the data\nThe principal components can be plotted to see how samples cluster together",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca-2",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca-2",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\n\n\nTo understand the logic of PCA, imagine we might plot the expression of one gene against that of another\n\n\n\n\n\n\n\n\n\nSamples\n\n\n\n\n\nCells\n\n\n\n\nThis gives us some in insight in how the sample/cells cluster. But we have a lot of genes (even for the stem cells) to consider. How do we know if the pair we use is typical? How can we consider all the genes at once?",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca-3",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca-3",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\n\n\nPCA is a solution for this - It takes a large number of continuous variables (like gene expression) and reduces them to a smaller number of ‚Äúprincipal components‚Äù that explain most of the variation in the data.\n\n\n\n\n\n\n\n\n\nSamples\n\n\n\n\n\nCells",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#pca-4",
    "href": "transcriptomics/week-5/study_before_workshop.html#pca-4",
    "title": "Independent Study to prepare for workshop",
    "section": "PCA",
    "text": "PCA\nWe have done PCA after differential expression, but often PCA might is one of the first exploratory steps because it gives you an idea whether you expect general patterns in gene expression that distinguish groups.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Volcano plots",
    "text": "Volcano plots\n\n\nVolcano plots often used to visualise the results of differential expression analysis\nThey are just a scatter of the adjusted p value against the fold change‚Ä¶.\nalmost - in fact we plot the negative log of the adjusted p value against the log fold change",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-2",
    "href": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-2",
    "title": "Independent Study to prepare for workshop",
    "section": "Volcano plots",
    "text": "Volcano plots\n\n\nThis is because small probabilities are important, large ones are not so the axis is counter intuitive because small p-values (i.e., significant values) are at the bottom of the axis)\nAnd since p-values range from 1 to very tiny the important points are all squashed at the bottom of the axis\n\n\n\nVolcano plot padj against fold change",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-3",
    "href": "transcriptomics/week-5/study_before_workshop.html#volcano-plots-3",
    "title": "Independent Study to prepare for workshop",
    "section": "Volcano plots",
    "text": "Volcano plots\n\n\nBy plotting the negative log of the adjusted p-value the values are spread out, and the most significant are at the top of the axis\n\n\n\nVolcano plot -log(adjusted p) against fold change",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#visualisations",
    "href": "transcriptomics/week-5/study_before_workshop.html#visualisations",
    "title": "Independent Study to prepare for workshop",
    "section": "Visualisations",
    "text": "Visualisations\n\nShould be done on normalised data so meaningful comparisons can be made\nThe üê≠ stem cell data were already log2normalised\nThe other datasets were normalised by the DE method and we saved the values to the results files. We will log transform them in the workshop",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#packages",
    "href": "transcriptomics/week-5/study_before_workshop.html#packages",
    "title": "Independent Study to prepare for workshop",
    "section": "Packages",
    "text": "Packages\nThis package is on the University computers which you can access on campus or remotely using the VDS\nIf you want to use your own machine you will need to install the package.\n\nInstall ggrepel from CRAN in the the normal way:\n\ninstall.packages(\"ggrepel\")\n\n\nThis package allows you to label points on a plot without them overlapping.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#workshops-1",
    "href": "transcriptomics/week-5/study_before_workshop.html#workshops-1",
    "title": "Independent Study to prepare for workshop",
    "section": "Workshops",
    "text": "Workshops\n\nTranscriptomics 1: Hello data Getting to know the data. Checking the distributions of values overall, across rows and columns to check things are as we expect and detect rows/columns that need to be removed\nTranscriptomics 2: Statistical Analysis. Identifying which genes are differentially expressed between treatments. This is the main analysis step. We will use different methods for bulk and single cell data.\nTranscriptomics 3: Visualising. Principal Component Analysis (PCA) volcano plots to visualise the results of the",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/week-5/study_before_workshop.html#references",
    "href": "transcriptomics/week-5/study_before_workshop.html#references",
    "title": "Independent Study to prepare for workshop",
    "section": "References",
    "text": "References\n\n\n\nüîó About Transcriptomics 3: Visualising\n\n\n\n\nRand, Emma. 2021. Data Science Strand of BIO00058M. https://doi.org/10.5281/zenodo.5527705.",
    "crumbs": [
      "Transcriptomics",
      "Week 5: Visualising and Interpreting",
      "Prepare!"
    ]
  },
  {
    "objectID": "transcriptomics/transcriptomics.html",
    "href": "transcriptomics/transcriptomics.html",
    "title": "Transcriptomics Data Analysis for Group Project",
    "section": "",
    "text": "This week you will meet your data. There are four datasets, one for each project in this strand. The independent study will concisely cover how each of these four data sets were generated and how they have been processed before being given to you. It will also give an overview of the analysis we will carry out over three workshops. In the workshop, you will learn what steps to take to get a good understanding of transciptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It will give you the understanding of the data and R data structures that you will need to code and trouble-shoot code. It will also allow you to spot failed or problematic samples and will inform your decisions on quality control. At the end of this workshop and the following independent study you will have performed quality control by filtering out uninformative genes and samples, and saved this filtered data for use in the next workshop. You will also have a script that you can use to repeat this process on other datasets.\n\n\n\nThis week we cover differential expression analysis on your quality controlled data. The independent study will allow you to check you have what you should have following the Transcriptomics 1: Hello Data workshop and Consolidation study. It then summarises the concepts and methods used to carry out differential expression analysis in workshop. In the workshop, you will perform the differential expression and learn how to compuationally annotate your genes with more information from the databases. This will include the Gene Ontology (GO) terms that describe the biological processes, molecular functions and cellular components that the gene is involved in. At the end of this workshop and the following independent study you will have files containing the genes which are differentially expressed, along with the statistical information, summary information and annotation. You will be able to consider which genes you want to investigates with your Project director and have what you need for the next workshop. You will also have a script that you can use to repeat this process on other datasets.\n\n\n\nThis week you will learn some how to do some common data visualisations for transcriptomic data. You will conduct and present a Principal Component Analysis (PCA) and a Volcano plot. We will also conduct a GO enrichment analysis. The independent study will allow you to check you have what you should have following the Transcriptomics 2: Statistical Analysis workshop and Consolidation study. At the end of this workshop and the following independent study you will at least two figures suitable for including in your report, along with an understanding of the results you can report on. You will also have a script that you can use to repeat this process on other datasets.\nReferences",
    "crumbs": [
      "Transcriptomics",
      "Transcriptomics Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "transcriptomics/transcriptomics.html#transcriptomics-1-hello-data",
    "href": "transcriptomics/transcriptomics.html#transcriptomics-1-hello-data",
    "title": "Transcriptomics Data Analysis for Group Project",
    "section": "",
    "text": "This week you will meet your data. There are four datasets, one for each project in this strand. The independent study will concisely cover how each of these four data sets were generated and how they have been processed before being given to you. It will also give an overview of the analysis we will carry out over three workshops. In the workshop, you will learn what steps to take to get a good understanding of transciptomics data before you consider any statistical analysis. This is an often overlooked, but very valuable and informative, part of any data pipeline. It will give you the understanding of the data and R data structures that you will need to code and trouble-shoot code. It will also allow you to spot failed or problematic samples and will inform your decisions on quality control. At the end of this workshop and the following independent study you will have performed quality control by filtering out uninformative genes and samples, and saved this filtered data for use in the next workshop. You will also have a script that you can use to repeat this process on other datasets.",
    "crumbs": [
      "Transcriptomics",
      "Transcriptomics Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "transcriptomics/transcriptomics.html#transcriptomics-2-statistical-analysis",
    "href": "transcriptomics/transcriptomics.html#transcriptomics-2-statistical-analysis",
    "title": "Transcriptomics Data Analysis for Group Project",
    "section": "",
    "text": "This week we cover differential expression analysis on your quality controlled data. The independent study will allow you to check you have what you should have following the Transcriptomics 1: Hello Data workshop and Consolidation study. It then summarises the concepts and methods used to carry out differential expression analysis in workshop. In the workshop, you will perform the differential expression and learn how to compuationally annotate your genes with more information from the databases. This will include the Gene Ontology (GO) terms that describe the biological processes, molecular functions and cellular components that the gene is involved in. At the end of this workshop and the following independent study you will have files containing the genes which are differentially expressed, along with the statistical information, summary information and annotation. You will be able to consider which genes you want to investigates with your Project director and have what you need for the next workshop. You will also have a script that you can use to repeat this process on other datasets.",
    "crumbs": [
      "Transcriptomics",
      "Transcriptomics Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "transcriptomics/transcriptomics.html#transcriptomics-3-visualising-and-interpreting",
    "href": "transcriptomics/transcriptomics.html#transcriptomics-3-visualising-and-interpreting",
    "title": "Transcriptomics Data Analysis for Group Project",
    "section": "",
    "text": "This week you will learn some how to do some common data visualisations for transcriptomic data. You will conduct and present a Principal Component Analysis (PCA) and a Volcano plot. We will also conduct a GO enrichment analysis. The independent study will allow you to check you have what you should have following the Transcriptomics 2: Statistical Analysis workshop and Consolidation study. At the end of this workshop and the following independent study you will at least two figures suitable for including in your report, along with an understanding of the results you can report on. You will also have a script that you can use to repeat this process on other datasets.\nReferences",
    "crumbs": [
      "Transcriptomics",
      "Transcriptomics Data Analysis for Group Project"
    ]
  },
  {
    "objectID": "core/week-6/study_after_workshop.html",
    "href": "core/week-6/study_after_workshop.html",
    "title": "Independent Study to consolidate this week",
    "section": "",
    "text": "These are suggestions",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Consolidate!"
    ]
  },
  {
    "objectID": "core/week-6/study_after_workshop.html#bio00088h-group-research-project-students",
    "href": "core/week-6/study_after_workshop.html#bio00088h-group-research-project-students",
    "title": "Independent Study to consolidate this week",
    "section": "BIO00088H Group Research Project students",
    "text": "BIO00088H Group Research Project students\n\nRevise previous Data Analysis materials. You can find the version you took on the VLE site for 17C / 08C. However, my latest versions (in development) are here: Data Analysis in R. The Becoming a Bioscientist (BABS) modules replace the Laboratory and Professional Skills modules. BABS1 and BABS2 are stage one, and I‚Äôve tried to improve them over 17C / 08C. The site is also searchable (icon top right)",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Consolidate!"
    ]
  },
  {
    "objectID": "core/week-6/study_after_workshop.html#msc-bioinformatics-students-doing-bio00070m",
    "href": "core/week-6/study_after_workshop.html#msc-bioinformatics-students-doing-bio00070m",
    "title": "Independent Study to consolidate this week",
    "section": "MSc Bioinformatics students doing BIO00070M",
    "text": "MSc Bioinformatics students doing BIO00070M\n\nMake sure you carry out the preparatory work for week 2 of 52M",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Consolidate!"
    ]
  },
  {
    "objectID": "core/week-6/study_before_workshop.html",
    "href": "core/week-6/study_before_workshop.html",
    "title": "Independent Study to prepare for workshop",
    "section": "",
    "text": "üìñ Read Understanding file systems. This is an approximately 15 - 20 minute read revising file types and filesystems. It covers concepts of working directories and paths. We learned these ideas in stage 1 and you may feel completely confident with them but many students will benefit from a refresher. For BIO00070M students, this is part of the work you will also be asked to complete for BIO00052M Data Analysis in R.\nIn previous years you have submitted and RStudio Project as part of your BABS work. In this module you will develop this by submitting a Research Compendium. A Research Compendium is a documented collection of all the digital parts of the research project including data (or access to data), code and outputs. The Compendium might be a single Quarto/RStudio Project, (like you have done previously but with better documentation) or it might be a folder including an Quarto/RStudio Project and other material/scripts including the description of unscripted processing. You might want to remind yourself of the example RStudio Project, Y12345678.zip used in BABS 2.",
    "crumbs": [
      "Core Supporting Info",
      "Week 6: Supporting Information 2",
      "Prepare!"
    ]
  },
  {
    "objectID": "core/core.html",
    "href": "core/core.html",
    "title": "Core: Supporting Information",
    "section": "",
    "text": "futureself, CC-BY-NC, by Julen Colomb\n\n\nThere are two workshops taken by everyone on BIO00088H. These are in weeks 2 and 6. These are important in understanding both how to assemble, curate and document your ‚ÄúSupporting Information‚Äù and how to work reproducibly so future you (Spring semester you) can painlessly work with past you and your work is demonstrably repeatable. This is essential because you will want to be able to set work aside for holidays and assessment periods and then restart easily. The Supporting Information you submit with your Report will be be assessed on its organisation, reproducibility and documentation.\nBIO00070M students do week 1 and 6 of the core workshops along with weeks 3, 4 and 5 of transcriptomics.\n\n\nWhy reproducibility matters, project-oriented workflow, organisation and naming things. You will also learn how to recognise and write cool üòé code, not üò© ugly code and code algorithmically and discover some awesome short cuts to help you write cool üòé code.\n\n\n\nDocumenting your Supporting Information with a read me and appropriate code commenting, curating code, non-coded processing",
    "crumbs": [
      "Core Supporting Info",
      "Core: Supporting Information"
    ]
  },
  {
    "objectID": "core/core.html#week-1-core-supporting-information-1",
    "href": "core/core.html#week-1-core-supporting-information-1",
    "title": "Core: Supporting Information",
    "section": "",
    "text": "Why reproducibility matters, project-oriented workflow, organisation and naming things. You will also learn how to recognise and write cool üòé code, not üò© ugly code and code algorithmically and discover some awesome short cuts to help you write cool üòé code.",
    "crumbs": [
      "Core Supporting Info",
      "Core: Supporting Information"
    ]
  },
  {
    "objectID": "core/core.html#week-6-core-supporting-information-2",
    "href": "core/core.html#week-6-core-supporting-information-2",
    "title": "Core: Supporting Information",
    "section": "",
    "text": "Documenting your Supporting Information with a read me and appropriate code commenting, curating code, non-coded processing",
    "crumbs": [
      "Core Supporting Info",
      "Core: Supporting Information"
    ]
  },
  {
    "objectID": "core/week-1-old/workshop.html",
    "href": "core/week-1-old/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop we will discuss why reproducibility matters and how to organise your work to make it reproducible. We will cover:"
  },
  {
    "objectID": "core/week-1-old/workshop.html#session-overview",
    "href": "core/week-1-old/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop we will discuss why reproducibility matters and how to organise your work to make it reproducible. We will cover:"
  },
  {
    "objectID": "core/week-1-old/workshop.html#what-is-reproducibility",
    "href": "core/week-1-old/workshop.html#what-is-reproducibility",
    "title": "Workshop",
    "section": "What is reproducibility?",
    "text": "What is reproducibility?\n\nReproducible: Same data + same analysis = identical results. ‚Äú‚Ä¶ obtaining consistent results using the same input data; computational steps, methods, and code; and conditions of analysis. This definition is synonymous with‚Äùcomputational reproducibility‚Äù (National Academies of Sciences et al. 2019)\nReplicable: Different data + same analysis = qualitatively similar results. The work is not dependent on the specificities of the data.\nRobust: Same data + different analysis = qualitatively similar or identical results. The work is not dependent on the specificities of the analysis.\nGeneralisable: Different data + different analysis = qualitatively similar results and same conclusions. The findings can be generalised\n\n\n\n\nThe Turing Way's definitions of reproducible research"
  },
  {
    "objectID": "core/week-1-old/workshop.html#why-does-it-matter",
    "href": "core/week-1-old/workshop.html#why-does-it-matter",
    "title": "Workshop",
    "section": "Why does it matter?",
    "text": "Why does it matter?\n\n\n\nfutureself, CC-BY-NC, by Julen Colomb\n\n\n\nFive selfish reasons to work reproducibly (Markowetz 2015). Alternatively, see the very entertaining talk\nMany high profile cases of work which did not reproduce e.g.¬†Anil Potti unravelled by Baggerly and Coombes (2009)\nWill become standard in Science and publishing e.g OECD Global Science Forum Building digital workforce capacity and skills for data-intensive science (OECD Global Science Forum 2020)"
  },
  {
    "objectID": "core/week-1-old/workshop.html#how-to-achieve-reproducibility",
    "href": "core/week-1-old/workshop.html#how-to-achieve-reproducibility",
    "title": "Workshop",
    "section": "How to achieve reproducibility",
    "text": "How to achieve reproducibility\n\nScripting\nOrganisation: Project-oriented workflows with file and folder structure, naming things\nDocumentation: Readme files, code comments, metadata, version control"
  },
  {
    "objectID": "core/week-1-old/workshop.html#rationale-for-scripting",
    "href": "core/week-1-old/workshop.html#rationale-for-scripting",
    "title": "Workshop",
    "section": "Rationale for scripting?",
    "text": "Rationale for scripting?\n\nScience is the generation of ideas, designing work to test them and reporting the results.\nWe ensure laboratory and field work is replicable, robust and generalisable by planning and recording in lab books and using standard protocols. Repeating results is still hard.\nWorkflows for computational projects, and the data analysis and reporting of other work can, and should, be 100% reproducible!\nScripting is the way to achieve this."
  },
  {
    "objectID": "core/week-1-old/workshop.html#project-oriented-workflow",
    "href": "core/week-1-old/workshop.html#project-oriented-workflow",
    "title": "Workshop",
    "section": "Project-oriented workflow",
    "text": "Project-oriented workflow\n\nuse folders to organise your work\nyou are aiming for structured, systematic and repeatable.\ninputs and outputs should be clearly identifiable from structure and/or naming\n\nExamples\n-- liver_transcriptome/\n   |__data\n      |__raw/\n      |__processed/\n   |__images/\n   |__code/\n   |__reports/\n   |__figures/"
  },
  {
    "objectID": "core/week-1-old/workshop.html#naming-things",
    "href": "core/week-1-old/workshop.html#naming-things",
    "title": "Workshop",
    "section": "Naming things",
    "text": "Naming things\n\n\n\ndocuments, CC-BY-NC, https://xkcd.com/1459/\n\n\nGuiding principle - Have a convention! Good file names are:\n\nmachine readable\nhuman readable\nplay nicely with sorting\n\nI suggest\n\nno spaces in names\nuse snake_case or kebab-case rather than CamelCase or dot.case\nuse all lower case except very occasionally where convention is otherwise, e.g., README, LICENSE\nordering: use left-padded numbers e.g., 01, 02‚Ä¶.99 or 001, 002‚Ä¶.999\ndates ISO 8601 format: 2020-10-16\nwrite down your conventions\n\n-- liver_transcriptome/\n   |__data\n      |__raw/\n         |__2022-03-21_donor_1.csv\n         |__2022-03-21_donor_2.csv\n         |__2022-03-21_donor_3.csv\n         |__2022-05-14_donor_1.csv\n         |__2022-05-14_donor_2.csv\n         |__2022-05-14_donor_3.csv\n      |__processed/\n   |__images/\n   |__code/\n      |__functions/\n         |__summarise.R\n         |__normalise.R\n         |__theme_volcano.R\n      |__01_data_processing.py\n      |__02_exploratory.R\n      |__03_modelling.R\n      |__04_figures.R\n   |__reports/\n      |__01_report.qmd\n      |__02_supplementary.qmd\n   |__figures/\n      |__01_volcano_donor_1_vs_donor_2.eps\n      |__02_volcano_donor_1_vs_donor_3.eps"
  },
  {
    "objectID": "core/week-1-old/workshop.html#readme-files",
    "href": "core/week-1-old/workshop.html#readme-files",
    "title": "Workshop",
    "section": "Readme files",
    "text": "Readme files\nREADMEs are a form of documentation which have been widely used for a long time. They contain all the information about the other files in a directory. They can be extensive but need not be. Concise is good. Bullet points are good\n\nGive a project title and description, brief\nstart date, last updated date and contact information\nOutline the folder structure\nGive software requirements: programs and versions used or required. There are packages that give session information in R Wickham et al. (2021) and Python Ostblom, Joel (2019)\n\nR:\nsessioninfo::session_info()\nPython:\nimport session_info\nsession_info.show()\n\nInstructions run the code, build reports, and reproduce the figures etc\nWhere to find the data, outputs\nAny other information that needed to understand and recreate the work\nIdeally, a summary of changes with the date\n\n-- liver_transcriptome/\n   |__data\n      |__raw/\n         |__2022-03-21_donor_1.csv\n         |__2022-03-21_donor_2.csv\n         |__2022-03-21_donor_3.csv\n         |__2022-05-14_donor_1.csv\n         |__2022-05-14_donor_2.csv\n         |__2022-05-14_donor_3.csv\n      |__processed/\n   |__images/\n   |__code/\n      |__functions/\n         |__summarise.R\n         |__normalise.R\n         |__theme_volcano.R\n      |__01_data_processing.py\n      |__02_exploratory.R\n      |__03_modelling.R\n      |__04_figures.R\n   |__README.md\n   |__reports/\n      |__01_report.qmd\n      |__02_supplementary.qmd\n   |__figures/\n      |__01_volcano_donor_1_vs_donor_2.eps\n      |__02_volcano_donor_1_vs_donor_3.eps"
  },
  {
    "objectID": "core/week-1-old/workshop.html#code-comments",
    "href": "core/week-1-old/workshop.html#code-comments",
    "title": "Workshop",
    "section": "Code comments",
    "text": "Code comments\n\nComments are notes in the code which are not executed. They are ignored by the computer but are read by humans. They are used to explain what the code is doing and why. They are also used to temporarily remove code from execution."
  },
  {
    "objectID": "core/week-1-old/overview.html",
    "href": "core/week-1-old/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week you will revise some essential concepts for scientific computing: file system organisation, file types, working directories and paths. The workshop will cover a rationale for working reproducibly, project oriented workflow, naming things and documenting your work. We will also examine some file types and the concept of tidy data.\n\nLearning objectives\nThe successful student will be able to:\n\nexplain the organisation of files and directories in a file systems including root, home and working directories\nexplain absolute and relative file paths\nexplain why working reproducibly is important\nknow how to use a project-oriented workflow to organise work\nbe able to give files human- and machine-readable names\noutline some common biological data file formats\n\n\n\nInstructions\n\nPrepare\n\nüìñ Read Understanding file systems\n\nWorkshop\nConsolidate"
  },
  {
    "objectID": "core/week-2-old/workshop.html",
    "href": "core/week-2-old/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop you will"
  },
  {
    "objectID": "core/week-2-old/workshop.html#session-overview",
    "href": "core/week-2-old/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "In this workshop you will"
  },
  {
    "objectID": "core/week-2-old/workshop.html#omics",
    "href": "core/week-2-old/workshop.html#omics",
    "title": "Workshop",
    "section": "Omics",
    "text": "Omics\n\ngene/transcript/protein/metabolite expression\ntranscriptomics 1\ntranscriptomics 2\nproteomics"
  },
  {
    "objectID": "core/week-2-old/workshop.html#images",
    "href": "core/week-2-old/workshop.html#images",
    "title": "Workshop",
    "section": "Images",
    "text": "Images\ncontrol_merged.tif\nlibrary(ijtiff)\nimg &lt;- read_tif(\"data/control_merged.tif\")\nimg\n\nan image at least one and usually more matrices of numbers representing the intensity of light at each pixel in the image\nthe number of matrices depends on the number of ‚Äòchannels‚Äô in the image\na channel is a colour in the image\na frame is a single image in a series of images\nwe might normally call this a multi-dimensional array: x and y coordinates of the pixels are 2 dimensions, the channel is the third dimension and time is the forth dimension\n\ndisplay(img)"
  },
  {
    "objectID": "core/week-2-old/workshop.html#structure",
    "href": "core/week-2-old/workshop.html#structure",
    "title": "Workshop",
    "section": "Structure",
    "text": "Structure\n1cq2.pdb"
  },
  {
    "objectID": "core/week-2-old/workshop.html#the-command-line",
    "href": "core/week-2-old/workshop.html#the-command-line",
    "title": "Workshop",
    "section": "The command line",
    "text": "The command line\nThe command line - or shell - is a text interface for your computer. It‚Äôs a program that takes in commands, which it passes on to the computer‚Äôs operating system to run.\n\nWindows PowerShell is a command-line in windows. It uses bash-like commands unlike the Command Prompt which uses dos commands (a sort of windows only language). You can open is by going to Start | Windows PowerShell or by searching for it in the search bar.\nTerminal is the command line in Mac OS X. You can open it by going to Applications | Utilities | Terminal or by searching for it in the Spotlight search bar.\ngit bash. I used the bash shell that comes with Git"
  },
  {
    "objectID": "core/week-2-old/workshop.html#rstudio-terminal",
    "href": "core/week-2-old/workshop.html#rstudio-terminal",
    "title": "Workshop",
    "section": "RStudio terminal",
    "text": "RStudio terminal\nThe RStudio terminal is a convenient interface to the shell without leaving RStudio. It is useful for running commands that are not available in R. For example, you can use it to run other programs like fasqc, git, ftp, ssh\nNavigating your file system\nSeveral commands are frequently used to create, inspect, rename, and delete files and directories.\n$\nThe dollar sign is the prompt (like &gt; on the R console), which shows us that the shell is waiting for input.\nYou can find out where you are using the pwd command, which stands for ‚Äúprint working directory‚Äù.\n\npwd\n\n/home/runner/work/BIO00088H-data/BIO00088H-data/core/week-2-old\n\n\nYou can find out what you can see with ls which stands for ‚Äúlist‚Äù.\n\nls\n\ndata\nimages\noverview.qmd\nstudy_after_workshop.html\nstudy_after_workshop.qmd\nstudy_before_workshop.ipynb\nstudy_before_workshop.qmd\nworkshop.html\nworkshop.qmd\nworkshop.rmarkdown\nworkshop_files\n\n\nYou might have noticed that unlike R, the commands do not have brackets after them. Instead, options (or switches) are given after the command. For example, we can modify the ls command to give us more information with the -l option, which stands for ‚Äúlong‚Äù.\n\nls -l\n\ntotal 152\ndrwxr-xr-x 2 runner docker  4096 Oct 25 10:38 data\ndrwxr-xr-x 2 runner docker  4096 Oct 25 10:38 images\n-rw-r--r-- 1 runner docker  1597 Oct 25 10:38 overview.qmd\n-rw-r--r-- 1 runner docker 22717 Oct 25 10:45 study_after_workshop.html\n-rw-r--r-- 1 runner docker   184 Oct 25 10:38 study_after_workshop.qmd\n-rw-r--r-- 1 runner docker  4807 Oct 25 10:38 study_before_workshop.ipynb\n-rw-r--r-- 1 runner docker 13029 Oct 25 10:38 study_before_workshop.qmd\n-rw-r--r-- 1 runner docker 58063 Oct 25 10:38 workshop.html\n-rw-r--r-- 1 runner docker  8550 Oct 25 10:38 workshop.qmd\n-rw-r--r-- 1 runner docker  8577 Oct 25 10:45 workshop.rmarkdown\ndrwxr-xr-x 3 runner docker  4096 Oct 25 10:38 workshop_files\n\n\nYou can use more than one option at once. The -h option stands for ‚Äúhuman readable‚Äù and makes the file sizes easier to understand for humans:\n\nls -hl\n\ntotal 152K\ndrwxr-xr-x 2 runner docker 4.0K Oct 25 10:38 data\ndrwxr-xr-x 2 runner docker 4.0K Oct 25 10:38 images\n-rw-r--r-- 1 runner docker 1.6K Oct 25 10:38 overview.qmd\n-rw-r--r-- 1 runner docker  23K Oct 25 10:45 study_after_workshop.html\n-rw-r--r-- 1 runner docker  184 Oct 25 10:38 study_after_workshop.qmd\n-rw-r--r-- 1 runner docker 4.7K Oct 25 10:38 study_before_workshop.ipynb\n-rw-r--r-- 1 runner docker  13K Oct 25 10:38 study_before_workshop.qmd\n-rw-r--r-- 1 runner docker  57K Oct 25 10:38 workshop.html\n-rw-r--r-- 1 runner docker 8.4K Oct 25 10:38 workshop.qmd\n-rw-r--r-- 1 runner docker 8.4K Oct 25 10:45 workshop.rmarkdown\ndrwxr-xr-x 3 runner docker 4.0K Oct 25 10:38 workshop_files\n\n\nThe -a option stands for ‚Äúall‚Äù and shows us all the files, including hidden files.\n\nls -alh\n\ntotal 160K\ndrwxr-xr-x 5 runner docker 4.0K Oct 25 10:45 .\ndrwxr-xr-x 8 runner docker 4.0K Oct 25 10:45 ..\ndrwxr-xr-x 2 runner docker 4.0K Oct 25 10:38 data\ndrwxr-xr-x 2 runner docker 4.0K Oct 25 10:38 images\n-rw-r--r-- 1 runner docker 1.6K Oct 25 10:38 overview.qmd\n-rw-r--r-- 1 runner docker  23K Oct 25 10:45 study_after_workshop.html\n-rw-r--r-- 1 runner docker  184 Oct 25 10:38 study_after_workshop.qmd\n-rw-r--r-- 1 runner docker 4.7K Oct 25 10:38 study_before_workshop.ipynb\n-rw-r--r-- 1 runner docker  13K Oct 25 10:38 study_before_workshop.qmd\n-rw-r--r-- 1 runner docker  57K Oct 25 10:38 workshop.html\n-rw-r--r-- 1 runner docker 8.4K Oct 25 10:38 workshop.qmd\n-rw-r--r-- 1 runner docker 8.4K Oct 25 10:45 workshop.rmarkdown\ndrwxr-xr-x 3 runner docker 4.0K Oct 25 10:38 workshop_files\n\n\nYou can move about with the cd command, which stands for ‚Äúchange directory‚Äù. You can use it to move into a directory by specifying the path to the directory:\n\ncd data\npwd\ncd ..\npwd\ncd data\npwd\n\n/home/runner/work/BIO00088H-data/BIO00088H-data/core/week-2-old/data\n/home/runner/work/BIO00088H-data/BIO00088H-data/core/week-2-old\n/home/runner/work/BIO00088H-data/BIO00088H-data/core/week-2-old/data\n\n\nhead 1cq2.pdb\nHEADER    OXYGEN STORAGE/TRANSPORT                04-AUG-99   1CQ2              \nTITLE     NEUTRON STRUCTURE OF FULLY DEUTERATED SPERM WHALE MYOGLOBIN AT 2.0    \nTITLE    2 ANGSTROM                                                             \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: MYOGLOBIN;                                                 \nCOMPND   3 CHAIN: A;                                                            \nCOMPND   4 ENGINEERED: YES;                                                     \nCOMPND   5 OTHER_DETAILS: PROTEIN IS FULLY DEUTERATED                           \nSOURCE    MOL_ID: 1;                                                            \nSOURCE   2 ORGANISM_SCIENTIFIC: PHYSETER CATODON;      \nhead -20 data/1cq2.pdb\nHEADER    OXYGEN STORAGE/TRANSPORT                04-AUG-99   1CQ2              \nTITLE     NEUTRON STRUCTURE OF FULLY DEUTERATED SPERM WHALE MYOGLOBIN AT 2.0    \nTITLE    2 ANGSTROM                                                             \nCOMPND    MOL_ID: 1;                                                            \nCOMPND   2 MOLECULE: MYOGLOBIN;                                                 \nCOMPND   3 CHAIN: A;                                                            \nCOMPND   4 ENGINEERED: YES;                                                     \nCOMPND   5 OTHER_DETAILS: PROTEIN IS FULLY DEUTERATED                           \nSOURCE    MOL_ID: 1;                                                            \nSOURCE   2 ORGANISM_SCIENTIFIC: PHYSETER CATODON;                               \nSOURCE   3 ORGANISM_COMMON: SPERM WHALE;                                        \nSOURCE   4 ORGANISM_TAXID: 9755;                                                \nSOURCE   5 EXPRESSION_SYSTEM: ESCHERICHIA COLI;                                 \nSOURCE   6 EXPRESSION_SYSTEM_TAXID: 562;                                        \nSOURCE   7 EXPRESSION_SYSTEM_VECTOR_TYPE: PLASMID;                              \nSOURCE   8 EXPRESSION_SYSTEM_PLASMID: PET15A                                    \nKEYWDS    HELICAL, GLOBULAR, ALL-HYDROGEN CONTAINING STRUCTURE, OXYGEN STORAGE- \nKEYWDS   2 TRANSPORT COMPLEX                                                    \nEXPDTA    NEUTRON DIFFRACTION                                                   \nAUTHOR    F.SHU,V.RAMAKRISHNAN,B.P.SCHOENBORN   \nless 1cq2.pdb\nless is a program that displays the contents of a file, one page at a time. It is useful for viewing large files because it does not load the whole file into memory before displaying it. Instead, it reads and displays a few lines at a time. You can navigate forward through the file with the spacebar, and backwards with the b key. Press q to quit.\nA wildcard is a character that can be used as a substitute for any of a class of characters in a search, The most common wildcard characters are the asterisk (*) and the question mark (?).\nls *.csv\ncp stands for ‚Äúcopy‚Äù. You can copy a file from one directory to another by giving cp the path to the file you want to copy and the path to the destination directory.\ncp 1cq2.pdb copy_of_1cq2.pdb\ncp 1cq2.pdb ../copy_of_1cq2.pdb\ncp 1cq2.pdb ../bob.txt\nTo delete a file use the rm command, which stands for ‚Äúremove‚Äù.\nrm ../bob.txt\nbut be careful because the file will be gone forever. There is no ‚Äúare you sure?‚Äù or undo.\nTo move a file from one directory to another, use the mv command. mv works like cp except that it also deletes the original file.\nmv ../copy_of_1cq2.pdb .\nMake a directory\nmkdir mynewdir"
  },
  {
    "objectID": "core/week-2-old/workshop.html#differences-between-r-and-python",
    "href": "core/week-2-old/workshop.html#differences-between-r-and-python",
    "title": "Workshop",
    "section": "Differences between R and python",
    "text": "Differences between R and python\nDemo\nYou‚Äôre finished!"
  },
  {
    "objectID": "core/week-2-old/overview.html",
    "href": "core/week-2-old/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week we will consider File types, workflow tips and other tools. The independent study reiterates the value of RStudio projects and shows you how you create them with usethis. You will also learn how to recognise and write cool üòé code, not üò© ugly code and code algorithmically. In the workshop we will examine some common biological data formats and discover some awesome short cuts to help you write cool üòé code. You will also get a brief introduction to the command line and Google Colab.\n\nLearning objectives\nThe successful student will be able to:\n\nexplain why RStudio are useful/essential and be able to use the usethis package\nwrite cool üòé code not üò© ugly code\nexplain the value of code which expresses the structure of the problem/solution.\ndescribe some common file types for biological data\nuse some useful shortcuts to help write cool üòé code\nknow what the command line is and how to use it for simple tasks\nuse Google colab to run code\nrecognise some of the differences between R and Python\n\n\n\nInstructions\n\nPrepare 20 mins reading on RStudio Projects revisited, formatting code and coding algorithmically\nWorkshop\n\nüí¨ Types of biological data files\nü™Ñ Workflow tips and shortcuts\nüíª The command line\nüíª Google colab\nüíª Python\n\nConsolidate\n\nüíª not sure yet :)"
  },
  {
    "objectID": "core/week-2/workshop.html#session-overview",
    "href": "core/week-2/workshop.html#session-overview",
    "title": "Workshop",
    "section": "Session overview",
    "text": "Session overview\nIn this workshop we will discuss why reproducibility matters and how to organise your work to make it reproducible. We will cover:\n\n\nWhat is reproducibility\nHow to achieve reproducibility\nRationale for scripting\nProject-oriented workflow\nCode formatting and style\nCoding algorithmically\nNaming things\nAnd some handy workflow tips",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#what-is-reproducibility",
    "href": "core/week-2/workshop.html#what-is-reproducibility",
    "title": "Workshop",
    "section": "What is reproducibility?",
    "text": "What is reproducibility?\n\n\nThe Turing Way‚Äôs definitions of reproducible research",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#definitions",
    "href": "core/week-2/workshop.html#definitions",
    "title": "Workshop",
    "section": "Definitions",
    "text": "Definitions\n\n\nThe Turing Way‚Äôs definitions of reproducible research\n\nReproducible: Same data + same analysis = identical results. ‚Äú‚Ä¶ obtaining consistent results using the same input data; computational steps, methods, and code; and conditions of analysis. This definition is synonymous with‚Äùcomputational reproducibility‚Äù (National Academies of Sciences et al. 2019). This is what we are concentrating on in the Supporting Information.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#definitions-1",
    "href": "core/week-2/workshop.html#definitions-1",
    "title": "Workshop",
    "section": "Definitions",
    "text": "Definitions\n\n\nThe Turing Way‚Äôs definitions of reproducible research\n\nReplicable: Different data + same analysis = qualitatively similar results. The work is not dependent on the specificities of the data.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#definitions-2",
    "href": "core/week-2/workshop.html#definitions-2",
    "title": "Workshop",
    "section": "Definitions",
    "text": "Definitions\n\n\nThe Turing Way‚Äôs definitions of reproducible research\n\nRobust: Same data + different analysis = qualitatively similar or identical results. The work is not dependent on the specificities of the analysis.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#definitions-3",
    "href": "core/week-2/workshop.html#definitions-3",
    "title": "Workshop",
    "section": "Definitions",
    "text": "Definitions\n\n\nThe Turing Way‚Äôs definitions of reproducible research\n\nGeneralisable: Different data + different analysis = qualitatively similar results and same conclusions.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#why-does-it-matter",
    "href": "core/week-2/workshop.html#why-does-it-matter",
    "title": "Workshop",
    "section": "Why does it matter?",
    "text": "Why does it matter?\n\n\nMany high profile cases of work which did not reproduce e.g.¬†Anil Potti unravelled by Baggerly and Coombes (2009)\nFive selfish reasons to work reproducibly (Markowetz 2015). Alternatively, see the very entertaining talk\nWill become standard in Science and publishing e.g OECD Global Science Forum Building digital workforce capacity and skills for data-intensive science (OECD Global Science Forum 2020)",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#how-to-achieve-reproducibility",
    "href": "core/week-2/workshop.html#how-to-achieve-reproducibility",
    "title": "Workshop",
    "section": "How to achieve reproducibility",
    "text": "How to achieve reproducibility\n\nReproducibility is a continuum. Some is better than none!\nScript everything\nOrganisation: Project-oriented workflows with file and folder structure, naming things\nCode: follow a consistent style, organise into sections and scripts (be modular), Code algorithmically\nDocumentation: Readme files, code comments, metadata,\nMore advanced: version, control, continuous integration and testing (not required for Supporting Information)",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rationale-for-scripting",
    "href": "core/week-2/workshop.html#rationale-for-scripting",
    "title": "Workshop",
    "section": "Rationale for scripting",
    "text": "Rationale for scripting\n\nScience is the generation of ideas, designing work to test them and reporting the results.\nWe ensure laboratory and field work is replicable, robust and generalisable by planning and recording in lab books and using standard protocols. Repeating results is still hard.\nWorkflows for computational projects, and the data analysis and reporting of other work can, and should, be 100% reproducible!\nScripting is the way to achieve this.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#project-oriented-workflow",
    "href": "core/week-2/workshop.html#project-oriented-workflow",
    "title": "Workshop",
    "section": "Project-oriented workflow",
    "text": "Project-oriented workflow\n\nuse folders to organise your work\nyou are aiming for structured, systematic and repeatable.\ninputs and outputs should be clearly identifiable from structure and/or naming",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#example-si-itself-is-an-rsp",
    "href": "core/week-2/workshop.html#example-si-itself-is-an-rsp",
    "title": "Workshop",
    "section": "Example: SI itself is an RSP",
    "text": "Example: SI itself is an RSP\n\n-- stem_cell_rna\n   |__stem_cell_rna.Rproj   \n   |__raw_ data/            \n      |__2019-03-21_donor_1.csv\n      |__2019-03-21_donor_2.csv\n      |__2019-03-21_donor_3.csv\n   |__README.md\n   |__R/\n      |__01_data_processing.R\n      |__02_exploratory.R\n      |__functions/\n         |__theme_volcano.R\n         |__normalise.R",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#example-si-includes-an-rsp",
    "href": "core/week-2/workshop.html#example-si-includes-an-rsp",
    "title": "Workshop",
    "section": "Example: SI includes an RSP",
    "text": "Example: SI includes an RSP\n\n-- stem_cell_rna\n   |__data_processing/\n      |__01_data_processing.py\n      |__02_exploratory.py\n      |__raw_data/\n         |__2019-03-21_donor_1.csv\n         |__2019-03-21_donor_2.csv\n         |__2019-03-21_donor_3.csv\n   |__README.md\n   |__statistical_analysis\n      |__statistical_analysis.Rproj   \n      |__processed_data/\n      |__R/\n         |__01_DGE.R\n         |__02_visualisation.R\n         |__functions/\n            |__theme_volcano.R\n            |__normalise.R",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects",
    "href": "core/week-2/workshop.html#rstudio-projects",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\nRStudio Projects make it easy to manage working directories and paths because they set the working directory to the RStudio Projects directory automatically.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects-1",
    "href": "core/week-2/workshop.html#rstudio-projects-1",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\n\n-- stem_cell_rna\n   |__stem_cell_rna.Rproj   \n   |__raw_ data/            \n      |__2019-03-21_donor_1.csv\n   |__README. md\n   |__R/\n      |__01_data_processing.R\n      |__02_exploratory.R\n      |__functions/\n         |__theme_volcano.R\n         |__normalise.R\n\n\nThe project directory is the folder at the top",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects-2",
    "href": "core/week-2/workshop.html#rstudio-projects-2",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\n\n-- stem_cell_rna\n   |__stem_cell_rna.Rproj   \n   |__raw_ data/            \n      |__2019-03-21_donor_1.csv\n   |__README. md\n   |__R/\n      |__01_data_processing.R\n      |__02_exploratory.R\n      |__functions/\n         |__theme_volcano.R\n         |__normalise.R\n\n\nthe .RProj file is directly under the project folder1. Its presence is what makes the folder an RStudio Project\n\nThanks to Mine √áetinkaya-Rundel who helped me work out how to highlight a line https://gist.github.com/mine-cetinkaya-rundel/3af3415eab70a65be3791c3dcff6e2e3. Note to futureself: the engine: knitr matters.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects-3",
    "href": "core/week-2/workshop.html#rstudio-projects-3",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\n\nWhen you open an RStudio Project, the working directory is set to the Project directory (i.e., the location of the .Rproj file).\nWhen you use an RStudio Project you do not need to use setwd()\nWhen someone, including future you, opens the project on another machine, all the paths just work.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#rstudio-projects-4",
    "href": "core/week-2/workshop.html#rstudio-projects-4",
    "title": "Workshop",
    "section": "RStudio Projects",
    "text": "RStudio Projects\n\nJenny BryanIn the words of Jenny Bryan:\n\n‚ÄúIf the first line of your R script is setwd(‚ÄùC:/Users/jenny/path/that/only/I/have‚Äù) I will come into your office and SET YOUR COMPUTER ON FIRE‚Äù",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#creating-an-rstudio-project",
    "href": "core/week-2/workshop.html#creating-an-rstudio-project",
    "title": "Workshop",
    "section": "Creating an RStudio Project",
    "text": "Creating an RStudio Project\nThere are two menus options:\n\nTop left, File menu\nTop Right, drop-down indicated by the .RProj icon\n\nThey both do the same thing.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#creating-an-rstudio-project-1",
    "href": "core/week-2/workshop.html#creating-an-rstudio-project-1",
    "title": "Workshop",
    "section": "Creating an RStudio Project",
    "text": "Creating an RStudio Project\nThen Choose: New Project | New Directory | New Project\nMake sure you ‚ÄúBrowse‚Äù to the folder you want to create the project.\n‚ùî Is your working directory a good place to create a Project folder?",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#creating-an-rstudio-project-2",
    "href": "core/week-2/workshop.html#creating-an-rstudio-project-2",
    "title": "Workshop",
    "section": "Creating an RStudio Project",
    "text": "Creating an RStudio Project\nWhen you create a new RStudio Project\n\n\nA folder called bananas/ is created\nRStudio starts a new session in bananas/ i.e., your working directory is now bananas/\n\nA file called bananas.Rproj is created\nthe .Rproj file is what makes the directory an RStudio Project",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#opening-and-closing",
    "href": "core/week-2/workshop.html#opening-and-closing",
    "title": "Workshop",
    "section": "Opening and closing",
    "text": "Opening and closing\nYou can close an RStudio Project with ONE of:\n\nFile | Close Project\nUsing the drop-down option on the far right of the tool bar where you see the Project name",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#opening-and-closing-1",
    "href": "core/week-2/workshop.html#opening-and-closing-1",
    "title": "Workshop",
    "section": "Opening and closing",
    "text": "Opening and closing\nYou can open an RStudio Project with ONE of:\n\nFile | Open Project or File | Recent Projects\n\nUsing the drop-down option on the far right of the tool bar where you see the Project name\n\nDouble-clicking an .Rproj file from your file explorer/finder\n\nWhen you open project, a new R session starts.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#code-formatting-and-style-1",
    "href": "core/week-2/workshop.html#code-formatting-and-style-1",
    "title": "Workshop",
    "section": "Code formatting and style",
    "text": "Code formatting and style\n\n‚ÄúGood coding style is like correct punctuation: you can manage without it butitsuremakesthingseasiertoread.‚Äù\n\nThe tidyverse style guide\n\nCode is not write only.\nCode is communication!",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#code-formatting-and-style-2",
    "href": "core/week-2/workshop.html#code-formatting-and-style-2",
    "title": "Workshop",
    "section": "Code formatting and style",
    "text": "Code formatting and style\nWe have all written code which is hard to read!\nWe all improve over time.\n\n\n\nThe only way to write good code is to write tons of shitty code first. Feeling shame about bad code stops you from getting to good code‚Äî Hadley Wickham (@hadleywickham) April 17, 2015",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#code-formatting-and-style-3",
    "href": "core/week-2/workshop.html#code-formatting-and-style-3",
    "title": "Workshop",
    "section": "Code formatting and style",
    "text": "Code formatting and style\nSome keys points:\n\n\nbe consistent, emulate experienced coders\n\nuse snake_case for variable names (not CamelCase, dot.case)\n\nuse &lt;- (not =) for assignment\n\nuse spacing around most operators and after commas\n\nuse indentation\n\navoid long lines, break up code blocks with new lines\n\nuse \" for quoting text (not ') unless the text contains double quotes\n\nspace after # for comments",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#ugly-code",
    "href": "core/week-2/workshop.html#ugly-code",
    "title": "Workshop",
    "section": "üò© Ugly code üò©",
    "text": "üò© Ugly code üò©\n\n\ndata&lt;-read_csv('../data-raw/Y101_Y102_Y201_Y202_Y101-5.csv',skip=2)\nlibrary(janitor);sol&lt;-clean_names(data)\ndata=data|&gt;filter(str_detect(description,\"OS=Homo sapiens\"))|&gt;filter(x1pep=='x')\ndata=data|&gt;\nmutate(g=str_extract(description,\n\"GN=[^\\\\s]+\")|&gt;str_replace(\"GN=\",''))\ndata&lt;-data|&gt;mutate(id=str_extract(accession,\"1::[^;]+\")|&gt;str_replace(\"1::\",\"\"))",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#ugly-code-1",
    "href": "core/week-2/workshop.html#ugly-code-1",
    "title": "Workshop",
    "section": "üò© Ugly code üò©",
    "text": "üò© Ugly code üò©\n\nno spacing or indentation\ninconsistent splitting of code blocks over lines\ninconsistent use of quote characters\nno comments\nvariable names convey no meaning\nuse of = for assignment and inconsistently\nmultiple commands on a line\nlibrary statement in the middle of the analysis",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#cool-code",
    "href": "core/week-2/workshop.html#cool-code",
    "title": "Workshop",
    "section": "üòé Cool code üòé",
    "text": "üòé Cool code üòé\n\n\n# Packages ----------------------------------------------------------------\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Import ------------------------------------------------------------------\n\n# define file name\nfile &lt;- \"../data-raw/Y101_Y102_Y201_Y202_Y101-5.csv\"\n\n# import: column headers and data are from row 3\nsolu_protein &lt;- read_csv(file, skip = 2) |&gt;\n  clean_names()\n\n# Tidy data ----------------------------------------------------------------\n\n# filter out the bovine proteins and those proteins \n# identified from fewer than 2 peptides\nsolu_protein &lt;- solu_protein |&gt;\n  filter(str_detect(description, \"OS=Homo sapiens\")) |&gt;\n  filter(x1pep == \"x\")\n\n# Extract the genename from description column to a column\n# of its own\nsolu_protein &lt;- solu_protein |&gt;\n  mutate(genename =  str_extract(description,\"GN=[^\\\\s]+\") |&gt;\n           str_replace(\"GN=\", \"\"))\n\n# Extract the top protein identifier from accession column (first\n# Uniprot ID after \"1::\") to a column of its own\nsolu_protein &lt;- solu_protein |&gt;\n  mutate(protid =  str_extract(accession, \"1::[^;]+\") |&gt;\n           str_replace(\"1::\", \"\"))",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#cool-code-1",
    "href": "core/week-2/workshop.html#cool-code-1",
    "title": "Workshop",
    "section": "üòé Cool code üòé",
    "text": "üòé Cool code üòé\n\nlibrary() calls collected\nUses code sections to make it easier to navigate\nUses white space and proper indentation\nCommented\nUses more informative name for the dataframe",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#code-algorithmically-1",
    "href": "core/week-2/workshop.html#code-algorithmically-1",
    "title": "Workshop",
    "section": "Code ‚Äòalgorithmically‚Äô",
    "text": "Code ‚Äòalgorithmically‚Äô\n\n\nWrite code which expresses the structure of the problem/solution.\nAvoid hard coding numbers if at all possible - declare variables instead\nDeclare frequently used values as variables at the start e.g., colour schemes, figure saving settings",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#hard-coding-numbers.",
    "href": "core/week-2/workshop.html#hard-coding-numbers.",
    "title": "Workshop",
    "section": "üò© Hard coding numbers.",
    "text": "üò© Hard coding numbers.\n\n\nSuppose we want to calculate the sums of squares, \\(SS(x)\\), for the number of eggs in five nests.\nThe formula is given by: \\(\\sum (x_i- \\bar{x})^2\\)\nWe could calculate the mean and copy it, and the individual numbers into the formula",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#hard-coding-numbers.-1",
    "href": "core/week-2/workshop.html#hard-coding-numbers.-1",
    "title": "Workshop",
    "section": "üò© Hard coding numbers.",
    "text": "üò© Hard coding numbers.\n\n# mean number of eggs per nest\nsum(3, 5, 6, 7, 8) / 5\n\n[1] 5.8\n\n# ss(x) of number of eggs\n(3 - 5.8)^2 + (5 - 5.8)^2 + (6 - 5.8)^2 + (7 - 5.8)^2 + (8 - 5.8)^2\n\n[1] 14.8\n\n\nI am coding the calculation of the mean rather using the mean() function only to explain what ‚Äòcoding algorithmically‚Äô means using a simple example.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#hard-coding-numbers",
    "href": "core/week-2/workshop.html#hard-coding-numbers",
    "title": "Workshop",
    "section": "üò© Hard coding numbers",
    "text": "üò© Hard coding numbers\n\n\nif any of the sample numbers must be altered, all the code needs changing\nit is hard to tell that the output of the first line is a mean\nits hard to recognise that the numbers in the mean calculation correspond to those in the next calculation\nit is hard to tell that 5 is just the number of nests\nno way of know if numbers are the same by coincidence or they refer to the same thing",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#better",
    "href": "core/week-2/workshop.html#better",
    "title": "Workshop",
    "section": "üòé Better",
    "text": "üòé Better\n\n# eggs each nest\neggs &lt;- c(3, 5, 6, 7, 8)\n\n# mean eggs per nest\nmean_eggs &lt;- sum(eggs) / length(eggs)\n\n# ss(x) of number of eggs\nsum((eggs - mean_eggs)^2)\n\n[1] 14.8",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#better-1",
    "href": "core/week-2/workshop.html#better-1",
    "title": "Workshop",
    "section": "üòé Better",
    "text": "üòé Better\n\n\nthe commenting is similar but it is easier to follow\nif any of the sample numbers must be altered, only that number needs changing\nassigning a value you will later use to a variable with a meaningful name allows us to understand the first and second calculations\nmakes use of R‚Äôs elementwise calculation which resembles the formula (i.e., is expressed as the general rule)",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#naming-things",
    "href": "core/week-2/workshop.html#naming-things",
    "title": "Workshop",
    "section": "Naming things",
    "text": "Naming things\n\n\n\n\ndocuments, CC-BY-NC, https://xkcd.com/1459/\n\n\nGuiding principle - Have a convention! Good file names are:\n\nmachine readable\nhuman readable\nplay nicely with sorting",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#naming-suggestions",
    "href": "core/week-2/workshop.html#naming-suggestions",
    "title": "Workshop",
    "section": "Naming suggestions",
    "text": "Naming suggestions\n\nno spaces in names\nuse snake_case or kebab-case rather than CamelCase or dot.case\nuse all lower case except very occasionally where convention is otherwise, e.g., README, LICENSE\nordering: use left-padded numbers e.g., 01, 02‚Ä¶.99 or 001, 002‚Ä¶.999\ndates ISO 8601 format: 2020-10-16\nwrite down your conventions",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#summary",
    "href": "core/week-2/workshop.html#summary",
    "title": "Workshop",
    "section": "Summary",
    "text": "Summary\n\n\nUse an RStudio project for any R work (you can also incorporate other languages)\nWrite Cool code not Ugly code: space, consistency, indentation, comments, meaningful variable names\nWrite code which expresses the structure of the problem/solution.\nAvoid hard coding numbers if at all possible - declare variables instead",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#reading",
    "href": "core/week-2/workshop.html#reading",
    "title": "Workshop",
    "section": "Reading",
    "text": "Reading\nCompletely optional suggestions for further reading\n\n\n\nProject-oriented workflow | What They Forgot to Teach You About R (Bryan et al., n.d.). Recommended if you still need convincing to use RStudio Projects\nTen simple rules for reproducible computational research (Sandve et al. 2013)\n\nGood enough practices in scientific computing (Wilson et al. 2017)\n\nExcuse Me, Do You Have a Moment to Talk About Version Control? (Bryan 2018)\n\n\nPages made with R (R Core Team 2024), Quarto (Allaire et al. 2024), knitr (Xie 2024, 2015, 2014), kableExtra (Zhu 2021)",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/workshop.html#references",
    "href": "core/week-2/workshop.html#references",
    "title": "Workshop",
    "section": "References",
    "text": "References\n\n\n\nüîó About Core: Supporting Information 1\n\n\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2024. ‚ÄúQuarto.‚Äù https://doi.org/10.5281/zenodo.5960048.\n\n\nBaggerly, Keith A, and Kevin R Coombes. 2009. ‚ÄúDERIVING CHEMOSENSITIVITY FROM CELL LINES: FORENSIC BIOINFORMATICS AND REPRODUCIBLE RESEARCH IN HIGH-THROUGHPUT BIOLOGY.‚Äù Ann. Appl. Stat. 3 (4): 1309‚Äì34. http://www.jstor.org/stable/27801549.\n\n\nBryan, Jennifer. 2018. ‚ÄúExcuse Me, Do You Have a Moment to Talk about Version Control?‚Äù Am. Stat. 72 (1): 20‚Äì27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\nBryan, Jennifer, Jim Hester, Shannon Pileggi, and E. David Aja. n.d. What They Forgot to Teach You about r. https://rstats.wtf/.\n\n\nMarkowetz, Florian. 2015. ‚ÄúFive Selfish Reasons to Work Reproducibly.‚Äù Genome Biol. 16 (December): 274. https://doi.org/10.1186/s13059-015-0850-7.\n\n\nNational Academies of Sciences, Engineering, Medicine, Policy, Global Affairs, Engineering, Medicine Committee on Science, Public Policy, Board on Research Data, et al. 2019. Understanding Reproducibility and Replicability. National Academies Press (US). https://www.ncbi.nlm.nih.gov/books/NBK547546/.\n\n\nOECD Global Science Forum. 2020. ‚ÄúBuilding Digital Workforce Capacity and Skills for Data-Intensive Science.‚Äù http://www.oecd.org/officialdocuments/publicdisplaydocumentpdf/?cote=DSTI/STP/GSF(2020)6/FINAL&docLanguage=En.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig. 2013. ‚ÄúTen Simple Rules for Reproducible Computational Research.‚Äù PLoS Comput. Biol. 9 (10): e1003285. https://doi.org/10.1371/journal.pcbi.1003285.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K Teal. 2017. ‚ÄúGood Enough Practices in Scientific Computing.‚Äù PLoS Comput. Biol. 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nXie, Yihui. 2014. ‚ÄúKnitr: A Comprehensive Tool for Reproducible Research in R.‚Äù In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\n‚Äî‚Äî‚Äî. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\n‚Äî‚Äî‚Äî. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. ‚ÄúkableExtra: Construct Complex Table with ‚ÄôKable‚Äô and Pipe Syntax.‚Äù https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "Workshop"
    ]
  },
  {
    "objectID": "core/week-2/overview.html",
    "href": "core/week-2/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week you will revise some essential concepts for scientific computing: file system organisation, file types, working directories and paths. You will also ensure you know how to use the virtual desktop service (VDS) to access the software you need for your work. The workshop will cover a rationale for working reproducibly, project oriented workflow, naming things and documenting your work.\n\nLearning objectives\nThe successful student will be able to:\n\nexplain the organisation of files and directories in a file systems including root, home and working directories\nexplain absolute and relative file paths\nexplain why working reproducibly is important\nknow how to use a project-oriented workflow to organise work\nbe able to give files human- and machine-readable names\nwrite cool üòé code not üò© ugly code\nexplain the value of code which expresses the structure of the problem/solution.\nuse some useful shortcuts to help write cool üòé code\nensure they can use the VDS to access files and software required for the work\n\n\n\nInstructions\n\nPrepare\n\nüìñ Read Understanding file systems\nüìñ Read Workflow in RStudio\nüñ•Ô∏èSet up the VDS and know how to use it.\n\nWorkshop\nConsolidate",
    "crumbs": [
      "Core Supporting Info",
      "Week 2: Supporting Information 1",
      "About"
    ]
  },
  {
    "objectID": "core/week-6-old/workshop.html",
    "href": "core/week-6-old/workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "Use this session to ask any questions about Core 1 Organising reproducible data analyses and Core 2 File types, workflow tips and other tools in particular, or about R and RStudio in general. We will also try to answer any questions about the ‚Äômics, Image and Structure strands.\n88H students might also review Stage 1 and 2 content to see if there are areas you might benefit from revisiting. You can access these through the past VLE sites but you might find it helpful to use the latest versions because there is no 2FA and the resources are searchable.\nStage 1\n\nData Analysis in R for Becoming a Bioscientist 1.Core concepts about scientific computing, types of variable, the role of variables in analysis and how to use RStudio to organise analysis and import, summarise and plot data.\nData Analysis in R for Becoming a Bioscientist 2. The logic of hypothesis testing, confidence intervals, what is meant by a statistical model, two-sample tests and one- and two-way analysis of variance (ANOVA).\n\nStage 2\n\nGet Introductory Statistical Tests as Linear models: A guide for R users\nA simple introduction to GLM for analysing Poisson and Binomial responses in R\n\n70M students might also review 52M content to see if there are areas you might benefit from revisiting. You can access these through the VLE site but you might find it helpful to use this link without 2FA.\n\n52M Data Analysis in R. Core concepts about scientific computing, types of variable, the role of variables in analysis and how to use RStudio to organise analysis and import, summarise and plot data, the logic of hypothesis testing, confidence intervals, what is meant by a statistical model, two-sample tests and one-way analysis of variance (ANOVA) and reproducible reports in Quarto.\n\nPages made with R (R Core Team 2024), Quarto (allaire2022?), knitr (knitr?), kableExtra (Zhu 2021)"
  },
  {
    "objectID": "core/week-6-old/workshop.html#session-overview",
    "href": "core/week-6-old/workshop.html#session-overview",
    "title": "Workshop",
    "section": "",
    "text": "Use this session to ask any questions about Core 1 Organising reproducible data analyses and Core 2 File types, workflow tips and other tools in particular, or about R and RStudio in general. We will also try to answer any questions about the ‚Äômics, Image and Structure strands.\n88H students might also review Stage 1 and 2 content to see if there are areas you might benefit from revisiting. You can access these through the past VLE sites but you might find it helpful to use the latest versions because there is no 2FA and the resources are searchable.\nStage 1\n\nData Analysis in R for Becoming a Bioscientist 1.Core concepts about scientific computing, types of variable, the role of variables in analysis and how to use RStudio to organise analysis and import, summarise and plot data.\nData Analysis in R for Becoming a Bioscientist 2. The logic of hypothesis testing, confidence intervals, what is meant by a statistical model, two-sample tests and one- and two-way analysis of variance (ANOVA).\n\nStage 2\n\nGet Introductory Statistical Tests as Linear models: A guide for R users\nA simple introduction to GLM for analysing Poisson and Binomial responses in R\n\n70M students might also review 52M content to see if there are areas you might benefit from revisiting. You can access these through the VLE site but you might find it helpful to use this link without 2FA.\n\n52M Data Analysis in R. Core concepts about scientific computing, types of variable, the role of variables in analysis and how to use RStudio to organise analysis and import, summarise and plot data, the logic of hypothesis testing, confidence intervals, what is meant by a statistical model, two-sample tests and one-way analysis of variance (ANOVA) and reproducible reports in Quarto.\n\nPages made with R (R Core Team 2024), Quarto (allaire2022?), knitr (knitr?), kableExtra (Zhu 2021)"
  },
  {
    "objectID": "core/week-6-old/overview.html",
    "href": "core/week-6-old/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This week‚Äôs session is a drop-in and introduces no new material. Instead, it is an opportunity to ask questions about the content from Core 1 and 2 and to revise skills from stage 1 and 2 as needed.\n\nInstructions\n\nPrepare\n\nüìñ Review content from Core 1 and 2\n\nWorkshop\n\nüíª Ask questions about the content from Core 1 and 2 as needed\nüíª Revise skills from stage 1 and 2 (88H students) or 52M (70M students) as needed\n\nConsolidate\n\nThere is no consolidation work for this drop-in"
  }
]