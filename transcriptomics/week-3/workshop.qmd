---
title: "Workshop"
subtitle: "Transcriptomics 1: Hello data!"
author: "Emma Rand"
toc: true
toc-depth: 4
toc-location: right
execute:
  echo: true
  include: true
  error: true
bibliography: ../../references.bib
editor: 
  markdown: 
    wrap: 72
---

# Introduction

## Session overview

In this workshop you will learn what steps to take to get a good
understanding of your transcriptomics data before you consider any statistical
analysis. This is an often overlooked, but very valuable and
informative, part of any data pipeline. It gives you the deep
understanding of the data structures and values that you will need to
code and trouble-shoot code, allows you to spot failed or problematic
samples and informs your decisions on quality control.

You should examine **all three data sets** because the comparisons will
give you a stronger understanding of your own project data.

# Exercises

## Set up a Project

🎬 Start RStudio from the Start menu

🎬 Make an RStudio project. Be deliberate about where you create it so
that it is a good place for you

🎬 Use the Files pane to make new folders for the data. I suggest
`data-raw` and `data-processed`

🎬 Make a new script called `workshop-1.R` to carry out the rest of the
work.

🎬 Record what you do and what you find out. All of it!

🎬 Load `tidyverse` [@tidyverse] for importing, summarising, plotting
and filtering.

```{r}
library(tidyverse)
```

## Examine the data in a spreadsheet

These are the three datasets. Each set compromises several files.

🐸 Frog development data:

-   [xlaevis_counts_S14.csv](data-raw/xlaevis_counts_S14.csv)
-   [xlaevis_counts_S20.csv](data-raw/xlaevis_counts_S20.csv)
-   [xlaevis_counts_S30.csv](data-raw/xlaevis_counts_S30.csv)


🐭 Stem cell data:

-   [surfaceome_hspc.csv](data-raw/surfaceome_hspc.csv)
-   [surfaceome_prog.csv](data-raw/surfaceome_prog.csv)
-   [surfaceome_lthsc.csv](data-raw/surfaceome_lthsc.csv)

🍂 xxxx data:

-   xxx
-   xxx

🎬 Save the files to `data-raw` and open them in Excel

🎬 Answer the following questions:

-   Describe how the sets of data are similar and how they are
    different.
-   What is in the rows and columns of each file?
-   How many rows and columns are there in each file? Are these the
    same? In all cases or some cases? Why?
-   Google an id. Where does your search take you? How much information
    is available?

🎬 Did you record all that??

## Import

Now let's get the data into R and visualise it.

🎬 Import [xlaevis_counts_S30.csv](data-raw/xlaevis_counts_S30.csv),
[surfaceome_hspc.csv](data-raw/surfaceome_hspc.csv) and xxxxxxxx

```{r}
# 🐸 import the s30 data
s30 <- read_csv("data-raw/xlaevis_counts_S30.csv")
```

```{r}
# 🐭 import the hspc data
hspc <- read_csv("data-raw/surfaceome_hspc.csv")
```

```{r}
# 🍂 xxxx import the xxxx data
# prog <- read_csv("")
```

🎬 Check these have the number of rows and column you were expecting and
that column types and names are as expected.

## Explore

The first task is to get an overview. We want to know

-   are there any missing values? If so, how many and how are they
    distributed?
-   how may zeros are there and how are they distributed
-   does it look as tough all the samples/cells were equally
    "successful"? Can we spot any problematic anomalies?
-   what is the distribution of values?

If our data collection has gone well we would hope to see approximately
the same average expression in each sample or cell of the same type.
That is replicates should be similar. We would also expect to see that
the average expression of genes varies. We might have genes which are
zero in every cell/sample. We will want to to filter those out.

We get this overview by looking at:

-   The distribution of values across the whole dataset

-   The distribution of values across the sample/cells (i.e., averaged
    across genes). This allows us to see variation between
    samples/cells:

-   The distribution of values across the genes (i.e., averaged across
    samples/cells). This allows us to see variation between genes.

### Distribution of values across the whole dataset

In all data sets, the values are spread over multiple columns so in
order to plot the distribution as a whole, we will need to first use
`pivot_longer()` to put the data in ['tidy'
format](https://3mmarand.github.io/BIO00017C-Data-Analysis-in-R-2020/workshops/02TestingDataTypesReadingInData.html#Tidy_format)
[@Wickham2014-nl] by stacking the columns. We *could* save a copy of the
stacked data and then plot it, but here, I have just piped the stacked
data straight into `ggplot()`.

#### 🐸 Frogs

🎬 Pivot the counts (stack the columns) so all the counts are in a
single column (`count`) and pipe into `ggplot()` to create a histogram:

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  ggplot(aes(x = count)) +
  geom_histogram()
```

This data is very skewed - there are so many low values that we can't
see the tiny bars for the higher values. Logging the counts is a way to
make the distribution more visible.

🎬 Repeat the plot on log of the counts.

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  ggplot(aes(x = log10(count))) +
  geom_histogram()
```

I've used base 10 only because it easy to convert to the original scale
(1 is 10, 2 is 100, 3 is 1000 etc). The warning about rows being removed
is expected - these are the counts of 0 since you can't log a value of
0. The peak at zero suggests quite a few counts of 1. We would expect we
would expect the distribution of counts to be roughly log normal because
this is expression of *all* the genes in the genome[^1]. That small peak
near the low end suggests that these lower counts might be anomalies.

[^1]: This a result of the [Central limit
    theorem](https://en.wikipedia.org/wiki/Central_limit_theorem),one
    consequence of which is that adding together lots of distributions -
    whatever distributions they are - will tend to a normal
    distribution.

The excess number of low counts indicates we might want to create a cut
off for quality control. The removal of low counts is a common
processing step in 'omic data. We will revisit this after we have
considered the distribution of counts across samples and genes.

#### 🐭 Mice

🎬 Pivot the expression values (stack the columns) so all the counts are
in a single column (`expr`) and pipe into `ggplot()` to create a
histogram:

```{r}
hspc |>
  pivot_longer(cols = -ensembl_gene_id,
               names_to = "cell",
               values_to = "expr") |> 
  ggplot(aes(x = expr)) +
  geom_histogram()
```

This is a very striking distribution. Is it what we are expecting?
Again,the excess number of low values is almost certainly anomalous.
They will be inaccurate measure and we will want to exclude expression
values below (about) 1. We will revisit this after we have considered
the distribution of expression across cells and genes.

What about the bimodal appearance of the the 'real' values? If we had
the whole genome we would not expect to see such a pattern - we'd expect
to see a roughly normal distribution[^2]. However, this is a subset of
the genome and the nature of the subsetting has had an influence here.
These are a subset of cell surface proteins that show a significant
difference between at least two of twelve cell subtypes. That is, all of
these genes are either high or low.

[^2]: This a result of the [Central limit
    theorem](https://en.wikipedia.org/wiki/Central_limit_theorem),one
    consequence of which is that adding together lots of distributions -
    whatever distributions they are - will tend to a normal
    distribution.

### Distribution of values across the sample/cells

#### 🐸 Frog samples

Summary statistics including the the number of NAs can be seen using the
`summary()`. It is most helpful which you have up to about 30 columns.
There is nothing special about the number 30, it is just that text
summaries of a larger number of columns are difficult to grasp.

🎬 Get a quick overview of the columns:

```{r}
# examine all the columns quickly
# works well with smaller numbers of column
summary(s30)
```

Notice that: - the minimum count is 0 and the maximums are very high in
all the columns - the medians are quite a lot lower than the means so
the data are skewed (hump to the left, tail to the right) - there must
be quite a lot of zeros - the columns are roughly similar and it doesn't
look like there is an anomalous replicate.

To find out how may zeros there are in a column we can make use of the
fact that `TRUE` evaluates to 1 and `FALSE` evaluates to 0. This means
`sum(S30_C_5 == 0)` gives the number of 0 in the `S30_C_5` column

🎬 Find the number of zeros in all six columns:

```{r}

s30 |>
  summarise(sum(S30_C_5 == 0),
            sum(S30_C_6 == 0),
            sum(S30_C_A == 0),
            sum(S30_F_5 == 0),
            sum(S30_F_6 == 0),
            sum(S30_F_A == 0))
```

There is a better way of doing this that saves you having to repeat so
much code - especially useful if you have a lot more than 6 columns. We
can use `pivot_longer()` to put the data in tidy format and then use the
`group_by()` and `summarise()` approach we have used extensively before.

🎬 Find the number of zeros in all columns:

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  group_by(sample) |>
  summarise(n_zero = sum(count == 0))
```

You could expand to get all the summary information

🎬 Summarise all the samples:

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  group_by(sample) |>
  summarise(min = min(count),
            lowerq = quantile(count, 0.25),
            mean = mean(count),
            median = median(count),
            upperq = quantile(count, 0.75),
            max = max(count),
            n_zero = sum(count == 0))
```

The mean count ranges from 260 to 426.

One advantage this has over using `summary()` is that the output is a
dataframe. For results, this is useful, and makes it easier to:

-   write to file
-   use in `ggplot()`
-   format in a Quarto report

🎬 Save the summary as a dataframe, `s30_summary_samp`.

```{r}
#| echo: false
s30_summary_samp <- s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  group_by(sample) |>
  summarise(min = min(count),
            lowerq = quantile(count, 0.25),
            mean = mean(count),
            median = median(count),
            upperq = quantile(count, 0.75),
            max = max(count),
            n_zero = sum(count == 0))
```

We can write to file using `write_csv()`

🎬 Write `s30_summary_samp` to a file called "s30_summary_samp.csv":

```{r}
write_csv(s30_summary_samp, 
          file = "data-processed/s30_summary_samp.csv")
```

Plotting the distribution of values is perhaps the easiest way to
understand the data. We could plot each column separately or we can pipe
the tidy format of data into `ggplot()` and make use of `facet_wrap()`

🎬 Pivot the data and pipe into `ggplot`:

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  ggplot(aes(count)) +
  geom_density() +
  facet_wrap(. ~ sample, nrow = 3)

```

We have many values (`r length(s30$xenbase_gene_id)`) so we are not
limited to using `geom_histogram()`. `geom_density()` gives us a smooth
distribution.

We have many low values and a few very high ones which makes it tricky
to see the distributions. Logging the counts will make these clearer.

🎬 Repeat the graph but taking the base 10 log of the counts:

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  ggplot(aes(log10(count))) +
  geom_density() +
  facet_wrap(. ~ sample, nrow = 3)
```

The key information to take from these plots is:

-   the distributions are roughly similar in width, height, location and
    overall shape so it doesn't look as though we have any suspect
    samples
-   the peak at zero suggests quite a few counts of 1.
-   since we would expect the distribution of counts in each sample to
    be roughly log normal so that the small rise near the low end, even
    before the peak at zero, suggests that these lower counts might be
    anomalies.

The excess number of low counts indicates we might want to create a cut
off for quality control. The removal of low counts is a common
processing step in 'omic data. We will revisit this after we have
considered the distribution of counts across genes (averaged over the
samples).

#### 🐭 Mouse cells

We used the `summary()` function to get an overview of the columns in
the frog data. Let's try that here.

🎬 Get a quick overview of the columns:

```{r}
#| class: outputscroll
summary(hspc)
```

Hmmmm, did you get all that? Nope, me neither! We have 701 cells but we
only have 6 samples for the frogs. We will need a different approach to
get an overview but I find it is still useful to look at the few columns

🎬 Get a quick overview the first 20 columns:

```{r}
#| class: outputscroll
summary(hspc[1:20])
```

Notice that:

-   the maximum value is much less high than for the frogs and has
    decimals. That is because the mouse data are logged (to base 2)
    normalised counts, not raw counts as they are in the frog data set.
-   a minimum value of 0 appears in all 20 columns - perhaps that is
    true across the whole dataset (or at least common)
-   at least some of the medians are zeros so there must be quite a lot
    of zeros
-   the few columns we can see are roughly similar
-   it would not be very practical to plot the distributions of values
    in cell cell using `facet_wrap()`.

In this data set, there is even more of an advantage of using the
`pivot_longer()`, `group_by()` and `summarise()` approach. We will be
able to open the dataframe in the Viewer and make plots to examine
whether the distributions are similar across cells.

🎬 Summarise all the cells:

```{r}
hspc_summary_samp <- hspc |>
  pivot_longer(cols = -ensembl_gene_id,
               names_to = "cell",
               values_to = "expr") |>
  group_by(cell) |>
  summarise(min = min(expr),
            lowerq = quantile(expr, 0.25),
            mean = mean(expr),
            median = median(expr),
            sd = sd(expr),
            upperq = quantile(expr, 0.75),
            max = max(expr),
            n_zero = sum(expr == 0))
```

Notice that I have used `cell` as the column name rather than `sample`
and `expr` (expression) rather than `count`. I've also added the
standard deviation.

🎬 View the `hspc_summary_samp` dataframe (click on it in the
environment).

All cells have quite a few zeros and the lower quartile is 0 for all
cells, *i.e.*, every cell has many genes with zero expression.

To get a better understanding of the distribution of expressions in
cells we can create a ggplot using the pointrange geom. Pointrange puts
a dot at the mean and a line between a minimum and a maximum such as +/-
one s.d. Not unlike a boxplot, but when you need the boxes too be very
narrow!

🎬 Create a pointrange plot.

```{r}
hspc_summary_samp |> 
  ggplot(aes(x = cell, y = mean)) +
  geom_pointrange(aes(ymin = mean - sd, 
                      ymax = mean + sd ),
                  size = 0.1)
```

You will need to use the Zoom button to pop the plot window out so you
can make it as wide as possible

The things to notice are:

-   the average expression in cells is similar for all cells. This is
    good to know - if some cells had much lower expression perhaps there
    is something wrong with them, or their sequencing, and they should
    be excluded.
-   the distributions are roughly similar in width too

The default order of `cell` is alphabetical. It can be easier to see
these (non-) effects if we order the lines by the size of the mean.

🎬 Order a pointrange plot with `reorder(variable_to_order, order_by)`.

```{r}
hspc_summary_samp |> 
  ggplot(aes(x = reorder(cell, mean), y = mean)) +
  geom_pointrange(aes(ymin = mean - sd, 
                      ymax = mean + sd ),
                  size = 0.1)
```

`reorder()` arranges `cell` in increasing size of `mean`

🎬 Write `hspc_summary_samp` to a file called "hspc_summary_samp.csv":

```{r}
#| echo: false
write_csv(hspc_summary_samp, 
          file = "data-processed/hspc_summary_samp.csv")
```

### Distribution of values across the genes

#### 🐸 Frog genes

There are lots of genes in this dataset therefore we will take the same
approach as that we took for the distributions across mouse cells. We
will pivot the data to tidy and then summarise the counts for each gene.

🎬 Summarise the counts for each genes:

```{r}
s30_summary_gene <- s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  group_by(xenbase_gene_id) |>
  summarise(min = min(count),
            lowerq = quantile(count, 0.25),
            sd = sd(count),
            mean = mean(count),
            median = median(count),
            upperq = quantile(count, 0.75),
            max = max(count),
            total = sum(count),
            n_zero = sum(count == 0))
```

I have calculated the values we used before with one addition: the sum
of the counts (`total`).

🎬 View the `s30_summary_gene` dataframe.

Notice that we have:

-   a lot of genes with counts of zero in every sample
-   a lot of genes with zero counts in several of the samples
-   some very very low counts.

These should be filtered out because they are unreliable - or, at the
least, uninformative. The goal of our downstream analysis will be to see
if there is a signifcance difference in gene expression between the
control and FGF-treated sibling. Since we have only three replicates in
each group, having one or two unreliable, missing or zero values, makes
such a determination impossible for a particular gene. We will use the
total counts and the number of samples with non-zero values to filter
our genes later.

As we have a lot of genes, it is again helpful to plot the mean counts
with pointrange to get an overview. We will plot the log of the counts -
we saw earlier that logging made it easier to understand the
distribution of counts over such a wide range. We will also order the
genes from lowest to highest mean count.

🎬 Plot the logged mean counts for each gene in order of size using
`geom_pointrange()`:

```{r}
s30_summary_gene |> 
  ggplot(aes(x = reorder(xenbase_gene_id, mean), y = log10(mean))) +
  geom_pointrange(aes(ymin = log10(mean - sd), 
                      ymax = log10(mean + sd )),
                  size = 0.1)
```

(Remember, the warning is expected since we have zeros).

You can see we also have quite a few genes with means less than 1 (log
below zero). Note that the variability between genes (average counts
between 0 and 102586) is far greater than between samples (average
counts from 260 to 426) which is exactly what we would expect to see.

🎬 Write `s30_summary_gene` to a file called "s30_summary_gene.csv":

```{r}
#| echo: false
write_csv(s30_summary_gene, 
          file = "data-processed/s30_summary_gene.csv")
```

#### 🐭 Mouse genes

There are fewer genes in this dataset, but still more than you can
understand without the overview provided by a plot. We will again pivot
the data to tidy and then summarise the expression for each gene.

🎬 Summarise the expression for each genes:

```{r}
hspc_summary_gene <- hspc |>
  pivot_longer(cols = -ensembl_gene_id,
               names_to = "cell",
               values_to = "expr") |>
  group_by(ensembl_gene_id) |>
  summarise(min = min(expr),
            lowerq = quantile(expr, 0.25),
            sd = sd(expr),
            mean = mean(expr),
            median = median(expr),
            upperq = quantile(expr, 0.75),
            max = max(expr),
            total = sum(expr),
            n_zero = sum(expr == 0))
```

🎬 View the `hspc_summary_gene` dataframe. Remember these are normalised
and logged (base 2) so we should not see very large values.

Notice that we have:

-   no genes with 0 in every cell
-   very few genes (9) with no zeros at all
-   quite a few genes with zero in many cells but this matters less than
    zeros in the frog samples because we had just 6 samples and we have
    701 cells.

As we have a lot of genes, it is again helpful to plot the mean
expression with pointrange to get an overview. We do not need to log the
values but ordering the genes will help.

🎬 Plot the logged mean counts for each gene in order of size using
`geom_pointrange()`:

```{r}
hspc_summary_gene |> 
  ggplot(aes(x = reorder(ensembl_gene_id, mean), y = mean)) +
  geom_pointrange(aes(ymin = mean - sd, 
                      ymax = mean + sd),
                  size = 0.1)
```

Note again that the variability between genes (average expression
between 0.02 and and 10.03) is far greater than between cells (average
expression from1.46 to 3.18) which is expected.

🎬 Write `s30_summary_gene` to a file called "s30_summary_gene.csv":

```{r}
#| echo: false
write_csv(hspc_summary_gene, 
          file = "data-processed/hspc_summary_gene.csv")
```

## Filtering for QC

### 🐸 Frog filtering

Our samples look to be similarly well sequenced. There are no samples we
should remove. However, some genes are not express or the expression
values are so low in for a gene that they are uninformative. We will
filter the `s30_summary_gene` dataframe to obtain a list of
`xenbase_gene_id` we can use to filter `s30`.

My suggestion is to include only the genes with counts in at least 3
samples[^3] and those with total counts above 20.

[^3]: I chose three because that would keep \[0, 0, 0\] \[#,#,#\]. This
    is difference we cannot test statistically, but which would matter
    biologically.

🎬 Filter the summary by gene dataframe:

```{r}
s30_summary_gene_filtered <- s30_summary_gene |> 
  filter(total > 20) |> 
  filter(n_zero < 4)
```

🎬 Write the filtered summary by gene to file:

```{r}
write_csv(s30_summary_gene_filtered, 
          file = "data-processed/s30_summary_gene_filtered.csv")
```

🎬 Use the list of `xenbase_gene_id` in the filtered summary to filter
the original dataset:

```{r}
s30_filtered <- s30 |> 
  filter(xenbase_gene_id %in%  s30_summary_gene_filtered$xenbase_gene_id)
```

🎬 Write the filtered original to file:

```{r}
write_csv(s30_filtered, 
          file = "data-processed/s30_filtered.csv")
```

### 🐭 Mouse filtering

We will take a different approach to filtering the single cell data. For
the Frog samples we are examining the control and the FGF treated
samples. This means have a low number of counts overall means the gene
is not really expressed (detected) in any condition, and filtering out
those genes is removing things that definitely are not interesting. For
the mice, we have examined only one cell type but will be making
comparisons between cells types. It may be that low expression of a gene
in this cell type tells us something if that gene is highly expressed in
another cell type. Instead, we will make statistical comparisons between
the cell types and then filter based on overall expression, the
difference in expression between cell types and whether that difference
is significant.

The number of "replicates" is also important. When you have only three
in each group it is not possible to make statistical comparisons when
several replicates are zero. This is less of an issue with single cell
data.

## 🤗 Look after future you!

**You need only do the section for your own project data**

### 🐸 Frogs and future you

🎬 Create a new Project, `frogs-88H`, populated with folders and your
data. Make a script file called `cont-fgf-s30.R`. This will a be
commented analysis of the control vs FGF at S30 comparison. You will
build on this each workshop and be able to use it as a template to
examine other comparisons. Copy in the appropriate code and comments
from `workshop-1.R`. Edit to improve your comments where your
understanding has developed since you made them. Make sure you can close
down RStudio, reopen it and run your whole script again.

### 🐭 Mice and future you

🎬 Create a new Project, `mice-88H`, populated with folders and your
data. Make a script file called `hspc-prog.R`. This will a be commented
analysis of the hspc cells vs the prog cells. At this point you will
have only code for the hspc cells. You will build on this each workshop
and be able to use it as a template to examine other comparisons. Copy
in the appropriate code and comments from `workshop-1.R`. Edit to
improve your comments where your understanding has developed since you
made them. Make sure you can close down RStudio, reopen it and run your
whole script again.

### 🍂 xxxx and future you

**Do one of the other two examples.**

<!-- 🎬 Create a new Project, `xxxx-88H`, populated with folders and your -->
<!-- data. Make a script file called `xxxx.R`. This will a be commented -->
<!-- analysis of t xxxxxxxxxxxx. -->

# 🥳 Finished

Well Done!

# Independent study following the workshop

[Consolidate](study_after_workshop.qmd)

# The Code file

These contain all the code needed in the workshop even where it is not
visible on the webpage.

The [workshop.qmd](workshop.qmd) file is the file I use to compile the
practical. Qmd stands for Quarto markdown. It allows code and ordinary
text to be interleaved to produce well-formatted reports including
webpages. Right-click on the link and choose Save-As to download. You
will be able to open the Qmd file in RStudio. Alternatively, [View in
Browser](https://github.com/3mmaRand/). Coding and thinking answers are
marked with `#---CODING ANSWER---` and `#---THINKING ANSWER---`

Pages made with R [@R-core], Quarto [@allaire2022], `knitr` [@knitr],
`kableExtra` [@kableExtra]

# References
