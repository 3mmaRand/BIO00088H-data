---
title: "Workshop"
subtitle: "Transcriptomics 1: Hello data!"
author: "Emma Rand"
toc: true
toc-depth: 4
toc-location: right
execute:
  echo: true
  include: true
  error: true
bibliography: ../../references.bib
editor: 
  markdown: 
    wrap: 72
---

# Introduction

## Session overview

In this workshop you will learn what steps to take to get a good
understanding of your transcriptomics data before you consider any statistical
analysis. This is an often overlooked, but very valuable and
informative, part of any data pipeline. It gives you the deep
understanding of the data structures and values that you will need to
code and trouble-shoot code, allows you to spot failed or problematic
samples and informs your decisions on quality control.

In this session, you should examine **all four data sets** because the 
comparisons will give you a much stronger understanding of your own 
project data. Compare and contrast is a very useful way to build 
understanding.

# Set up a Project

🎬 Start RStudio from the Start menu

🎬 Make an RStudio project. Be deliberate about where you create it so
that it is a good place for you

🎬 Use the Files pane to make new folders for the data. I suggest
`data-raw` and `data-processed`

🎬 Make a new script called `workshop-1.R` to carry out the rest of the
work.

🎬 Record what you do and what you find out. All of it!

🎬 Load `tidyverse` [@tidyverse] for importing, summarising, plotting
and filtering.

```{r}
library(tidyverse)
```

# Examine data using a spreadsheet

These are the four datasets. Each set compromises several files.

🐸 Frog development:

-   [xlaevis_counts_S14.csv](data-raw/xlaevis_counts_S14.csv)
-   [xlaevis_counts_S20.csv](data-raw/xlaevis_counts_S20.csv)
-   [xlaevis_counts_S30.csv](data-raw/xlaevis_counts_S30.csv)


🐭 Stem cells:

-   [surfaceome_hspc.csv](data-raw/surfaceome_hspc.csv)
-   [surfaceome_prog.csv](data-raw/surfaceome_prog.csv)
-   [surfaceome_lthsc.csv](data-raw/surfaceome_lthsc.csv)

🎄 Arabidopsis:

-   xxx
-   xxx

💉 Leishmania:

-   xxx

🎬 Save the files to `data-raw` and open them in Excel

🎬 Answer the following questions:

-   Describe how the sets of data are similar and how they are
    different.
-   What is in the rows and columns of each file?
-   How many rows and columns are there in each file? Are these the
    same? In all cases or some cases? Why?
-   Google gene id. Where does your search take you? How much information
    is available?

🎬 Did you record all that??


# Explore data in R

The first task is to get an overview. We want to know

-   how may zeros are there and how are they distributed?
-   what is the distribution of non-zero values?
-   does it look as though all the samples/cells were equally
    "successful"? Can we spot any problems?
-   can we determine a quality threshold for filtering out genes with
    very low counts?


Genes which are zero in *every* cell/sample, i.e., genes that are not 
expressed at all give us no information. We will want to to filter those out. 
We will also want to filter out genes with very low counts for quality control. These are mostly likely false positives.
If our data collection has gone well we would hope to see approximately
the same average expression in each sample/cell of the same type.
That is, replicates should be similar. In contrast, we would expect to 
see that the average expression of genes varies. 

We get this overview by looking at:

-   The **distribution of values across all the data in the file**

-   The **distribution of values across the samples/cells** (i.e., averaged
    across genes). This allows us to see variation between
    samples/cells:

-   The **distribution of values across the genes** (i.e., averaged across
    samples/cells). This allows us to see variation between genes.

The next sections will guide you through each of these for each of the 
data sets. Start with your own data set then move on to at least one of the others.

## 🐸 Frog development

### Import

Import the data for stage 30.

🎬 Import [xlaevis_counts_S30.csv](data-raw/xlaevis_counts_S30.csv)

```{r}
# 🐸 import the s30 data
s30 <- read_csv("data-raw/xlaevis_counts_S30.csv")
```

🎬 Check the dataframe has the number of rows and columns you were expecting and that column types and names are as expected.


### Distribution of values across all the data in the file

The values are spread over multiple columns so in
order to plot the distribution as a whole, we will need to first use
`pivot_longer()` to put the data in ['tidy'
format](https://3mmarand.github.io/R4BABS/r4babs1/week-9/workshop.html#tidying-data)
[@wickham2014] by stacking the columns. We *could* save a copy of the
stacked data and then plot it, but here, I have just piped the stacked
data straight into `ggplot()`. This helps me avoid cluttering my R environment with
temporary objects.

🎬 Pivot the counts (stack the columns) so all the counts are in a
single column (`count`) labelled in `sample` by the column it came from 
and pipe into `ggplot()` to create a histogram:

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  ggplot(aes(x = count)) +
  geom_histogram()
```

This data is very skewed - there are very many low counts and a very few higher numbers. It is hard to see the very low bars for the higher values. Logging the counts is a way to make the distribution more visible. You cannot take the log of 0 so we add 1 to the count before logging. The log of 1 is zero so we will be 
able to see how many zeros we had.

🎬 Repeat the plot of log of the counts.

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  ggplot(aes(x = log10(count + 1))) +
  geom_histogram()
```

I've used base 10 only because it easy to convert to the original scale
(1 is 10, 2 is 100, 3 is 1000 etc).
Notice we have a peak at zero indicating there are many zeros. We would 
expect the distribution of counts to be roughly log normal because
this is expression of *all* the genes in the genome[^1]. The number of low
counts is inflated (small peak near the low end). This suggests that these lower counts might be false positives. The removal of low counts is a common
processing step in 'omic data. We will revisit this after we have
considered the distribution of counts across samples and genes.

### Distribution of values across the samples

Summary statistics including the the number of NAs can be seen using the
`summary()`. It is most helpful which you have up to about 25 columns.
There is nothing special about the number 25, it is just that 
summaries of a larger number of columns are difficult to grasp.

🎬 Get a quick overview of the 7 columns:

```{r}
# examine all the columns quickly
# works well with smaller numbers of column
summary(s30)
```

Notice that: 

-  the minimum count is 0 and the maximums are very high in
   all the columns 
-  the medians are quite a lot lower than the means so the data are 
   skewed (hump to the left, tail to the right) and there must
   be quite a lot of zeros 
-  `S30_F_3` does have a somewhat lower maximum count

We want to know how many zeros there are in each a column. A useful trick to determine this is to make use we can make use of the fact that `TRUE` 
evaluates to 1 and `FALSE` evaluates to 0. This means you can sum a column 
of TRUE/FALSE values to get the number of TRUE values. For example,
`sum(S30_C_1 > 0)` gives the number of values above zero in the `S30_C_1` 
column. If you wanted the number of zeros, you would use `sum(S30_C_1 == 0)`.

🎬 Find the number values above zero in all six columns:

```{r}

s30 |>
  summarise(sum(S30_C_1 > 0),
            sum(S30_C_2 > 0),
            sum(S30_C_3 > 0),
            sum(S30_F_1 > 0),
            sum(S30_F_2 > 0),
            sum(S30_F_3 > 0))
```


There is a better way of doing this that saves you having to repeat so
much code - very useful if you have a lot more than 6 columns! We
can use `pivot_longer()` to put the data in tidy format and then use the
`group_by()` and `summarise()` approach we have used extensively before.

🎬 Find the number of zeros in all columns:

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  group_by(sample) |>
  summarise(n_above_zero = sum(count > 0))
```

You could expand this code to get get other useful summary information

🎬 Summarise all the samples:

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  group_by(sample) |>
  summarise(min = min(count),
            lowerq = quantile(count, 0.25),
            mean = mean(count),
            median = median(count),
            upperq = quantile(count, 0.75),
            max = max(count),
            n_above_zero = sum(count > 0))
```

The mean count ranges from 260 to 426. `S30_F_3` does stand out a little but not by too much. If we had more replicates we might consider analysing with and with out this replicate. Since we have just 3, we will leave it in. The potential effect of 
an odd replicate is slightly statistical power. The major differences in gene expression will still be uncovered. Differences between genes with lower average expression and or more variable expression might be missed. Whether this matters depends on the biological question you are asking. In this case, it does not matter
because the major differences in gene expression will be enough.

🎬 Save the summary as a dataframe, `s30_summary_samp` (using assignment).

```{r}
#| echo: false
#---CODING ANSWER---
s30_summary_samp <- s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  group_by(sample) |>
  summarise(min = min(count),
            lowerq = quantile(count, 0.25),
            mean = mean(count),
            median = median(count),
            upperq = quantile(count, 0.75),
            max = max(count),
            n_above_zero = sum(count > 0))
```



We can also plot the distribution of counts across samples. We have many values (`r length(s30$xenbase_gene_id)`) so we are not limited to using 
`geom_histogram()`. `geom_density()` gives us a smooth distribution.

🎬 Plot the log10 of the counts + 1 again but this time facet by the sample:

```{r}
s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  ggplot(aes(log10(count + 1))) +
  geom_density() +
  facet_wrap(. ~ sample, nrow = 3)
```

The key information to take from these plots is:

-   the distributions are roughly similar with `S30_F_3` does stand out a little
-   the peak at zero suggests quite a few counts of 1.
-   since we would expect the distribution of counts in each sample to
    be roughly log normal so that the small rise near the low end, even
    before the peak at zero, suggests that these lower counts might be
    anomalies.

We have found the distribution across samples to be like that to the distribution over all. This is good because it means that the samples are fairly consistent with each other. We can now move on to the next step.


### Distribution of values across the genes

There are lots of genes in this dataset therefore we will take a slightly different approach. We would not want to use plot a distribution for each gene in the same way. Will pivot the data to tidy and then summarise the counts for each gene.

🎬 Summarise the counts for each gene and save the result as `s30_summary_gene`. Include the same columns as we had in the by sample summary (`s30_summary_samp`) *and* an additional column, `total` for the total number of counts for each gene.



```{r}
#| echo: false
#---CODING ANSWER---
# you need to group by `xenbase_gene_id` rather than `sample_id`
# sum(count) will find the sum of counts
s30_summary_gene <- s30 |>
  pivot_longer(cols = -xenbase_gene_id,
               names_to = "sample",
               values_to = "count") |>
  group_by(xenbase_gene_id) |>
  summarise(min = min(count),
            lowerq = quantile(count, 0.25),
            sd = sd(count),
            mean = mean(count),
            median = median(count),
            upperq = quantile(count, 0.75),
            max = max(count),
            total = sum(count),
            n_above_zero = sum(count > 0))
```

🎬 View the `s30_summary_gene` dataframe.

Notice that we have:

-   a lot of genes with counts of zero in every sample
-   a lot of genes with zero counts in several of the samples
-   some very very low counts.

Genes with very low counts should be filtered out because they are 
unreliable - or, at the least, uninformative. The goal of our 
downstream analysis will be to see if there is a significant difference in gene expression between the control and FGF-treated sibling. Since we have only three replicates in each group, having one or two unreliable, missing or zero 
values, makes such a determination impossible for a particular gene.
We will use the total counts (`total`) and the number of samples with 
non-zero values (`n_above_zero`) in this dataframe to filter our genes later.

As we have a lot of genes, it is again helpful to plot the mean counts
with `geom_pointrange()` to get an overview of the distributions. We will 
again plot the log of the mean counts. We will also order the
genes from lowest to highest mean count.



🎬 Plot the logged mean counts for each gene in order of size using
`geom_pointrange()`:

```{r}
s30_summary_gene |> 
  ggplot(aes(x = reorder(xenbase_gene_id, mean), y = log10(mean))) +
  geom_pointrange(aes(ymin = log10(mean - sd), 
                      ymax = log10(mean + sd )),
                  size = 0.1)
```

(Note the warning is expected since we have zero means).

You can see we also have quite a few genes with means less than 1 (log
below zero). Note that the variability between genes (average counts
between 0 and 102586) is far greater than between samples (average
counts from 260 to 426) which is exactly what we would expect to see.


## 🐭 Stem cells

### Distribution of values across all the data in the file

### Distribution of values across the samples

### Distribution of values across the genes

## 🎄 Arabidopsis

### Distribution of values across all the data in the file

### Distribution of values across the samples

### Distribution of values across the genes

## 💉 Leishmania

### Distribution of values across all the data in the file

### Distribution of values across the samples

### Distribution of values across the genes




# Filtering for QC

## 🐸 Frog development

Our samples look to be similarly well sequenced. There are no samples we
should remove. However, some genes are not expressed or the expression
values are so low in for a gene that they are uninformative. We will
filter the `s30_summary_gene` dataframe to obtain a list of
`xenbase_gene_id` we can use to filter `s30`.

My suggestion is to include only the genes with counts in at least 3
samples. and those with total counts above 20. I chose 3 because that 
would keep genes expressed only in one sample: \[0, 0, 0\] \[#,#,#\]. 
This is a difference we cannot test statistically, but which matters 
biologically.

🎬 Filter the summary by gene dataframe:

```{r}
s30_summary_gene_filtered <- s30_summary_gene |> 
  filter(total > 20) |> 
  filter(n_above_zero >= 3)
```

❓ How many genes do you have left

<!-- #---THINKING ANSWER--- -->

<!-- there are 10136 genes left. 1757 have total counts less than 20 and or -->
<!-- have counts in fewer the 3 samples. -->


🎬 Use the list of `xenbase_gene_id` in the filtered summary to filter
the original dataset:

```{r}
s30_filtered <- s30 |> 
  filter(xenbase_gene_id %in%  s30_summary_gene_filtered$xenbase_gene_id)
```

🎬 Write the filtered original to file:

```{r}
write_csv(s30_filtered, 
          file = "data-processed/s30_filtered.csv")
```

## 🐭 Stem cells

We will take a different approach to filtering the single cell data. For
the Frog samples we are examining the control and the FGF treated
samples. This means have a low number of counts overall means the gene
is not really expressed (detected) in any condition, and filtering out
those genes is removing things that definitely are not interesting. For
the mice, we have examined only one cell type but will be making
comparisons between cells types. It may be that low expression of a gene
in this cell type tells us something if that gene is highly expressed in
another cell type. Instead, we will make statistical comparisons between
the cell types and then filter based on overall expression, the
difference in expression between cell types and whether that difference
is significant.

The number of "replicates" is also important. When you have only three
in each group it is not possible to make statistical comparisons when
several replicates are zero. This is less of an issue with single cell
data.

## 🎄 Arabidopsis


## 💉 Leishmania



# 🤗 Look after future you!

You need only do the section for your own project data but **completing this step is important**.

## 🐸 Frogs and future you

🎬 Create a new Project, `frogs-88H`, populated with folders and your
data. Make a script file called `cont-fgf-s30.R`. This will a be
commented analysis of the control vs FGF at S30 comparison. You will
build on this each workshop and be able to use it as a template to
examine other comparisons. Copy in the appropriate code and comments
from `workshop-1.R`. Edit to improve your comments where your
understanding has developed since you made them. Make sure you can close
down RStudio, reopen it and run your whole script again.

## 🐭 Mice and future you

🎬 Create a new Project, `mice-88H`, populated with folders and your
data. Make a script file called `hspc-prog.R`. This will a be commented
analysis of the hspc cells vs the prog cells. At this point you will
have only code for the hspc cells. You will build on this each workshop
and be able to use it as a template to examine other comparisons. Copy
in the appropriate code and comments from `workshop-1.R`. Edit to
improve your comments where your understanding has developed since you
made them. Make sure you can close down RStudio, reopen it and run your
whole script again.

## 🎄 Arabidopsis and future you


## 💉 Leishmania and future you



# 🥳 Finished

Well Done!

# Independent study following the workshop

[Consolidate](study_3fter_workshop.qmd)

# The Code file

These contain all the code needed in the workshop even where it is not
visible on the webpage.

The [workshop.qmd](workshop.qmd) file is the file I use to compile the
practical. Qmd stands for Quarto markdown. It allows code and ordinary
text to be interleaved to produce well-formatted reports including
webpages. Right-click on the link and choose Save-As to download. You
will be able to open the Qmd file in RStudio. Alternatively, [View in
Browser](https://github.com/3mmaRand/BIO00088H-data/blob/main/transcriptomics/week-3/workshop.qmd). Coding and thinking answers are
marked with `#---CODING ANSWER---` and `#---THINKING ANSWER---`

Pages made with R [@R-core], Quarto [@Allaire_Quarto_2024], `knitr` [@knitr1; @knitr2; @knitr3], `kableExtra` [@kableExtra]

[^1]: This a result of the [Central limit
    theorem](https://en.wikipedia.org/wiki/Central_limit_theorem),one
    consequence of which is that adding together lots of distributions -
    whatever distributions they are - will tend to a normal
    distribution.

# References
